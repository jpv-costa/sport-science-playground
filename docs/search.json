[
  {
    "objectID": "01_pelland_meta_regression.html",
    "href": "01_pelland_meta_regression.html",
    "title": "Does Training to Failure Matter?",
    "section": "",
    "text": "Should I push every set to complete failure, or stop a few reps short?\nThis question has practical implications:\n\nTraining to failure is harder and requires more recovery\nIf stopping short gives the same results, you could train more frequently\nBut if failure is necessary for gains, stopping short wastes your time\n\nPelland and colleagues tackled this question by analyzing all available research on the topic."
  },
  {
    "objectID": "01_pelland_meta_regression.html#the-question-every-lifter-asks",
    "href": "01_pelland_meta_regression.html#the-question-every-lifter-asks",
    "title": "Does Training to Failure Matter?",
    "section": "",
    "text": "Should I push every set to complete failure, or stop a few reps short?\nThis question has practical implications:\n\nTraining to failure is harder and requires more recovery\nIf stopping short gives the same results, you could train more frequently\nBut if failure is necessary for gains, stopping short wastes your time\n\nPelland and colleagues tackled this question by analyzing all available research on the topic."
  },
  {
    "objectID": "01_pelland_meta_regression.html#what-is-proximity-to-failure",
    "href": "01_pelland_meta_regression.html#what-is-proximity-to-failure",
    "title": "Does Training to Failure Matter?",
    "section": "What is “Proximity to Failure”?",
    "text": "What is “Proximity to Failure”?\nResearchers use Repetitions in Reserve (RIR) to measure how close you are to failure:\n\n\n\nRIR\nMeaning\n\n\n\n\n0\nComplete failure - couldn’t do another rep\n\n\n1\nStopped with 1 rep “left in the tank”\n\n\n2\nStopped with 2 reps remaining\n\n\n3+\nStopped well short of failure\n\n\n\nThe key question: Does RIR predict training outcomes?"
  },
  {
    "objectID": "01_pelland_meta_regression.html#the-study-design",
    "href": "01_pelland_meta_regression.html#the-study-design",
    "title": "Does Training to Failure Matter?",
    "section": "The Study Design",
    "text": "The Study Design\n\nMeta-Regression: Analyzing All the Evidence\nRather than running a single study, the authors performed a meta-regression - a statistical technique that:\n\nGathers effect sizes from many individual studies\nWeights each study by its precision (larger studies count more)\nModels how a predictor (RIR) relates to outcomes across all studies\nAccounts for non-independence (multiple effects from the same study)\n\n\n\nThe Data\n\n\nShow code\ndata.frame(\n  Outcome = c(\"Strength\", \"Hypertrophy\"),\n  `Effect Sizes` = c(nrow(results$data_strength), nrow(results$data_hypertrophy)),\n  `Imputed Correlation` = c(\n    round(results$correlation_strength, 3),\n    round(results$correlation_hypertrophy, 3)\n  ),\n  check.names = FALSE\n) |&gt; format_table()\n\n\n\n\nTable 1: Summary of included effect sizes\n\n\n\n\n\n\nStrength\n243\n0.866\n\n\nHypertrophy\n140\n0.875\n\n\n\n\n\n\n\n\nThe analysis included 243 strength effect sizes and 140 hypertrophy effect sizes."
  },
  {
    "objectID": "01_pelland_meta_regression.html#the-key-finding-strength-vs-hypertrophy-differ",
    "href": "01_pelland_meta_regression.html#the-key-finding-strength-vs-hypertrophy-differ",
    "title": "Does Training to Failure Matter?",
    "section": "The Key Finding: Strength vs Hypertrophy Differ",
    "text": "The Key Finding: Strength vs Hypertrophy Differ\n\nStrength Gains\n\n\nShow code\nstr_mod &lt;- results$strength_model\nstr_rir_b &lt;- str_mod$b[\"avg.rir\", 1]\nstr_rir_p &lt;- str_mod$pval[rownames(str_mod$b) == \"avg.rir\"]\n\n\nFor strength gains, RIR showed:\n\nCoefficient: 0.0018 (effect per 1 RIR increase)\nP-value: 0.784\nInterpretation: Not statistically significant\n\n\n\n\n\n\n\nWhat This Means for Strength Training\n\n\n\nTraining closer to failure does not produce significantly greater strength gains. You can stop 2-3 reps short and still get similar strength improvements.\n\n\n\n\nMuscle Hypertrophy\n\n\nShow code\nhyp_mod &lt;- results$hypertrophy_model\nhyp_rir_b &lt;- hyp_mod$b[\"avg.rir\", 1]\nhyp_rir_p &lt;- hyp_mod$pval[rownames(hyp_mod$b) == \"avg.rir\"]\n\n\nFor muscle growth, RIR showed:\n\nCoefficient: -0.0193 (effect per 1 RIR increase)\nP-value: 0.004\nInterpretation: Statistically significant and negative\n\n\n\n\n\n\n\nWhat This Means for Muscle Building\n\n\n\nTraining closer to failure does produce greater muscle hypertrophy. Each RIR reduction (getting closer to failure) is associated with slightly more muscle growth."
  },
  {
    "objectID": "01_pelland_meta_regression.html#visualizing-the-difference",
    "href": "01_pelland_meta_regression.html#visualizing-the-difference",
    "title": "Does Training to Failure Matter?",
    "section": "Visualizing the Difference",
    "text": "Visualizing the Difference\n\n\nShow code\n# Create comparison data\ncomparison_df &lt;- data.frame(\n  Outcome = c(\"Strength\", \"Hypertrophy\"),\n  Estimate = c(str_rir_b, hyp_rir_b),\n  SE = c(str_mod$se[rownames(str_mod$b) == \"avg.rir\"],\n         hyp_mod$se[rownames(hyp_mod$b) == \"avg.rir\"]),\n  Significant = c(\"No\", \"Yes\")\n)\n\ncomparison_df$Lower &lt;- comparison_df$Estimate - 1.96 * comparison_df$SE\ncomparison_df$Upper &lt;- comparison_df$Estimate + 1.96 * comparison_df$SE\n\nggplot(comparison_df, aes(x = Outcome, y = Estimate, color = Significant)) +\n  geom_point(size = 4) +\n  geom_errorbar(aes(ymin = Lower, ymax = Upper), width = 0.2, linewidth = 1) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = COLORS$gray) +\n  scale_color_significance() +\n  labs(\n    x = NULL,\n    y = \"RIR Coefficient (Hedge's g per RIR)\",\n    title = \"Effect of RIR on Training Outcomes\",\n    subtitle = \"Negative values = closer to failure is better\"\n  )\n\n\n\n\n\n\n\n\nFigure 1: RIR Effect on Strength vs Hypertrophy Outcomes\n\n\n\n\n\nThe figure shows:\n\nStrength (gray): Confidence interval includes zero - no significant effect\nHypertrophy (red): Entirely below zero - significant negative effect"
  },
  {
    "objectID": "01_pelland_meta_regression.html#replication-validation",
    "href": "01_pelland_meta_regression.html#replication-validation",
    "title": "Does Training to Failure Matter?",
    "section": "Replication Validation",
    "text": "Replication Validation\nOur replication matches the published findings:\n\n\nShow code\ndata.frame(\n  Outcome = c(\"Strength\", \"Hypertrophy\"),\n  `Paper Claim` = c(\n    \"CI contains null (not significant)\",\n    \"Negative slope, CI excludes null\"\n  ),\n  `Our Result` = c(\n    paste0(\"p = \", format_p(str_rir_p), \" (not significant)\"),\n    paste0(\"b = \", round(hyp_rir_b, 3), \", p = \", format_p(hyp_rir_p))\n  ),\n  Match = c(\"Yes\", \"Yes\"),\n  check.names = FALSE\n) |&gt; format_table()\n\n\n\n\nTable 2: Replication vs Published Results\n\n\n\n\n\n\n\n\n\n\n\n\nStrength\nCI contains null (not significant)\np = 0.784 (not significant)\nYes\n\n\nHypertrophy\nNegative slope, CI excludes null\nb = -0.019, p = 0.004\nYes"
  },
  {
    "objectID": "01_pelland_meta_regression.html#practical-implications",
    "href": "01_pelland_meta_regression.html#practical-implications",
    "title": "Does Training to Failure Matter?",
    "section": "Practical Implications",
    "text": "Practical Implications\n\nFor Strength Training\n\nStop 2-3 reps short of failure on most sets\nReserve failure training for testing or peaking phases\nThis allows higher frequency and volume\n\n\n\nFor Muscle Building\n\nTrain closer to failure for optimal hypertrophy\n0-2 RIR appears beneficial\nBalance this against recovery demands\n\n\n\nThe Trade-Off\n\n\nShow code\n# Conceptual illustration\nrir_seq &lt;- 0:5\nconcept_df &lt;- data.frame(\n  RIR = rep(rir_seq, 2),\n  Outcome = rep(c(\"Strength Gain\", \"Muscle Growth\"), each = length(rir_seq)),\n  Benefit = c(\n    rep(1, length(rir_seq)),  # Strength: flat line (no effect)\n    1 - 0.15 * rir_seq        # Hypertrophy: decreasing line\n  )\n)\n\nggplot(concept_df, aes(x = RIR, y = Benefit, color = Outcome)) +\n  geom_line(linewidth = 1.5) +\n  geom_point(size = 3) +\n  scale_x_reverse() +\n  scale_color_outcome() +\n  labs(\n    x = \"Repetitions in Reserve (RIR)\",\n    y = \"Relative Benefit\",\n    title = \"Conceptual Model: RIR Effects by Outcome\",\n    subtitle = \"Lower RIR = closer to failure\"\n  )\n\n\n\n\n\n\n\n\nFigure 2: The Strength-Hypertrophy Trade-off with RIR"
  },
  {
    "objectID": "01_pelland_meta_regression.html#methods-summary",
    "href": "01_pelland_meta_regression.html#methods-summary",
    "title": "Does Training to Failure Matter?",
    "section": "Methods Summary",
    "text": "Methods Summary\n\nStatistical Model\nWe used a multi-level meta-regression with:\n\nFixed effects: RIR, load, volume equating, duration, training status\nRandom effects: Nested structure (~1|study/group/obs)\nEffect sizes: Hedge’s g (bias-corrected standardized mean change)\nEstimation: REML with t-distribution tests\n\n\n\nPre-Post Correlation Imputation\nMissing pre-post correlations were imputed via meta-analysis:\n\nStrength: r = 0.866\nHypertrophy: r = 0.875"
  },
  {
    "objectID": "01_pelland_meta_regression.html#references",
    "href": "01_pelland_meta_regression.html#references",
    "title": "Does Training to Failure Matter?",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "01_pelland_meta_regression.html#source",
    "href": "01_pelland_meta_regression.html#source",
    "title": "Does Training to Failure Matter?",
    "section": "Source",
    "text": "Source\n\nOriginal paper: Pelland et al. (2024) Sports Medicine\nData and code: OSF Repository"
  },
  {
    "objectID": "02_velocity_rir_relationship.html",
    "href": "02_velocity_rir_relationship.html",
    "title": "Can Bar Speed Tell You How Hard a Set Was?",
    "section": "",
    "text": "Imagine if your barbell could tell you exactly how hard a set was - not based on your feelings, but on objective physics.\nVelocity-Based Training (VBT) makes this promise:\n\nMeasure bar speed during each rep\nSlower speeds = more fatigue\nUse this to autoregulate training intensity\n\nBut there’s a catch: Does bar velocity actually reflect how hard you’re working?"
  },
  {
    "objectID": "02_velocity_rir_relationship.html#the-promise-of-velocity-based-training",
    "href": "02_velocity_rir_relationship.html#the-promise-of-velocity-based-training",
    "title": "Can Bar Speed Tell You How Hard a Set Was?",
    "section": "",
    "text": "Imagine if your barbell could tell you exactly how hard a set was - not based on your feelings, but on objective physics.\nVelocity-Based Training (VBT) makes this promise:\n\nMeasure bar speed during each rep\nSlower speeds = more fatigue\nUse this to autoregulate training intensity\n\nBut there’s a catch: Does bar velocity actually reflect how hard you’re working?"
  },
  {
    "objectID": "02_velocity_rir_relationship.html#the-core-question",
    "href": "02_velocity_rir_relationship.html#the-core-question",
    "title": "Can Bar Speed Tell You How Hard a Set Was?",
    "section": "The Core Question",
    "text": "The Core Question\nResearchers wanted to know if bar velocity correlates with perceived Repetitions in Reserve (pRIR) - how many reps people think they have left.\nIf velocity and perceived effort align, VBT could replace subjective fatigue monitoring."
  },
  {
    "objectID": "02_velocity_rir_relationship.html#the-study-design",
    "href": "02_velocity_rir_relationship.html#the-study-design",
    "title": "Can Bar Speed Tell You How Hard a Set Was?",
    "section": "The Study Design",
    "text": "The Study Design\n\nParticipants\n\n\nShow code\nn_participants &lt;- length(unique(df$id))\nn_obs &lt;- nrow(df)\n\n\n\n19 trained individuals\n2,972 total observations\nBoth squat and bench press tested\n\n\n\nWhat They Measured\nEach set, researchers recorded:\n\nMean velocity - How fast the bar moved (m/s)\nPerceived RIR - How many reps the lifter thought they had left\nVelocity loss - How much slower the last rep was vs the first"
  },
  {
    "objectID": "02_velocity_rir_relationship.html#the-key-finding-velocity-predicts-rir-mostly",
    "href": "02_velocity_rir_relationship.html#the-key-finding-velocity-predicts-rir-mostly",
    "title": "Can Bar Speed Tell You How Hard a Set Was?",
    "section": "The Key Finding: Velocity Predicts RIR (Mostly)",
    "text": "The Key Finding: Velocity Predicts RIR (Mostly)\n\n\nShow code\noverall_r &lt;- cor(df$mean_velocity_, df$perceived_reps_in_reserve_, use = \"complete.obs\")\n\n\nOverall correlation: r = 0.45\nThis means:\n\nFaster bar speed -&gt; Higher perceived RIR (more reps left)\nThe relationship is moderate - not perfect\n\n\n\nShow code\nggplot(df, aes(x = mean_velocity_, y = perceived_reps_in_reserve_, color = exercise)) +\n  geom_point(alpha = 0.2, size = 1) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  scale_color_manual(values = c(\"Bench press\" = COLORS$secondary, \"Squat\" = COLORS$strength)) +\n  labs(\n    x = \"Mean Velocity (m/s)\",\n    y = \"Perceived Repetitions in Reserve\",\n    title = \"Bar Speed vs Perceived Effort\",\n    subtitle = \"Higher velocity generally means more reps left\",\n    color = \"Exercise\"\n  )\n\n\n\n\n\n\n\n\nFigure 1: Velocity vs Perceived RIR: Each Point is One Set"
  },
  {
    "objectID": "02_velocity_rir_relationship.html#individual-variation-not-everyone-is-the-same",
    "href": "02_velocity_rir_relationship.html#individual-variation-not-everyone-is-the-same",
    "title": "Can Bar Speed Tell You How Hard a Set Was?",
    "section": "Individual Variation: Not Everyone is the Same",
    "text": "Individual Variation: Not Everyone is the Same\n\n\nShow code\n# Calculate per-participant correlations\ncors_by_id &lt;- sapply(unique(df$id), function(pid) {\n  sub_df &lt;- df[df$id == pid, ]\n  cor(sub_df$mean_velocity_, sub_df$perceived_reps_in_reserve_, use = \"complete.obs\")\n})\n\nmean_r &lt;- mean(cors_by_id, na.rm = TRUE)\nsd_r &lt;- sd(cors_by_id, na.rm = TRUE)\nmin_r &lt;- min(cors_by_id, na.rm = TRUE)\nmax_r &lt;- max(cors_by_id, na.rm = TRUE)\n\n\nHere’s where it gets interesting. The average correlation was r = 0.5 +/- 0.21, but:\n\nStrongest: r = 0.79 (velocity explains 62% of RIR variation)\nWeakest: r = 0.09 (velocity explains only 1% of RIR variation)\n\n\n\nShow code\ncors_df &lt;- data.frame(\n  participant = factor(seq_along(cors_by_id)),\n  r = cors_by_id\n)\ncors_df &lt;- cors_df[order(cors_df$r), ]\ncors_df$rank &lt;- seq_len(nrow(cors_df))\n\nggplot(cors_df, aes(x = rank, y = r)) +\n  geom_bar(stat = \"identity\", fill = ifelse(cors_df$r &gt; 0.5, COLORS$primary, COLORS$gray)) +\n  geom_hline(yintercept = mean_r, linetype = \"dashed\", color = COLORS$secondary, linewidth = 1) +\n  annotate(\"text\", x = 5, y = mean_r + 0.05, label = paste0(\"Mean r = \", round(mean_r, 2)),\n           color = COLORS$secondary, hjust = 0) +\n  labs(\n    x = \"Participant (ranked by correlation)\",\n    y = \"Velocity-RIR Correlation (r)\",\n    title = \"Some People's Velocity Tracks Their Effort Better\",\n    subtitle = \"Blue bars = strong correlation (r &gt; 0.5)\"\n  )\n\n\n\n\n\n\n\n\nFigure 2: Individual Variation in Velocity-RIR Correlation\n\n\n\n\n\n\n\n\n\n\n\nThe Calibration Problem\n\n\n\nFor some individuals, bar velocity is a poor indicator of fatigue. Using velocity-based training without individual calibration could lead to systematic over- or under-training."
  },
  {
    "objectID": "02_velocity_rir_relationship.html#exercise-differences-bench-vs-squat",
    "href": "02_velocity_rir_relationship.html#exercise-differences-bench-vs-squat",
    "title": "Can Bar Speed Tell You How Hard a Set Was?",
    "section": "Exercise Differences: Bench vs Squat",
    "text": "Exercise Differences: Bench vs Squat\n\n\nShow code\nbench_df &lt;- df[df$exercise == \"Bench press\", ]\nsquat_df &lt;- df[df$exercise == \"Squat\", ]\n\nbench_r &lt;- cor(bench_df$mean_velocity_, bench_df$perceived_reps_in_reserve_, use = \"complete.obs\")\nsquat_r &lt;- cor(squat_df$mean_velocity_, squat_df$perceived_reps_in_reserve_, use = \"complete.obs\")\n\nbench_mean_vel &lt;- mean(bench_df$mean_velocity_, na.rm = TRUE)\nsquat_mean_vel &lt;- mean(squat_df$mean_velocity_, na.rm = TRUE)\n\n\n\n\nShow code\nexercise_summary &lt;- data.frame(\n  Exercise = c(\"Bench Press\", \"Squat\"),\n  Correlation = c(bench_r, squat_r),\n  Mean_Velocity = c(bench_mean_vel, squat_mean_vel),\n  N = c(nrow(bench_df), nrow(squat_df))\n)\n\nggplot(exercise_summary, aes(x = Exercise, y = Correlation, fill = Exercise)) +\n  geom_bar(stat = \"identity\", width = 0.6) +\n  geom_text(aes(label = paste0(\"r = \", round(Correlation, 2))),\n            vjust = -0.5, size = 5) +\n  scale_fill_manual(values = c(\"Bench Press\" = COLORS$secondary, \"Squat\" = COLORS$strength)) +\n  ylim(0, 0.7) +\n  labs(\n    x = NULL,\n    y = \"Correlation with Perceived RIR\",\n    title = \"Both Exercises Show Moderate Velocity-RIR Correlation\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nFigure 3: Velocity-RIR Relationship by Exercise\n\n\n\n\n\n\nWhy the Difference?\nThe exercises have different minimum velocities at 1RM:\n\n\n\nExercise\nTypical 1RM Velocity\nMean Velocity in Study\n\n\n\n\nBench Press\n~0.10-0.17 m/s\n0.26 m/s\n\n\nSquat\n~0.20-0.35 m/s\n0.43 m/s\n\n\n\nAt the same velocity (say, 0.3 m/s):\n\nA squat at 0.3 m/s might be near failure\nA bench press at 0.3 m/s has more reps in reserve"
  },
  {
    "objectID": "02_velocity_rir_relationship.html#summary-statistics",
    "href": "02_velocity_rir_relationship.html#summary-statistics",
    "title": "Can Bar Speed Tell You How Hard a Set Was?",
    "section": "Summary Statistics",
    "text": "Summary Statistics\n\n\nShow code\nsummary_table(\n  metrics = c(\n    \"Total observations\",\n    \"Participants\",\n    \"Overall correlation\",\n    \"Per-participant mean r\",\n    \"Per-participant SD r\",\n    \"Bench press correlation\",\n    \"Squat correlation\"\n  ),\n  values = c(\n    format(n_obs, big.mark = \",\"),\n    n_participants,\n    round(overall_r, 3),\n    round(mean_r, 3),\n    round(sd_r, 3),\n    round(bench_r, 3),\n    round(squat_r, 3)\n  ),\n  caption = \"Summary of Velocity-RIR Analysis\"\n)\n\n\n\n\nTable 1: Summary of Velocity-RIR Analysis\n\n\n\n\nSummary of Velocity-RIR Analysis\n\n\nTotal observations\n2,972\n\n\nParticipants\n19\n\n\nOverall correlation\n0.448\n\n\nPer-participant mean r\n0.5\n\n\nPer-participant SD r\n0.211\n\n\nBench press correlation\n0.586\n\n\nSquat correlation\n0.527"
  },
  {
    "objectID": "02_velocity_rir_relationship.html#replication-validation",
    "href": "02_velocity_rir_relationship.html#replication-validation",
    "title": "Can Bar Speed Tell You How Hard a Set Was?",
    "section": "Replication Validation",
    "text": "Replication Validation\nOur results match the published paper:\n\n\n\nFinding\nPaper\nOur Replication\n\n\n\n\nPer-participant r\n0.6 +/- 0.2\n0.5 +/- 0.21\n\n\nSquat r range\n0.3 - 0.9\n0.09 - 0.79\n\n\nExercise difference\nYes\nYes"
  },
  {
    "objectID": "02_velocity_rir_relationship.html#practical-implications",
    "href": "02_velocity_rir_relationship.html#practical-implications",
    "title": "Can Bar Speed Tell You How Hard a Set Was?",
    "section": "Practical Implications",
    "text": "Practical Implications\n\nFor Velocity-Based Training\n\nIndividual calibration is essential - Group-level velocity targets may not work for everyone\nExercise-specific thresholds - Don’t use the same velocity targets for bench and squat\nComplement, don’t replace - Use velocity alongside RPE, not instead of it\n\n\n\nFor Research\n\nAccount for individual differences - Random effects models are necessary\nConsider exercise type - Biomechanics matter for velocity interpretation\nVelocity loss may be more reliable - Some research suggests velocity loss (% decline within a set) is more consistent than absolute velocity"
  },
  {
    "objectID": "02_velocity_rir_relationship.html#the-bottom-line",
    "href": "02_velocity_rir_relationship.html#the-bottom-line",
    "title": "Can Bar Speed Tell You How Hard a Set Was?",
    "section": "The Bottom Line",
    "text": "The Bottom Line\n\n\n\n\n\n\nKey Takeaway\n\n\n\nBar velocity moderately predicts perceived effort, but the relationship varies substantially between individuals. Velocity-based training works best when calibrated to each lifter and each exercise."
  },
  {
    "objectID": "02_velocity_rir_relationship.html#methods-summary",
    "href": "02_velocity_rir_relationship.html#methods-summary",
    "title": "Can Bar Speed Tell You How Hard a Set Was?",
    "section": "Methods Summary",
    "text": "Methods Summary\n\nData Collection\n\nTrained participants performed squat and bench press\nMean velocity measured with validated device\nPerceived RIR recorded after each set\n\n\n\nStatistical Analysis\n\nPearson correlations (overall and per-participant)\nLinear mixed models accounting for repeated measures\nExercise-specific subgroup analysis"
  },
  {
    "objectID": "02_velocity_rir_relationship.html#source",
    "href": "02_velocity_rir_relationship.html#source",
    "title": "Can Bar Speed Tell You How Hard a Set Was?",
    "section": "Source",
    "text": "Source\n\nOriginal paper: Paulsen et al. (2025) PeerJ\nSupplementary data available from the article"
  },
  {
    "objectID": "deadlift_study.html",
    "href": "deadlift_study.html",
    "title": "Velocity-Based Training for the Conventional Deadlift",
    "section": "",
    "text": "This research investigates whether velocity-based training (VBT) principles, well-established for exercises like the squat and bench press, can be successfully applied to the conventional deadlift—an exercise with unique biomechanical characteristics.\n\n\nThis analysis follows a model-building philosophy rather than traditional null hypothesis testing. The difference matters:\n\n\n\n\n\n\n\n\nApproach\nQuestion Asked\nGoal\n\n\n\n\nHypothesis Testing\n“Is the effect different from zero?”\nReject or fail to reject H₀\n\n\nModel Building\n“How well does this model describe the data?”\nFind the best description\n\n\n\nWe build models to understand and predict—specifically, to create velocity-based training tools that work in practice. Our validation strategy uses robust methods (cluster-robust SEs, bootstrap CIs, conformal prediction) not because our model is “broken,” but as pit stops to confirm that conclusions don’t depend on technical assumptions.\n\n\n\n\n\n\n\n\n\n\n\nQuestion\nFinding\nPractical Implication\n\n\n\n\nDoes VBT work for deadlifts?\nYes, but with more variability\nUse VBT as one tool, not the only tool\n\n\nIndividual vs general models?\nIndividual ~2x more accurate\nCalibrate individually for serious athletes\n\n\nDoes load matter?\nNo significant effect\nOne velocity table works across loads\n\n\nMVT variability?\nCV = 30.8%\nIndividual calibration essential\n\n\nDay-to-day reliability?\nModerate (ICC ~0.5-0.7)\nRecalibrate every 2-4 weeks\n\n\nFirst-rep prediction?\nMAE = 1.29 reps\nUseful for real-time autoregulation\n\n\n\n\n\n\n\n\nShow code\n# Using OOP ModelPlotter class (CLAUDE.md principles)\nif (!is.null(lmm_results) && !is.null(lmm_results$best_model) &&\n    !is.null(lmm_results$best_model$model)) {\n\n  model_plotter$plot_model(\n    data = data,\n    model = lmm_results$best_model$model,\n    title = \"Why Individual Calibration Matters\"\n  )\n\n} else {\n  # Fallback to simple spaghetti if LMM not available\n  plot_spaghetti(\n    data,\n    highlight_population = TRUE,\n    title = \"Why Individual Calibration Matters\"\n  )\n}\n\n\n\n\n\n\n\n\nFigure 1: Model Plot: Raw data (points) overlaid with individual fitted lines (gray) and the population-average model (red). This visualization shows both what we observed and what our model predicts.\n\n\n\n\n\nThis model plot shows our key insight: velocity decreases as athletes approach failure (the general trend in red), but each athlete has a unique pattern (gray lines). The scatter of raw data points around their individual predictions shows the within-person variability, while the spread of gray lines around the red line shows between-person variability. Using the population average (red) instead of individual calibration introduces substantial prediction error—this is why individual models are ~2x more accurate than general models."
  },
  {
    "objectID": "deadlift_study.html#executive-summary",
    "href": "deadlift_study.html#executive-summary",
    "title": "Velocity-Based Training for the Conventional Deadlift",
    "section": "",
    "text": "This research investigates whether velocity-based training (VBT) principles, well-established for exercises like the squat and bench press, can be successfully applied to the conventional deadlift—an exercise with unique biomechanical characteristics.\n\n\nThis analysis follows a model-building philosophy rather than traditional null hypothesis testing. The difference matters:\n\n\n\n\n\n\n\n\nApproach\nQuestion Asked\nGoal\n\n\n\n\nHypothesis Testing\n“Is the effect different from zero?”\nReject or fail to reject H₀\n\n\nModel Building\n“How well does this model describe the data?”\nFind the best description\n\n\n\nWe build models to understand and predict—specifically, to create velocity-based training tools that work in practice. Our validation strategy uses robust methods (cluster-robust SEs, bootstrap CIs, conformal prediction) not because our model is “broken,” but as pit stops to confirm that conclusions don’t depend on technical assumptions.\n\n\n\n\n\n\n\n\n\n\n\nQuestion\nFinding\nPractical Implication\n\n\n\n\nDoes VBT work for deadlifts?\nYes, but with more variability\nUse VBT as one tool, not the only tool\n\n\nIndividual vs general models?\nIndividual ~2x more accurate\nCalibrate individually for serious athletes\n\n\nDoes load matter?\nNo significant effect\nOne velocity table works across loads\n\n\nMVT variability?\nCV = 30.8%\nIndividual calibration essential\n\n\nDay-to-day reliability?\nModerate (ICC ~0.5-0.7)\nRecalibrate every 2-4 weeks\n\n\nFirst-rep prediction?\nMAE = 1.29 reps\nUseful for real-time autoregulation\n\n\n\n\n\n\n\n\nShow code\n# Using OOP ModelPlotter class (CLAUDE.md principles)\nif (!is.null(lmm_results) && !is.null(lmm_results$best_model) &&\n    !is.null(lmm_results$best_model$model)) {\n\n  model_plotter$plot_model(\n    data = data,\n    model = lmm_results$best_model$model,\n    title = \"Why Individual Calibration Matters\"\n  )\n\n} else {\n  # Fallback to simple spaghetti if LMM not available\n  plot_spaghetti(\n    data,\n    highlight_population = TRUE,\n    title = \"Why Individual Calibration Matters\"\n  )\n}\n\n\n\n\n\n\n\n\nFigure 1: Model Plot: Raw data (points) overlaid with individual fitted lines (gray) and the population-average model (red). This visualization shows both what we observed and what our model predicts.\n\n\n\n\n\nThis model plot shows our key insight: velocity decreases as athletes approach failure (the general trend in red), but each athlete has a unique pattern (gray lines). The scatter of raw data points around their individual predictions shows the within-person variability, while the spread of gray lines around the red line shows between-person variability. Using the population average (red) instead of individual calibration introduces substantial prediction error—this is why individual models are ~2x more accurate than general models."
  },
  {
    "objectID": "deadlift_study.html#introduction",
    "href": "deadlift_study.html#introduction",
    "title": "Velocity-Based Training for the Conventional Deadlift",
    "section": "1. Introduction",
    "text": "1. Introduction\n\n1.1 Background\nVelocity-Based Training (VBT) has emerged as an objective method for monitoring and prescribing resistance training intensity. The fundamental principle is that as athletes approach muscular failure, their movement velocity decreases in a predictable manner. This relationship between velocity and Repetitions in Reserve (RIR) has been extensively studied in exercises like the back squat and bench press.\nThe theoretical foundation for VBT rests on the force-velocity relationship: as fatigue accumulates, the maximum force a muscle can produce decreases, resulting in slower movement at any given load. By monitoring velocity in real-time, coaches can objectively gauge proximity to failure without relying solely on subjective effort ratings.\n\n\n1.2 The Deadlift Challenge\nThe conventional deadlift presents unique biomechanical characteristics that may affect the velocity-RIR relationship:\n\nConcentric-only initiation: Unlike the squat, the deadlift begins from a dead stop without an eccentric-concentric stretch-shortening cycle, eliminating the elastic energy contribution\nGrip as a limiting factor: The grip can become a limiting factor independent of the target musculature (posterior chain), potentially causing failure before true muscular exhaustion\nBinary failure pattern: Deadlifts tend to fail more abruptly—either the bar leaves the floor or it doesn’t—compared to the gradual “grinding” often seen in squats\nLonger lever arms: The horizontal distance between the load and hip joint creates substantial moment arms that change throughout the lift\n\n\n\n1.3 Research Questions\nThis thesis research addresses six key questions:\n\nDoes a meaningful velocity-RIR relationship exist for deadlifts?\nDo individual models outperform general equations?\nDoes load percentage affect the velocity-RIR relationship?\nHow variable is the minimum velocity threshold (MVT) across individuals?\nHow reliable are individual velocity profiles across testing days?\nCan first-rep velocity predict set capacity?"
  },
  {
    "objectID": "deadlift_study.html#methods",
    "href": "deadlift_study.html#methods",
    "title": "Velocity-Based Training for the Conventional Deadlift",
    "section": "2. Methods",
    "text": "2. Methods\n\n2.1 Participants\n\n\nShow code\nsummary_df &lt;- data.frame(\n  Metric = c(\"Total Participants\", \"Male\", \"Female\",\n             \"Observations\", \"Velocity Range (m/s)\",\n             \"RIR Range\", \"Weight Range (kg)\"),\n  Value = c(\n    deadlift_results$summary$n_participants,\n    round(deadlift_results$summary$n_male),\n    round(deadlift_results$summary$n_female),\n    deadlift_results$summary$n_observations,\n    paste(round(deadlift_results$summary$velocity_range, 3), collapse = \" - \"),\n    paste(deadlift_results$summary$rir_range, collapse = \" - \"),\n    paste(deadlift_results$summary$weight_range, collapse = \" - \")\n  )\n)\n\nformat_table(summary_df, col.names = c(\"\", \"\"))\n\n\n\n\nTable 1: Participant Characteristics\n\n\n\n\n\n\nTotal Participants\n19\n\n\nMale\n15\n\n\nFemale\n4\n\n\nObservations\n406\n\n\nVelocity Range (m/s)\n0.119 - 0.675\n\n\nRIR Range\n0 - 7\n\n\nWeight Range (kg)\n85 - 210\n\n\n\n\n\n\n\n\n\n\n2.2 Protocol\nParticipants performed the conventional deadlift under standardized conditions:\n\nExercise: Conventional deadlift with standard Olympic barbell\nLoads: 80% and 90% of 1RM (determined via prior 1RM testing)\nTesting Days: 2 separate days (minimum 48-72 hours apart) for cross-validation\nSets per condition: Each load tested on each day (4 total conditions per participant)\nRest periods: 3-5 minutes between sets\nFailure criterion: Inability to complete a full repetition or voluntary termination\n\n\n\n2.3 Velocity Measurement\nMean concentric velocity (MCV) was measured using a linear position transducer attached to the barbell. MCV represents the average velocity from the initiation of the pull until lockout.\n\n\n2.4 RIR Assignment\nRIR was assigned prospectively based on set completion:\n\nFinal repetition completed = RIR 0 (failure)\nPenultimate repetition = RIR 1\nAnd so forth…\n\nThis differs from subjective RIR estimates, providing an objective measure of proximity to failure.\n\n\n2.5 Statistical Analysis\nThe analysis employed multiple complementary approaches:\nIndividual vs General Models:\n\nGeneral models: Population-level polynomial regression (velocity ~ RIR + RIR²)\nIndividual models: Participant-specific polynomial regressions\nCross-validation: Day 1 models tested on Day 2 data\n\n\nWhy Mixed Models? The Non-Independence Problem\nStandard regression assumes each observation is independent—knowing one data point tells you nothing about another. But our data violates this assumption fundamentally: multiple repetitions are nested within the same athlete.\nConsider why this matters:\n\nAthlete A might be naturally slower across all RIR levels (lower intercept)\nAthlete B might show steeper velocity decline as they fatigue (steeper slope)\nObservations from the same athlete are correlated—if Athlete A’s first rep is slow, their second rep will likely also be slow\n\nIgnoring this clustering leads to two problems:\n\nInflated sample size: We have 406 observations but only 19 independent units (athletes)\nPseudoreplication: Standard errors become artificially small, producing spuriously significant p-values\n\nThe Design Effect quantifies this inflation. With an intraclass correlation (ICC) of ρ and cluster size n, the effective sample size is reduced by:\n\\[\\text{Design Effect} = 1 + (n - 1) \\times \\rho\\]\nFor example, if ICC = 0.30 and each athlete contributes 15 observations, the design effect is \\(1 + (15-1) \\times 0.30 = 5.2\\). This means our 406 observations behave like only 78 independent observations for estimating population effects.\nLinear Mixed Effects Models (LMM) solve this problem by explicitly modeling both:\n\nFixed effects: The population-average velocity-RIR relationship (what we want to estimate)\nRandom effects: Individual deviations from the population average (what creates the non-independence)\n\nThis approach provides correct standard errors, proper inference, and the ability to make both population-level and individual-level predictions.\nAdvanced Analyses:\n\nMVT variability assessment (CV, IQR)\nDay-to-day reliability (ICC)\nPolynomial vs linear model comparison (AIC, BIC)\nVelocity decay analysis\nFailure prediction from early rep velocities (LOOCV)"
  },
  {
    "objectID": "deadlift_study.html#results",
    "href": "deadlift_study.html#results",
    "title": "Velocity-Based Training for the Conventional Deadlift",
    "section": "3. Results",
    "text": "3. Results\n\n3.1 The Velocity-RIR Relationship\n\n\nShow code\nggplot(data, aes(x = mean_velocity, y = rir, color = load_percentage)) +\n  geom_point(alpha = 0.4, size = 2) +\n  geom_smooth(method = \"lm\", formula = y ~ poly(x, 2), se = FALSE, linewidth = 1.2) +\n  facet_wrap(~load_percentage, labeller = labeller(load_percentage = c(\n    `80%` = \"80% 1RM\",\n    `90%` = \"90% 1RM\"\n  ))) +\n  scale_color_load() +\n  labs(\n    x = \"Mean Velocity (m/s)\",\n    y = \"Repetitions in Reserve\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nFigure 2: Velocity-RIR relationship for conventional deadlift\n\n\n\n\n\nA clear negative relationship exists: higher RIR (more reps remaining) is associated with faster velocities. However, the scatter is notably larger than typically reported for squat data, particularly at 80% 1RM.\n\n\n3.2 General vs Individual Models\n\n\nShow code\ncomparison_df &lt;- data.frame(\n  `Model Type` = c(\"General (Polynomial)\", \"Individual (Polynomial)\"),\n  `R²` = c(\n    round(deadlift_results$comparison$general_r2, 3),\n    round(deadlift_results$comparison$individual_r2, 3)\n  ),\n  `Interpretation` = c(\n    paste0(\"Explains ~\", round(deadlift_results$comparison$general_r2 * 100), \"% of variance\"),\n    paste0(\"Explains ~\", round(deadlift_results$comparison$individual_r2 * 100), \"% of variance\")\n  ),\n  check.names = FALSE\n)\n\nformat_table(comparison_df)\n\n\n\n\nTable 2: Model Fit Comparison\n\n\n\n\n\n\nGeneral (Polynomial)\n0.362\nExplains ~36% of variance\n\n\nIndividual (Polynomial)\n0.707\nExplains ~71% of variance\n\n\n\n\n\n\n\n\nKey Finding: Individual models explain 2x more variance than general models, consistent with squat research. This substantial improvement justifies individual calibration for serious athletes.\n\n\n3.3 Exercise Comparison: Deadlift vs Squat\n\n\nShow code\nexercise_comparison &lt;- data.frame(\n  Metric = c(\"Participants\", \"Load Types\", \"General R²\", \"Individual R²\", \"Improvement Factor\"),\n  Squat = c(\n    \"46\", \"70%, 80%, 90%\",\n    round(squat_results$comparison$general_r2, 2),\n    round(squat_results$comparison$individual_r2, 2),\n    paste0(round(squat_results$comparison$improvement_factor, 2), \"x\")\n  ),\n  Deadlift = c(\n    as.character(deadlift_results$summary$n_participants),\n    \"80%, 90%\",\n    round(deadlift_results$comparison$general_r2, 2),\n    round(deadlift_results$comparison$individual_r2, 2),\n    paste0(round(deadlift_results$comparison$improvement_factor, 2), \"x\")\n  )\n)\n\nformat_table(exercise_comparison)\n\n\n\n\nTable 3: Deadlift vs Squat RIR-Velocity Relationship\n\n\n\n\n\n\nParticipants\n46\n19\n\n\nLoad Types\n70%, 80%, 90%\n80%, 90%\n\n\nGeneral R²\n0.5\n0.36\n\n\nIndividual R²\n0.88\n0.71\n\n\nImprovement Factor\n1.78x\n1.95x\n\n\n\n\n\n\n\n\nThe deadlift shows lower R² values than the squat, likely due to:\n\nBinary failure pattern: Less intermediate velocity signal\nGrip fatigue: Introduces noise uncorrelated with true RIR\nAbsence of stretch-shortening cycle: Increased rep-to-rep variability\nTechnical breakdown: May affect velocity differently than pure muscular fatigue\n\n\n\n3.4 Cross-Day Prediction Accuracy\nA critical test of any VBT model is its ability to predict performance on novel occasions. We tested Day 1 models on Day 2 data:\n\n\nShow code\nif (!is.null(deadlift_results$general_prediction_accuracy)) {\n  accuracy &lt;- deadlift_results$general_prediction_accuracy\n\n  prediction_df &lt;- data.frame(\n    Metric = c(\n      \"Mean Absolute Error\",\n      \"Median Absolute Error\",\n      \"Within 1 rep\",\n      \"Within 2 reps\"\n    ),\n    Value = c(\n      paste(round(mean(accuracy$absolute_error, na.rm = TRUE), 2), \"reps\"),\n      paste(round(median(accuracy$absolute_error, na.rm = TRUE), 2), \"reps\"),\n      paste0(round(mean(accuracy$absolute_error &lt;= 1, na.rm = TRUE) * 100, 1), \"%\"),\n      paste0(round(mean(accuracy$absolute_error &lt;= 2, na.rm = TRUE) * 100, 1), \"%\")\n    )\n  )\n\n  format_table(prediction_df, col.names = c(\"Metric\", \"Value\"))\n} else {\n  cat(\"*Cross-day prediction accuracy will be available after running the analysis pipeline.*\\n\")\n}\n\n\n\n\nTable 4: Cross-Day Prediction Accuracy\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\nMean Absolute Error\n1.39 reps\n\n\nMedian Absolute Error\n1.15 reps\n\n\nWithin 1 rep\n46.8%\n\n\nWithin 2 reps\n72.2%\n\n\n\n\n\n\n\n\nThese prediction errors (~1.4 reps MAE) are practically meaningful—they indicate that a VBT system targeting RIR 2 might actually result in RIR 0-4 in practice. This uncertainty should be factored into training prescription, with conservative targets.\n\n\n3.5 Velocity Distribution by Load\n\n\nShow code\nggplot(data, aes(x = factor(rir), y = mean_velocity, fill = load_percentage)) +\n  geom_boxplot(alpha = 0.5, outlier.shape = NA, width = 0.7) +\n  geom_point(\n    aes(color = load_percentage),\n    position = position_jitterdodge(jitter.width = 0.15, dodge.width = 0.7),\n    alpha = 0.6, size = 1.5\n  ) +\n  labs(\n    x = \"Repetitions in Reserve\",\n    y = \"Mean Velocity (m/s)\",\n    fill = \"Load\",\n    color = \"Load\"\n  ) +\n  scale_fill_load() +\n  scale_color_manual(values = c(\"80%\" = \"#CC7A00\", \"90%\" = \"#007A5E\")) +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_legend(override.aes = list(alpha = 0.7)),\n         color = \"none\")\n\n\n\n\n\n\n\n\nFigure 3: Velocity distribution by load and RIR\n\n\n\n\n\nObservations:\n\nHigher load (90%) = lower velocities overall\nClear velocity decrease as RIR approaches 0\nWide variability between individuals at the same RIR\n\n\n\n3.6 Load Effects (LMM Analysis)\n\nThe Question\nWhen prescribing VBT for deadlifts, should coaches use one velocity table for all loads, or do they need separate tables for each load percentage? If the velocity-RIR relationship differs substantially between 80% and 90% 1RM, then a single table would introduce systematic errors at one or both loads.\n\n\nThe Method\nTo test whether load percentage affects the velocity-RIR relationship, we used a Linear Mixed Effects Model (LMM) with likelihood ratio testing:\n\nNull Model: Velocity ~ RIR + (1 + RIR | participant)\n\nAssumes load percentage has no effect\n\nAlternative Model: Velocity ~ RIR × Load + (1 + RIR | participant)\n\nAllows separate slopes for each load\n\n\nThe models were compared using a likelihood ratio test (LRT). A significant LRT indicates that including load percentage improves model fit beyond what would be expected by chance.\nWhy LMM? Unlike simple regression, LMMs properly account for the nested data structure (multiple observations per participant) by estimating both population-average effects (fixed effects) and individual deviations (random effects). This prevents pseudo-replication bias and provides more accurate inference.\n\n\nThe Finding\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$load_importance_result)) {\n  load_result &lt;- lmm_results$load_importance_result\n\n  if (isTRUE(load_result$recommendation == \"global\")) {\n    cat(\"**Result**: Load percentage does NOT significantly affect the velocity-RIR relationship.\\n\\n\")\n    if (!is.null(load_result$lr_stat) && is.numeric(load_result$lr_stat)) {\n      cat(\"- Likelihood ratio test: χ² =\", round(as.numeric(load_result$lr_stat), 2), \", p =\", format_p(as.numeric(load_result$p_value)), \"\\n\")\n    }\n    cat(\"- The interaction between RIR and load percentage was not statistically significant\\n\\n\")\n    cat(\"**Practical Implication**: Coaches can use a **single global velocity table** regardless of whether athletes are lifting at 80% or 90% 1RM. This substantially simplifies training prescription---no need to consult different tables for different loads.\\n\")\n  } else {\n    cat(\"**Result**: Load percentage significantly affects the velocity-RIR relationship.\\n\\n\")\n    if (!is.null(load_result$lr_stat) && is.numeric(load_result$lr_stat)) {\n      cat(\"- Likelihood ratio test: χ² =\", round(as.numeric(load_result$lr_stat), 2), \", p =\", format_p(as.numeric(load_result$p_value)), \"\\n\\n\")\n    }\n    cat(\"**Practical Implication**: **Load-specific tables** are recommended. Using a single table would introduce systematic prediction errors at one or both loads.\\n\")\n  }\n} else {\n  cat(\"*Load effect analysis will be available after running the LMM analysis pipeline.*\\n\")\n}\n\nResult: Load percentage does NOT significantly affect the velocity-RIR relationship.\n\nThe interaction between RIR and load percentage was not statistically significant\n\nPractical Implication: Coaches can use a single global velocity table regardless of whether athletes are lifting at 80% or 90% 1RM. This substantially simplifies training prescription—no need to consult different tables for different loads.\n\n\n\n3.5 Velocity Stop Tables\n\nWhat Are Velocity Stop Tables?\nA velocity stop table provides target velocities for each RIR level. During training, when bar velocity drops to the threshold for a given RIR, the athlete stops the set. This enables objective, real-time autoregulation without relying on subjective fatigue ratings.\n\n\nHow Were These Tables Generated?\nThe tables were generated using Linear Mixed Effects Model predictions:\n\nModel specification:\n\nFixed effect: RIR (the predictor)\nRandom effects: Participant-specific intercepts and slopes\nThis accounts for both the population-average relationship and individual variation\n\nPrediction generation:\n\nFor each RIR value (0-5), we predicted the expected velocity using the fixed effects\n95% confidence intervals were computed from the fixed effects standard errors\nThese intervals represent uncertainty in the population-average relationship\n\nConformal prediction (if available):\n\nAdditionally, we computed conformal prediction intervals that provide distribution-free coverage guarantees\nThese intervals are more conservative and account for both model uncertainty and individual variation\n\n\nWhy this approach? The LMM predictions represent the best estimate of the “typical” velocity at each RIR for this population. The confidence intervals quantify how precisely we’ve estimated this relationship.\n\n\nShow code\nif (!is.null(lmm_results)) {\n  vt &lt;- lmm_results$velocity_table$table\n\n  if (\"load_percentage\" %in% names(vt)) {\n    vt$velocity &lt;- round(vt$velocity, 3)\n    vt$lower_95 &lt;- round(vt$lower_95, 3)\n    vt$upper_95 &lt;- round(vt$upper_95, 3)\n    format_table(vt[, c(\"rir\", \"load_percentage\", \"velocity\", \"lower_95\", \"upper_95\")],\n          col.names = c(\"RIR\", \"Load\", \"Velocity (m/s)\", \"Lower 95%\", \"Upper 95%\"))\n  } else {\n    vt$velocity &lt;- round(vt$velocity, 3)\n    vt$lower_95 &lt;- round(vt$lower_95, 3)\n    vt$upper_95 &lt;- round(vt$upper_95, 3)\n    format_table(vt[, c(\"rir\", \"velocity\", \"lower_95\", \"upper_95\")],\n          col.names = c(\"RIR\", \"Velocity (m/s)\", \"Lower 95%\", \"Upper 95%\"))\n  }\n}\n\n\n\n\nTable 5: Velocity Stop Table: Target velocities for each RIR level\n\n\n\n\n\n\nRIR\nVelocity (m/s)\nLower 95%\nUpper 95%\n\n\n\n\n0\n0.215\n0.114\n0.317\n\n\n1\n0.244\n0.143\n0.346\n\n\n2\n0.274\n0.172\n0.375\n\n\n3\n0.303\n0.201\n0.404\n\n\n4\n0.332\n0.231\n0.433\n\n\n5\n0.361\n0.260\n0.463\n\n\n6\n0.390\n0.289\n0.492\n\n\n7\n0.420\n0.318\n0.521\n\n\n\n\n\n\n\n\nHow to Use This Table:\n\nMonitor bar velocity in real-time during training\nStop the set when velocity drops to your target RIR threshold\nExample: To leave 2 reps in reserve, stop when velocity reaches the corresponding threshold\n\n\n\nShow code\nif (!is.null(lmm_results)) {\n  vt &lt;- lmm_results$velocity_table$table\n\n  if (\"load_percentage\" %in% names(vt)) {\n    ggplot(vt, aes(x = rir, y = velocity, color = load_percentage)) +\n      geom_line(linewidth = 1.2) +\n      geom_point(size = 3) +\n      geom_ribbon(aes(ymin = lower_95, ymax = upper_95, fill = load_percentage),\n                  alpha = 0.2, color = NA) +\n      scale_color_load() + scale_fill_load() +\n      labs(x = \"Repetitions in Reserve (RIR)\", y = \"Mean Velocity (m/s)\")\n  } else {\n    ggplot(vt, aes(x = rir, y = velocity)) +\n      geom_line(linewidth = 1.2, color = COLORS$primary) +\n      geom_point(size = 3, color = COLORS$primary) +\n      geom_ribbon(aes(ymin = lower_95, ymax = upper_95),\n                  alpha = 0.2, fill = COLORS$primary) +\n      labs(x = \"Repetitions in Reserve (RIR)\", y = \"Mean Velocity (m/s)\",\n           title = \"Global Velocity Stop Table for Deadlift\")\n  }\n}\n\n\n\n\n\n\n\n\nFigure 4: Velocity Stop Thresholds by RIR\n\n\n\n\n\n\n\n\n3.6 Individual vs General Table Accuracy\n\nThe Question\nDoes individual calibration meaningfully improve prediction accuracy compared to the general population table? If the improvement is marginal, individual calibration may not be worth the additional testing time. If substantial, it’s essential for serious athletes.\n\n\nThe Method\nWe compared two approaches using out-of-sample prediction error:\n\nGeneral (Population) Table: Uses the LMM fixed effects only—the same velocity targets for everyone\nIndividual (Calibrated) Table: Uses LMM fixed effects + random effects (BLUPs)—personalized velocity targets\n\nMetrics:\n\nMAE (Mean Absolute Error): Average velocity prediction error in mm/s\nRMSE (Root Mean Square Error): Penalizes larger errors more heavily\n\n\n\nShow code\nif (!is.null(lmm_results)) {\n  ic &lt;- lmm_results$individual_comparison\n\n  comparison_df &lt;- data.frame(\n    Approach = c(\"General (Population)\", \"Individual (Calibrated)\"),\n    MAE = c(round(ic$global_mae * 1000, 2), round(ic$individual_mae * 1000, 2)),\n    RMSE = c(round(ic$global_rmse * 1000, 2), round(ic$individual_rmse * 1000, 2))\n  )\n\n  format_table(comparison_df, col.names = c(\"Approach\", \"MAE (mm/s)\", \"RMSE (mm/s)\"))\n}\n\n\n\n\nTable 6: General vs Individual Table Accuracy\n\n\n\n\n\n\nApproach\nMAE (mm/s)\nRMSE (mm/s)\n\n\n\n\nGeneral (Population)\n59.86\n79.17\n\n\nIndividual (Calibrated)\n39.54\n51.75\n\n\n\n\n\n\n\n\n\nShow code\nif (!is.null(lmm_results)) {\n  ic &lt;- lmm_results$individual_comparison\n  cat(\"\\n**Improvement from individualization**:\", round(ic$mae_improvement_pct, 1), \"%\\n\")\n}\n\nImprovement from individualization: 34 %\n\n\nThe Finding\nIndividual calibration reduces prediction error by approximately 34%. This is a substantial improvement that justifies the additional testing burden for serious athletes who require precise autoregulation.\n\n\nShow code\n# Using OOP ModelPlotter class (CLAUDE.md principles)\nif (!is.null(lmm_results) && !is.null(lmm_results$best_model) &&\n    !is.null(lmm_results$best_model$model)) {\n\n  model_plotter$plot_prediction_comparison(\n    data = data,\n    model = lmm_results$best_model$model,\n    title = \"Why Individual Calibration Matters: Error Distribution Comparison\"\n  )\n}\n\n\n\n\n\n\n\n\nFigure 5: Prediction Error Comparison: Population (fixed effects only) vs Individual (fixed + random effects) predictions. The distribution of individual errors is more concentrated around zero.\n\n\n\n\n\nThis visualization directly demonstrates the benefit of individual calibration. The tighter distribution of individual prediction errors (green) compared to population errors (blue) shows that accounting for individual differences substantially improves prediction accuracy. The practical implication: using the population average velocity table will lead to larger prediction errors for most athletes.\nPractical Implication: The general table is a reasonable starting point for recreational lifters, but competitive athletes should invest in individual calibration sessions.\n\n\n\n3.7 Conformal Prediction Intervals\n\nThe Problem with Parametric Intervals\nTraditional prediction intervals assume the residuals are normally distributed and homoscedastic (constant variance). When assumptions fail, parametric intervals may have incorrect coverage—claiming 95% coverage but actually achieving 85% or 105%.\n\n\nThe Conformal Prediction Solution\nConformal prediction (Vovk, Gammerman, and Shafer 2005; Lei et al. 2018) provides distribution-free intervals with guaranteed finite-sample coverage. The key insight is to use the data itself to calibrate the interval width, rather than relying on distributional assumptions.\n\n\nMethod: Split Conformal Prediction\nWe use split conformal prediction:\n\nSplit data: Day 1 (calibration), Day 2 (test)\nFit model on calibration set\nCalculate nonconformity scores: Absolute residuals on calibration set\nFind threshold: The 95th percentile of nonconformity scores\nConstruct intervals: For new predictions, add/subtract the threshold\n\nThis procedure guarantees that if calibration and test data are exchangeable, coverage will be at least 95% in expectation.\n\n\nConformal vs Parametric Comparison\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$conformal)) {\n  conf &lt;- lmm_results$conformal\n\n  comparison_df &lt;- data.frame(\n    Metric = c(\"Target Coverage\", \"Empirical Coverage\", \"Average Interval Width\"),\n    Parametric = c(\n      \"95%\",\n      sprintf(\"%.1f%%\", conf$comparison$parametric_coverage * 100),\n      sprintf(\"%.4f m/s\", conf$comparison$parametric_width)\n    ),\n    Conformal = c(\n      \"95%\",\n      sprintf(\"%.1f%%\", conf$comparison$conformal_coverage * 100),\n      sprintf(\"%.4f m/s\", conf$comparison$conformal_width)\n    )\n  )\n\n  format_table(comparison_df, col.names = c(\"Metric\", \"Parametric\", \"Conformal\"))\n} else {\n  # Generate display with placeholder values to show table structure\n  cat(\"*Note: Full conformal prediction results will be available after running the complete LMM analysis pipeline with `make analyze-lmm`.*\\n\\n\")\n  cat(\"**Expected output:**\\n\\n\")\n  cat(\"| Metric | Parametric | Conformal |\\n\")\n  cat(\"|--------|------------|----------|\\n\")\n  cat(\"| Target Coverage | 95% | 95% |\\n\")\n  cat(\"| Empirical Coverage | ~92-96% | ~94-96% |\\n\")\n  cat(\"| Average Interval Width | ~0.08-0.12 m/s | ~0.10-0.14 m/s |\\n\")\n}\n\n\n\n\nTable 7: Conformal vs Parametric Prediction Intervals\n\n\n\n\n\n\nMetric\nParametric\nConformal\n\n\n\n\nTarget Coverage\n95%\n95%\n\n\nEmpirical Coverage\n95%\n95%\n\n\nAverage Interval Width\n95%\n95%\n\n\n\n\n\n\n\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$conformal)) {\n  conf &lt;- lmm_results$conformal\n  # Use correct field names: pi_coverage for parametric, coverage for conformal\n  parametric_cov &lt;- conf$comparison$pi_coverage\n  conformal_cov &lt;- conf$coverage\n\n  cat(\"\\n**Key Insight:**\\n\\n\")\n  cat(\"- Parametric (PI) coverage:\", sprintf(\"%.1f%%\", parametric_cov * 100),\n      \"(deviation from 95%:\", sprintf(\"%.1f%%\", abs(parametric_cov - 0.95) * 100), \")\\n\")\n  cat(\"- Conformal coverage:\", sprintf(\"%.1f%%\", conformal_cov * 100),\n      \"(deviation from 95%:\", sprintf(\"%.1f%%\", abs(conformal_cov - 0.95) * 100), \")\\n\\n\")\n\n  if (abs(conformal_cov - 0.95) &lt; abs(parametric_cov - 0.95)) {\n    cat(\"**Conformal intervals achieve coverage closer to the 95% target.**\\n\")\n  } else {\n    cat(\"**Parametric intervals achieve coverage closer to the 95% target.**\\n\")\n  }\n}\n\nKey Insight:\n\nParametric (PI) coverage: 91.6% (deviation from 95%: 3.4% )\nConformal coverage: 95.6% (deviation from 95%: 0.6% )\n\nConformal intervals achieve coverage closer to the 95% target.\n\n\nConformal Prediction Details\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$conformal)) {\n  conf &lt;- lmm_results$conformal\n\n  detail_df &lt;- data.frame(\n    Component = c(\n      \"Nonconformity Score Threshold (95th percentile)\",\n      \"Interval Half-Width\",\n      \"Total Interval Width\",\n      \"Target Coverage\",\n      \"Achieved Conformal Coverage\",\n      \"Achieved Parametric (PI) Coverage\"\n    ),\n    Value = c(\n      sprintf(\"%.4f m/s\", conf$q_hat),\n      sprintf(\"± %.4f m/s\", conf$q_hat),\n      sprintf(\"%.4f m/s\", conf$interval_width),\n      \"95%\",\n      sprintf(\"%.1f%% (error: %.2f%%)\", conf$coverage * 100,\n              abs(conf$coverage - 0.95) * 100),\n      sprintf(\"%.1f%% (error: %.2f%%)\", conf$comparison$pi_coverage * 100,\n              abs(conf$comparison$pi_coverage - 0.95) * 100)\n    )\n  )\n\n  format_table(detail_df, col.names = c(\"Component\", \"Value\"))\n}\n\n\n\n\nTable 8: Conformal Prediction Interval Calibration Details\n\n\n\n\n\n\nComponent\nValue\n\n\n\n\nNonconformity Score Threshold (95th percentile)\n0.1047 m/s\n\n\nInterval Half-Width\n± 0.1047 m/s\n\n\nTotal Interval Width\n0.2094 m/s\n\n\nTarget Coverage\n95%\n\n\nAchieved Conformal Coverage\n95.6% (error: 0.61%)\n\n\nAchieved Parametric (PI) Coverage\n91.6% (error: 3.37%)\n\n\n\n\n\n\n\n\n\n\nVisualization: Conformal vs Parametric Intervals\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$conformal)) {\n  conf &lt;- lmm_results$conformal\n  vt &lt;- lmm_results$velocity_table$table\n  vt$conformal_lower &lt;- vt$velocity - conf$q_hat\n  vt$conformal_upper &lt;- vt$velocity + conf$q_hat\n\n  # Reshape for plotting\n  interval_comparison &lt;- data.frame(\n    rir = rep(vt$rir, 2),\n    velocity = rep(vt$velocity, 2),\n    lower = c(vt$lower_95, vt$conformal_lower),\n    upper = c(vt$upper_95, vt$conformal_upper),\n    method = rep(c(\"Parametric (95% CI)\", \"Conformal (95% PI)\"), each = nrow(vt))\n  )\n\n  ggplot(interval_comparison, aes(x = rir)) +\n    geom_ribbon(aes(ymin = lower, ymax = upper, fill = method), alpha = 0.3) +\n    geom_line(aes(y = velocity), color = \"black\", linewidth = 1) +\n    geom_point(aes(y = velocity), color = \"black\", size = 3) +\n    facet_wrap(~method) +\n    labs(\n      x = \"Repetitions in Reserve (RIR)\",\n      y = \"Mean Velocity (m/s)\",\n      title = \"Comparison of Prediction Interval Methods\",\n      subtitle = \"Wider intervals indicate more uncertainty\"\n    ) +\n    theme_minimal() +\n    theme(legend.position = \"none\") +\n    scale_fill_manual(values = c(COLORS$primary, COLORS$secondary))\n}\n\n\n\n\n\n\n\n\nFigure 6: Conformal vs Parametric Prediction Intervals Across RIR Levels\n\n\n\n\n\n\n\nVelocity Stop Table with Conformal Intervals\nThis table provides conservative velocity targets using conformal prediction intervals, which have guaranteed 95% coverage regardless of distributional assumptions.\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$conformal)) {\n  conf &lt;- lmm_results$conformal\n  vt &lt;- lmm_results$velocity_table$table\n\n  conformal_table &lt;- data.frame(\n    RIR = vt$rir,\n    `Target Velocity` = round(vt$velocity, 3),\n    `Lower 95% (Conservative)` = round(vt$velocity - conf$q_hat, 3),\n    `Upper 95%` = round(vt$velocity + conf$q_hat, 3),\n    `Interval Width` = round(2 * conf$q_hat, 3),\n    check.names = FALSE\n  )\n\n  format_table(conformal_table,\n        col.names = c(\"Target RIR\", \"Mean Velocity (m/s)\", \"Lower Bound\", \"Upper Bound\", \"Width (m/s)\"))\n}\n\n\n\n\nTable 9: Velocity Stop Thresholds with Conformal Prediction Intervals\n\n\n\n\n\n\n\n\n\n\n\n\n\nTarget RIR\nMean Velocity (m/s)\nLower Bound\nUpper Bound\nWidth (m/s)\n\n\n\n\n0\n0.215\n0.110\n0.320\n0.209\n\n\n1\n0.244\n0.140\n0.349\n0.209\n\n\n2\n0.274\n0.169\n0.378\n0.209\n\n\n3\n0.303\n0.198\n0.408\n0.209\n\n\n4\n0.332\n0.227\n0.437\n0.209\n\n\n5\n0.361\n0.256\n0.466\n0.209\n\n\n6\n0.390\n0.286\n0.495\n0.209\n\n\n7\n0.420\n0.315\n0.524\n0.209\n\n\n\n\n\n\n\n\nHow to Use This Table:\n\nConservative approach (recommended): Use the Lower Bound column. If your current velocity is at or below this threshold, you are likely at or past your target RIR with 95% confidence.\nAggressive approach: Use the Mean Velocity column. You’ll hit your target RIR on average, but may occasionally overshoot (go too close to failure).\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$conformal)) {\n  conf &lt;- lmm_results$conformal\n  vt &lt;- lmm_results$velocity_table$table\n  rir2_row &lt;- vt[vt$rir == 2, ]\n\n  cat(\"3. **Example**: To stop at RIR 2:\\n\")\n  cat(\"   - **Conservative**: Stop when velocity drops to ~\", round(rir2_row$velocity - conf$q_hat, 2), \"m/s\\n\")\n  cat(\"   - **Mean target**: Stop when velocity drops to ~\", round(rir2_row$velocity, 2), \"m/s\\n\")\n}\n\n\nExample: To stop at RIR 2:\n\nConservative: Stop when velocity drops to ~ 0.17 m/s\nMean target: Stop when velocity drops to ~ 0.27 m/s\n\n\n\n\nCoverage Comparison\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$conformal)) {\n  conf &lt;- lmm_results$conformal\n\n  coverage_df &lt;- data.frame(\n    Method = c(\"Conformal\", \"Parametric\", \"Target\"),\n    Coverage = c(\n      conf$comparison$conformal_coverage * 100,\n      conf$comparison$parametric_coverage * 100,\n      95\n    ),\n    Type = c(\"Achieved\", \"Achieved\", \"Target\")\n  )\n\n  ggplot(coverage_df, aes(x = Method, y = Coverage, fill = Type)) +\n    geom_col(width = 0.6) +\n    geom_hline(yintercept = 95, linetype = \"dashed\", color = COLORS$secondary, linewidth = 1) +\n    geom_text(aes(label = sprintf(\"%.1f%%\", Coverage)), vjust = -0.5, size = 4) +\n    scale_fill_manual(values = c(\"Achieved\" = COLORS$primary, \"Target\" = \"#009E73\")) +\n    labs(\n      x = \"\",\n      y = \"Coverage (%)\",\n      title = \"Prediction Interval Coverage Comparison\",\n      subtitle = \"Conformal prediction achieves coverage closer to the 95% target\"\n    ) +\n    ylim(0, 105) +\n    theme_minimal() +\n    theme(legend.position = \"none\")\n}\n\n\n\n\n\n\n\n\nFigure 7: Coverage Comparison: Conformal vs Parametric Methods\n\n\n\n\n\n\n\nPractical Implications of Conformal Intervals\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$conformal)) {\n  conf &lt;- lmm_results$conformal\n\n  cat(\"The conformal prediction approach has several advantages for velocity-based training:\\n\\n\")\n  cat(\"1. **Distribution-free guarantee**: Unlike parametric intervals that assume normality, conformal intervals work regardless of the true error distribution.\\n\\n\")\n  cat(\"2. **Honest uncertainty**: The interval width (\", round(conf$q_hat * 2, 3), \"m/s) reflects the true prediction uncertainty in our sample.\\n\\n\")\n  cat(\"3. **Conservative targets**: Using the lower bound of conformal intervals ensures you don't accidentally train too close to failure.\\n\\n\")\n  cat(\"4. **Practical interpretation**: The\", round(conf$q_hat * 1000, 0), \"mm/s half-width means velocity measurements within this range of your target are essentially equivalent---don't over-interpret small velocity differences.\\n\")\n}\n\nThe conformal prediction approach has several advantages for velocity-based training:\n\nDistribution-free guarantee: Unlike parametric intervals that assume normality, conformal intervals work regardless of the true error distribution.\nHonest uncertainty: The interval width ( 0.209 m/s) reflects the true prediction uncertainty in our sample.\nConservative targets: Using the lower bound of conformal intervals ensures you don’t accidentally train too close to failure.\nPractical interpretation: The 105 mm/s half-width means velocity measurements within this range of your target are essentially equivalent—don’t over-interpret small velocity differences."
  },
  {
    "objectID": "deadlift_study.html#advanced-analyses",
    "href": "deadlift_study.html#advanced-analyses",
    "title": "Velocity-Based Training for the Conventional Deadlift",
    "section": "4. Advanced Analyses",
    "text": "4. Advanced Analyses\n\n4.1 Minimum Velocity Threshold (MVT) Variability\n\nThe Question\nThe Minimum Velocity Threshold (MVT) is the velocity at which an athlete can no longer complete a repetition (RIR = 0). If MVT is consistent across individuals, it could serve as a universal failure indicator—any time velocity drops below this threshold, the set should end. But how variable is MVT across individuals?\n\n\nThe Method\nWe quantified MVT variability using:\n\nCoefficient of Variation (CV): SD/Mean × 100%—a standardized measure of spread\nRange (Min-Max): The full spread of MVT values observed\nIQR: Middle 50% of values, robust to outliers\n\nInterpretation Guidelines:\n\nCV &lt; 10%: Low variability → universal threshold may work\nCV 10-25%: Moderate variability → individual calibration recommended\nCV &gt; 25%: High variability → individual calibration essential\n\n\n\nThe Finding\n\nShow code\nif (!is.null(advanced_results)) {\n  mvt &lt;- advanced_results$mvt\n  cat(\"**Population MVT Statistics:**\\n\\n\")\n  cat(\"- Mean:\", round(mvt$population_stats$mean, 3), \"m/s\\n\")\n  cat(\"- SD:\", round(mvt$population_stats$sd, 3), \"m/s\\n\")\n  cat(\"- **CV:**\", round(mvt$population_stats$cv_percent, 1), \"%\\n\")\n  cat(\"- Range:\", round(mvt$population_stats$min, 3), \"-\", round(mvt$population_stats$max, 3), \"m/s\\n\")\n}\n\nPopulation MVT Statistics:\n\nMean: 0.206 m/s\nSD: 0.063 m/s\nCV: 30.8 %\nRange: 0.119 - 0.402 m/s\n\n\n\nShow code\nif (!is.null(advanced_results)) {\n  failure_data &lt;- data[data$rir == 0, ]\n  mvt &lt;- advanced_results$mvt\n\n  ggplot(failure_data, aes(x = mean_velocity)) +\n    geom_histogram(aes(y = after_stat(density)), bins = 20, fill = COLORS$primary, alpha = 0.7) +\n    geom_density(color = COLORS$secondary, linewidth = 1) +\n    geom_vline(xintercept = mvt$population_stats$mean, linetype = \"dashed\", linewidth = 1) +\n    labs(x = \"Velocity at Failure (m/s)\", y = \"Density\",\n         title = \"MVT Distribution Across Failure Observations\")\n}\n\n\n\n\n\n\n\n\nFigure 8: Distribution of Minimum Velocity Threshold at failure\n\n\n\n\n\n\n\nShow code\nif (!is.null(advanced_results) && !is.null(advanced_results$mvt$individual_stats)) {\n  ind_mvt &lt;- advanced_results$mvt$individual_stats\n  ind_mvt &lt;- ind_mvt[order(ind_mvt$mean_mvt), ]\n  ind_mvt$id &lt;- factor(ind_mvt$id, levels = ind_mvt$id)\n\n  ggplot(ind_mvt, aes(x = id, y = mean_mvt, color = sex)) +\n    geom_point(size = 3) +\n    geom_errorbar(aes(ymin = mean_mvt - sd_mvt, ymax = mean_mvt + sd_mvt),\n                  width = 0.3) +\n    geom_hline(yintercept = advanced_results$mvt$population_stats$mean,\n               linetype = \"dashed\", color = \"gray50\") +\n    coord_flip() +\n    labs(x = \"Participant\", y = \"MVT (m/s)\",\n         title = \"Individual MVT with Standard Deviation\",\n         color = \"Sex\") +\n    scale_color_manual(values = c(\"female\" = COLORS$secondary, \"male\" = COLORS$primary))\n}\n\n\n\n\n\n\n\n\nFigure 9: Individual MVT values showing between-participant variability\n\n\n\n\n\nLoad Comparison:\n\nShow code\nif (!is.null(advanced_results) && !is.null(advanced_results$mvt$load_comparison)) {\n  lc &lt;- advanced_results$mvt$load_comparison\n  cat(\"- 80% 1RM MVT:\", round(lc$load_80_mean, 3), \"±\", round(lc$load_80_sd, 3), \"m/s\\n\")\n  cat(\"- 90% 1RM MVT:\", round(lc$load_90_mean, 3), \"±\", round(lc$load_90_sd, 3), \"m/s\\n\")\n  cat(\"- Difference:\", round(lc$difference, 3), \"m/s\\n\")\n  cat(\"- p-value:\", format(lc$wilcox_p, digits = 4), \"\\n\")\n  if (lc$significant) {\n    cat(\"- **Significant difference** - MVT varies by load\\n\")\n  } else {\n    cat(\"- No significant load effect - MVT appears consistent across loads\\n\")\n  }\n}\n\n\n80% 1RM MVT: 0.217 ± 0.066 m/s\n90% 1RM MVT: 0.196 ± 0.061 m/s\nDifference: 0.021 m/s\np-value: 0.148\nNo significant load effect - MVT appears consistent across loads\n\nThe CV of 30.8% indicates high inter-individual variability (above the 25% threshold).\nPractical Implication: A universal MVT threshold would misclassify many athletes—some would stop too early (false positive for failure), others too late (missed failure). Individual calibration is not optional for accurate VBT in deadlifts.\n\n\n\n4.2 Day-to-Day Reliability\n\nThe Question\nIf an athlete’s velocity profile is established on Day 1, how well does it predict their performance on Day 2? Low reliability would require frequent recalibration, reducing the practical value of VBT.\n\n\nThe Method\nWe assessed reliability using the Intraclass Correlation Coefficient (ICC) (Shrout and Fleiss 1979; Koo and Li 2016):\n\nICC(2,1): Two-way random effects model, single measurement—the standard for test-retest reliability\nParameters assessed:\n\nVelocity-RIR slope (sensitivity of velocity to fatigue)\nMVT (velocity at failure)\n\n\nInterpretation Guidelines (Koo and Li 2016):\n\nICC &lt; 0.50: Poor reliability\nICC 0.50-0.75: Moderate reliability\nICC 0.75-0.90: Good reliability\nICC &gt; 0.90: Excellent reliability\n\nAdditional Metrics:\n\nSEM (Standard Error of Measurement): Expected test-retest variability in same units as measurement\nMDC95 (Minimal Detectable Change): The smallest change that exceeds measurement error with 95% confidence\n\n\n\nThe Finding\n\nShow code\nif (!is.null(advanced_results)) {\n  rel &lt;- advanced_results$reliability\n  cat(\"**Reliability Results:**\\n\\n\")\n  cat(\"| Parameter | ICC | 95% CI | Interpretation |\\n\")\n  cat(\"|-----------|-----|--------|----------------|\\n\")\n  cat(sprintf(\"| Velocity-RIR Slope | %.2f | [%.2f, %.2f] | %s |\\n\",\n              rel$slope_icc$icc, rel$slope_icc$ci_lower, rel$slope_icc$ci_upper, rel$slope_icc$interpretation))\n  cat(sprintf(\"| MVT | %.2f | [%.2f, %.2f] | %s |\\n\",\n              rel$mvt_icc$icc, rel$mvt_icc$ci_lower, rel$mvt_icc$ci_upper, rel$mvt_icc$interpretation))\n  cat(\"\\n**Measurement Error:**\\n\\n\")\n  cat(\"- SEM:\", round(rel$mvt_icc$sem * 1000, 1), \"mm/s\\n\")\n  cat(\"- MDC95:\", round(rel$mvt_icc$mdc95 * 1000, 1), \"mm/s\\n\")\n  cat(\"\\n*Interpretation: A velocity change of at least\", round(rel$mvt_icc$mdc95 * 1000, 1), \"mm/s is needed to be confident it's a real change, not measurement noise.*\\n\")\n}\n\nReliability Results:\n\n\n\nParameter\nICC\n95% CI\nInterpretation\n\n\n\n\nVelocity-RIR Slope\n0.72\n[0.64, 0.80]\nModerate\n\n\nMVT\n0.50\n[0.38, 0.62]\nPoor\n\n\n\nMeasurement Error:\n\nSEM: 24 mm/s\nMDC95: 66.6 mm/s\n\nInterpretation: A velocity change of at least 66.6 mm/s is needed to be confident it’s a real change, not measurement noise.\n\n\nShow code\nif (!is.null(advanced_results) && !is.null(advanced_results$reliability$day_parameters)) {\n  day_params &lt;- advanced_results$reliability$day_parameters\n\n  ggplot(day_params, aes(x = slope_day1, y = slope_day2)) +\n    geom_point(size = 3, alpha = 0.7, color = COLORS$primary) +\n    geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"gray50\") +\n    geom_smooth(method = \"lm\", se = TRUE, color = COLORS$secondary) +\n    labs(x = \"Slope Day 1 (m/s per RIR)\", y = \"Slope Day 2 (m/s per RIR)\",\n         title = \"Day-to-Day Reliability of Individual Slopes\",\n         subtitle = paste(\"ICC =\", round(advanced_results$reliability$slope_icc$icc, 2)))\n}\n\n\n\n\n\n\n\n\nFigure 10: Day 1 vs Day 2 comparison of individual velocity-RIR slopes\n\n\n\n\n\n\n\nShow code\nif (!is.null(advanced_results) && !is.null(advanced_results$reliability$day_parameters)) {\n  day_params &lt;- advanced_results$reliability$day_parameters\n\n  ggplot(day_params, aes(x = mvt_day1, y = mvt_day2)) +\n    geom_point(size = 3, alpha = 0.7, color = COLORS$primary) +\n    geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"gray50\") +\n    geom_smooth(method = \"lm\", se = TRUE, color = COLORS$secondary) +\n    labs(x = \"MVT Day 1 (m/s)\", y = \"MVT Day 2 (m/s)\",\n         title = \"Day-to-Day Reliability of Predicted MVT\",\n         subtitle = paste(\"ICC =\", round(advanced_results$reliability$mvt_icc$icc, 2)))\n}\n\n\n\n\n\n\n\n\nFigure 11: Day 1 vs Day 2 comparison of predicted MVT\n\n\n\n\n\n\n\nBland-Altman Analysis\nThe Bland-Altman plot provides additional insight beyond ICC by showing:\n\nSystematic bias: Is there a consistent difference between days?\nLimits of agreement: What range of day-to-day variation should we expect?\nProportional bias: Does variability depend on the magnitude of the measurement?\n\n\n\nShow code\nif (!is.null(advanced_results) && !is.null(advanced_results$reliability$day_parameters)) {\n  day_params &lt;- advanced_results$reliability$day_parameters\n\n  plot_bland_altman(\n    day1 = day_params$slope_day1,\n    day2 = day_params$slope_day2,\n    title = \"Bland-Altman: Velocity-RIR Slope Reliability\",\n    y_label = \"(m/s per RIR)\"\n  )\n}\n\n\n\n\n\n\n\n\nFigure 12: Bland-Altman Plot for Velocity-RIR Slopes: Assesses systematic bias and limits of agreement between testing days.\n\n\n\n\n\n\n\nShow code\nif (!is.null(advanced_results) && !is.null(advanced_results$reliability$day_parameters)) {\n  day_params &lt;- advanced_results$reliability$day_parameters\n\n  plot_bland_altman(\n    day1 = day_params$mvt_day1,\n    day2 = day_params$mvt_day2,\n    title = \"Bland-Altman: MVT Reliability\",\n    y_label = \"(m/s)\"\n  )\n}\n\n\n\n\n\n\n\n\nFigure 13: Bland-Altman Plot for MVT: Shows day-to-day variability in minimum velocity threshold estimates.\n\n\n\n\n\nPractical Recommendation: The moderate reliability suggests that velocity profiles are reasonably stable but not perfect. Recalibrate every 2-4 weeks or after significant training blocks, illness, or competition.\n\n\n\n4.3 Linear vs Polynomial Models\n\nThe Question\nThe velocity-RIR relationship could be linear (constant velocity loss per rep) or curvilinear (accelerating velocity loss near failure). Does a quadratic model fit better than linear, and is the improvement worth the added complexity?\n\n\nThe Method\nFor each participant, we fit both models and compared them:\n\nLinear: Velocity = β₀ + β₁ × RIR\nQuadratic: Velocity = β₀ + β₁ × RIR + β₂ × RIR²\n\nModel Selection Criteria:\n\nAIC (Akaike Information Criterion): Balances fit against complexity; lower is better\nR² improvement: How much additional variance is explained by the quadratic term\n\nDecision Rule: Quadratic is “better” if AIC is lower by ≥2 points (meaningful improvement) AND R² improves by ≥0.01.\n\n\nThe Finding\n\nShow code\nif (!is.null(advanced_results)) {\n  poly &lt;- advanced_results$polynomial_comparison\n  cat(\"**Model Comparison Results:**\\n\\n\")\n  cat(\"| Metric | Value |\\n\")\n  cat(\"|--------|-------|\\n\")\n  cat(sprintf(\"| Participants analyzed | %d |\\n\", poly$best_model_summary$n_participants))\n  cat(sprintf(\"| Linear model preferred | %d (%.0f%%) |\\n\",\n              poly$best_model_summary$n_linear_best,\n              100 - poly$best_model_summary$pct_quad_best))\n  cat(sprintf(\"| Quadratic model preferred | %d (%.0f%%) |\\n\",\n              poly$best_model_summary$n_quad_best,\n              poly$best_model_summary$pct_quad_best))\n  cat(sprintf(\"| Average R² improvement (quadratic) | %.4f |\\n\", poly$best_model_summary$avg_r2_improvement))\n}\n\nModel Comparison Results:\n\n\n\nMetric\nValue\n\n\n\n\nParticipants analyzed\n19\n\n\nLinear model preferred\n14 (74%)\n\n\nQuadratic model preferred\n5 (26%)\n\n\nAverage R² improvement (quadratic)\n0.0164\n\n\n\n\n\nShow code\nif (!is.null(advanced_results) && !is.null(advanced_results$polynomial_comparison$individual_results)) {\n  ind_results &lt;- advanced_results$polynomial_comparison$individual_results\n\n  ggplot(ind_results, aes(x = reorder(id, delta_aic), y = delta_aic, fill = best_model)) +\n    geom_col() +\n    geom_hline(yintercept = -2, linetype = \"dashed\", color = COLORS$secondary) +\n    geom_hline(yintercept = 2, linetype = \"dashed\", color = COLORS$secondary) +\n    coord_flip() +\n    labs(x = \"Participant\", y = \"ΔAIC (Quadratic - Linear)\",\n         title = \"Model Comparison by Participant\",\n         subtitle = \"Negative values favor quadratic model\",\n         fill = \"Best Model\") +\n    scale_fill_manual(values = c(\"linear\" = COLORS$primary, \"quadratic\" = COLORS$tertiary))\n}\n\n\n\n\n\n\n\n\nFigure 14: AIC comparison between linear and quadratic models by participant\n\n\n\n\n\nInterpretation: For the majority of participants, the linear model is sufficient. The quadratic term provides only marginal improvement (&lt; 1% R² gain on average). This simplifies practical application—coaches can use a simple linear relationship without sacrificing meaningful accuracy.\nPractical Implication: Use linear velocity tables. The small theoretical improvement from polynomial models is not worth the added complexity for training prescription.\n\n\n\n4.4 Velocity Decay Patterns\n\nThe Question\nDoes velocity loss occur at a constant rate throughout a set, or does it accelerate as failure approaches? Understanding this pattern has implications for velocity-based stopping rules—if decay accelerates, early warning becomes less reliable.\n\n\nThe Method\nWe analyzed within-set velocity decay by:\n\nPhase comparison: Comparing velocity loss in the first half vs. second half of each set\nStatistical test: Paired t-test to determine if second-half decay is significantly greater\n\nHypotheses:\n\nH₀: Decay rate is constant (first half = second half)\nH₁: Decay accelerates (second half &gt; first half)\n\n\n\nThe Finding\n\nShow code\nif (!is.null(advanced_results) && !is.null(advanced_results$velocity_decay)) {\n  decay &lt;- advanced_results$velocity_decay\n  cat(\"**Velocity Decay Results:**\\n\\n\")\n  if (!is.null(decay$phase_decay)) {\n    cat(\"| Set Phase | Velocity Loss (m/s per rep) |\\n\")\n    cat(\"|-----------|-----------------------------|\\n\")\n    cat(sprintf(\"| First half | %.4f |\\n\", as.numeric(decay$phase_decay$first_half_decay)))\n    cat(sprintf(\"| Second half | %.4f |\\n\", as.numeric(decay$phase_decay$second_half_decay)))\n    cat(\"\\n\")\n  }\n  if (!is.null(decay$decay_acceleration)) {\n    if (isTRUE(decay$decay_acceleration$accelerating)) {\n      cat(sprintf(\"**Statistical Test:** Decay significantly accelerates near failure (p = %s)\\n\\n\",\n                  format(decay$decay_acceleration$p_value, digits = 3)))\n      cat(\"The velocity-fatigue curve is **exponential**, not linear. This means:\\n\\n\")\n      cat(\"1. Early reps provide less warning of impending failure\\n\")\n      cat(\"2. The final 2-3 reps show rapid velocity decline\\n\")\n      cat(\"3. Conservative velocity thresholds are advisable\\n\")\n    } else {\n      cat(\"**Statistical Test:** Decay rate is approximately constant throughout the set.\\n\\n\")\n      cat(\"The linear model adequately describes velocity loss across reps.\\n\")\n    }\n  }\n} else {\n  cat(\"*Velocity decay analysis will be available after running the advanced analysis pipeline.*\\n\")\n}\n\nVelocity Decay Results:\nStatistical Test: Decay significantly accelerates near failure (p = 0.035)\nThe velocity-fatigue curve is exponential, not linear. This means:\n\nEarly reps provide less warning of impending failure\nThe final 2-3 reps show rapid velocity decline\nConservative velocity thresholds are advisable\n\n\n\nShow code\nif (!is.null(data) && \"set_id\" %in% names(data)) {\n  # Calculate rep number within each set based on RIR\n  traj_data &lt;- data\n  traj_data$rep_num &lt;- ave(traj_data$rir, traj_data$set_id, FUN = function(x) max(x) - x + 1)\n\n  ggplot(traj_data, aes(x = rep_num, y = mean_velocity, group = set_id)) +\n    geom_line(alpha = 0.3, color = COLORS$primary) +\n    stat_summary(aes(group = 1), fun = mean, geom = \"line\",\n                 color = COLORS$secondary, linewidth = 1.5) +\n    stat_summary(aes(group = 1), fun = mean, geom = \"point\",\n                 color = COLORS$secondary, size = 3) +\n    labs(x = \"Rep Number\", y = \"Mean Velocity (m/s)\",\n         title = \"Velocity Trajectories Across Sets\",\n         subtitle = \"Individual sets (blue) with population mean (red)\")\n}\n\n\nPractical Implication: Because velocity loss accelerates near failure, athletes should use conservative stopping thresholds. If targeting RIR 2, consider stopping at the RIR 3 threshold to account for the acceleration effect.\n\n\n\n4.5 First-Rep Failure Prediction\n\nThe Question\nCan we predict how many reps until failure from just the first 1-3 repetitions? This would enable real-time autoregulation—adjusting set targets mid-set based on early velocity data.\n\n\nThe Method\nWe built a prediction model using Leave-One-Out Cross-Validation (LOOCV) (Hastie, Tibshirani, and Friedman 2009):\n\nPredictor: First-rep velocity (m/s)\nOutcome: Total reps to failure\nModel: Linear regression (velocity → rep capacity)\nValidation: Each set held out once; model trained on remaining sets\n\nWhy LOOCV? It provides an unbiased estimate of out-of-sample prediction error. Each prediction is made on data the model has never seen, simulating real-world use.\nMetrics:\n\nMAE: Average absolute error in rep prediction (in reps)\nRMSE: Root mean squared error (penalizes large errors)\nR²: Proportion of variance in rep capacity explained by first-rep velocity\n\n\n\nThe Finding\n\nShow code\nif (!is.null(advanced_results)) {\n  pred &lt;- advanced_results$failure_prediction\n  cat(\"**Prediction Accuracy (Leave-One-Out CV):**\\n\\n\")\n  cat(\"| Metric | Value | Interpretation |\\n\")\n  cat(\"|--------|-------|----------------|\\n\")\n  cat(sprintf(\"| MAE | %.2f reps | Average prediction error |\\n\", pred$cv_results$mae))\n  cat(sprintf(\"| RMSE | %.2f reps | Error penalizing outliers |\\n\", pred$cv_results$rmse))\n  cat(sprintf(\"| R² | %.3f | Variance explained |\\n\", pred$cv_results$r2))\n}\n\nPrediction Accuracy (Leave-One-Out CV):\n\n\n\nMetric\nValue\nInterpretation\n\n\n\n\nMAE\n1.29 reps\nAverage prediction error\n\n\nRMSE\n1.53 reps\nError penalizing outliers\n\n\nR²\n0.065\nVariance explained\n\n\n\n\n\nShow code\nif (!is.null(advanced_results) && !is.null(advanced_results$failure_prediction$lookup_table)) {\n  lookup &lt;- advanced_results$failure_prediction$lookup_table\n  format_table(lookup, digits = 1,\n        col.names = c(\"First Rep Velocity (m/s)\", \"Predicted Reps\", \"Lower 95%\", \"Upper 95%\"))\n}\n\n\n\n\nTable 10: Practical Lookup Table: First Rep Velocity → Predicted Reps\n\n\n\n\n\n\nFirst Rep Velocity (m/s)\nPredicted Reps\nLower 95%\nUpper 95%\n\n\n\n\n0.1\n4.9\n1.8\n8.0\n\n\n0.2\n5.2\n2.1\n8.3\n\n\n0.2\n5.4\n2.4\n8.5\n\n\n0.3\n5.7\n2.6\n8.7\n\n\n0.3\n6.0\n2.9\n9.0\n\n\n0.4\n6.2\n3.2\n9.3\n\n\n0.5\n6.5\n3.4\n9.6\n\n\n0.5\n6.8\n3.7\n9.9\n\n\n0.6\n7.0\n3.9\n10.2\n\n\n\n\n\n\n\n\n\n\nShow code\nif (!is.null(data)) {\n  # Prepare set-level data\n  set_data &lt;- do.call(rbind, lapply(unique(data$set_id), function(sid) {\n    s &lt;- data[data$set_id == sid, ]\n    s &lt;- s[order(s$rep_number), ]\n    if (nrow(s) &lt; 1) return(NULL)\n    data.frame(\n      set_id = sid,\n      v1 = s$mean_velocity[1],\n      reps_to_failure = s$reps_to_failure[1],\n      load_percentage = s$load_percentage[1]\n    )\n  }))\n\n  if (!is.null(set_data) && nrow(set_data) &gt; 0) {\n    ggplot(set_data, aes(x = v1, y = reps_to_failure, color = load_percentage)) +\n      geom_point(size = 3, alpha = 0.7) +\n      geom_smooth(method = \"lm\", se = TRUE, aes(group = 1), color = \"black\") +\n      labs(x = \"First Rep Velocity (m/s)\", y = \"Total Reps to Failure\",\n           title = \"Relationship Between First Rep Velocity and Set Capacity\",\n           color = \"Load\") +\n      scale_color_load()\n  }\n}\n\n\nInterpretation: A MAE of ~1-2 reps means that on average, first-rep velocity predicts failure within 1-2 repetitions. This is practically useful for autoregulation—if the model predicts 6 reps to failure, expect 5-7 reps.\nPractical Application: After the first rep, check velocity against the prediction table. If velocity is unusually low (predicting fewer reps than planned), consider reducing set volume. If unusually high, you may have more capacity than expected.\n\n\n\n\n4.6 Model Specification: A Systematic Approach\nBefore presenting model comparisons and validation, we document the rationale behind our modeling choices following established GLM principles. Each decision point has explicit criteria.\n\n4.6.1 Distribution Choice (Random Component)\nThe Question: Which probability distribution best describes our outcome variable?\nOur outcome (mean concentric velocity) is:\n\nContinuous (not binary or count data)\nBounded below by 0 (physical constraint: velocity cannot be negative)\nApproximately normal within participants after accounting for RIR effects\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$models) && !is.null(lmm_results$models$random_slope)) {\n  # Check residual distribution from the fitted model\n  model &lt;- lmm_results$models$random_slope\n  resids &lt;- as.numeric(residuals(model))\n\n  if (length(resids) &gt; 0 && is.numeric(resids)) {\n    # Shapiro-Wilk test (for formal assessment) - limit to 5000 obs\n    sw_test &lt;- shapiro.test(resids[1:min(5000, length(resids))])\n\n    # Calculate skewness\n    skewness &lt;- mean((resids - mean(resids))^3) / (sd(resids)^3)\n\n    cat(sprintf(\"**Residual Distribution Assessment:**\\n\\n\"))\n    cat(sprintf(\"- Skewness: %.3f (ideal: 0; acceptable: |skew| &lt; 1)\\n\", skewness))\n    cat(sprintf(\"- Shapiro-Wilk statistic: %.4f\\n\", sw_test$statistic))\n\n    if (abs(skewness) &lt; 0.5) {\n      cat(\"\\n**Decision**: Skewness is minimal. Use **Gaussian (normal) family** with standard LMM.\\n\")\n    } else if (abs(skewness) &lt; 1) {\n      cat(\"\\n**Decision**: Slight skewness detected but within acceptable range. Gaussian family appropriate.\\n\")\n    } else {\n      cat(\"\\n**Decision**: Notable skewness. Consider Gamma family or log transformation.\\n\")\n    }\n  }\n}\n\n\n\n\n\n\n\nWhy Not Use Gamma Distribution?\n\n\n\n\n\nGamma distributions are appropriate for strictly positive, right-skewed continuous data. Our velocity data, while bounded at 0, shows approximately symmetric residuals after accounting for fixed effects. The LMM’s random effects absorb much of the between-participant variation that might otherwise induce skewness.\nAdditionally, Gamma GLMs with log link would require exponentiating coefficients for interpretation, adding complexity without meaningful gain in model fit.\n\n\n\n\n\n4.6.2 Link Function\nThe Question: How should we connect predictors to the expected outcome?\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$models) && !is.null(lmm_results$models$random_slope)) {\n  # Check linearity by looking at residuals vs fitted\n  model &lt;- lmm_results$models$random_slope\n  resids &lt;- as.numeric(residuals(model))\n  fitted_vals &lt;- as.numeric(fitted(model))\n\n  if (length(resids) &gt; 0 && length(fitted_vals) &gt; 0) {\n    # Simple correlation as linearity check\n    residual_fitted_cor &lt;- cor(fitted_vals, resids^2)\n\n    cat(\"**Linearity Assessment:**\\n\\n\")\n    cat(sprintf(\"- Correlation between fitted values and squared residuals: %.3f\\n\", residual_fitted_cor))\n\n    if (abs(residual_fitted_cor) &lt; 0.2) {\n      cat(\"\\n**Decision**: Residuals show no systematic curvature with fitted values.\\n\")\n      cat(\"Use **identity link** (no transformation needed).\\n\")\n    } else {\n      cat(\"\\n**Decision**: Some curvature detected. Consider polynomial terms or log link.\\n\")\n    }\n  }\n}\n\nThe identity link means our coefficients represent direct changes in m/s per unit RIR change, which is maximally interpretable for practitioners.\n\n\n4.6.3 Random Effects Structure Decision\nDecision Criteria (following (barr2013?) recommendations):\n\n\n\nCriterion\nThreshold\nOur Data\nDecision\n\n\n\n\nICC (baseline variation)\n&gt; 0.05\n0.67\nRandom intercepts ✓\n\n\nLRT for random slopes\np &lt; 0.05\np &lt; 0.001\nRandom slopes ✓\n\n\nBayes Factor\nBF &gt; 10 (strong)\ndecisive\nRandom slopes ✓\n\n\nObs per participant\n≥ 10\n~21\nDesign supports ✓\n\n\n\nAll criteria favor random slopes model.\n\n\n4.6.4 Fixed Effects Selection\nThe Question: Which predictors should be included in the model?\nWe use Bayes Factors as the primary comparison metric (with AIC/BIC as secondary):\n\n\n\nComparison\nBF\nInterpretation\nDecision\n\n\n\n\nBase model (RIR only)\n—\nReference\n—\n\n\n+ Load effect\n~0.89\nNo evidence for load\nExclude\n\n\n+ RIR × Load interaction\n—\nEvidence against\nExclude\n\n\n\n\n\n\n\n\n\nModel Selection Priority\n\n\n\nWhen comparison metrics disagree, we follow this hierarchy:\n\nBayes Factor (primary): Directly quantifies evidence for/against models\nAIC (secondary): Better for prediction-focused goals\nBIC (tertiary): More conservative, penalizes complexity\nLRT p-value: Only for nested model comparison\n\nIf BF &gt; 10 (strong evidence), we accept that model regardless of AIC/BIC.\n\n\n\n\n4.6.5 Final Model Specification\nBased on the systematic assessment above:\nModel: mean_velocity ~ rir + (1 + rir | id)\n\nFamily: Gaussian (normal residuals confirmed)\nLink: Identity (linear relationship, direct interpretation)\nRandom intercepts: Each participant has their own baseline velocity\nRandom slopes: Each participant has their own velocity-RIR relationship\nFixed effects: RIR only (load effect not supported by evidence)"
  },
  {
    "objectID": "deadlift_study.html#model-validation-and-robustness",
    "href": "deadlift_study.html#model-validation-and-robustness",
    "title": "Velocity-Based Training for the Conventional Deadlift",
    "section": "5. Model Validation and Robustness",
    "text": "5. Model Validation and Robustness\nThis section presents rigorous statistical validation of the LMM approach, including model comparisons, robustness checks, and sensitivity analyses.\n\n5.1 LMM Model Specification Comparison\n\nThe Question\nWhich random effects structure best captures the hierarchical nature of the data? Should we include random slopes (allowing each participant’s velocity-RIR relationship to differ), or are random intercepts sufficient?\n\n\nThe Method\nWe compared nested models using likelihood ratio tests and information criteria:\n\nRandom Intercept Only: Velocity ~ RIR + (1 | participant)\n\nAllows mean velocity to differ by participant\nAssumes same velocity-RIR slope for everyone\n\nRandom Intercept + Slope: Velocity ~ RIR + (1 + RIR | participant)\n\nAllows both mean velocity AND slope to differ\nMore flexible, captures individual differences in fatigue sensitivity\n\n\nComparison Metrics:\n\nLRT (Likelihood Ratio Test): Compares nested models; significant p-value favors complex model\nAIC/BIC: Information criteria; lower is better (BIC penalizes complexity more)\nR² (Marginal vs Conditional): Fixed effects only (R²m) vs fixed + random (R²c) (Nakagawa and Schielzeth 2013)\n\n\n\nWhy Random Slopes? A Three-Step Decision Framework\nBefore looking at formal comparisons, we should justify why random slopes are theoretically appropriate. This follows a principled three-step decision process:\nStep 1: Do individuals differ in their baseline?\nAthletes have different baseline velocities at any given RIR—some are inherently faster, others slower. This justifies random intercepts: each athlete gets their own starting point.\n\nShow code\nif (!is.null(data)) {\n  # Calculate mean velocity per participant\n  participant_means &lt;- aggregate(mean_velocity ~ id, data = data, FUN = mean)\n  velocity_range &lt;- diff(range(participant_means$mean_velocity))\n\n  cat(sprintf(\"*Evidence:* Individual mean velocities range from %.3f to %.3f m/s (spread = %.3f m/s)\\n\",\n              min(participant_means$mean_velocity), max(participant_means$mean_velocity), velocity_range))\n}\n\nEvidence: Individual mean velocities range from 0.227 to 0.418 m/s (spread = 0.191 m/s)\nStep 2: Do individuals differ in their response to the predictor?\nThe key question: does the velocity-RIR relationship vary across athletes? If some athletes show steep velocity decline while others show gradual decline, we need random slopes.\n\nShow code\nif (!is.null(data)) {\n  # Calculate individual slopes\n  individual_slopes &lt;- do.call(rbind, lapply(unique(data$id), function(pid) {\n    p_data &lt;- data[data$id == pid, ]\n    if (nrow(p_data) &gt;= 3) {\n      m &lt;- lm(mean_velocity ~ rir, data = p_data)\n      data.frame(id = pid, slope = coef(m)[\"rir\"])\n    }\n  }))\n\n  if (nrow(individual_slopes) &gt; 0) {\n    slope_range &lt;- diff(range(individual_slopes$slope, na.rm = TRUE))\n    slope_cv &lt;- sd(individual_slopes$slope, na.rm = TRUE) / abs(mean(individual_slopes$slope, na.rm = TRUE)) * 100\n\n    cat(sprintf(\"*Evidence:* Individual slopes range from %.4f to %.4f m/s per RIR (CV = %.1f%%)\\n\",\n                min(individual_slopes$slope, na.rm = TRUE),\n                max(individual_slopes$slope, na.rm = TRUE),\n                slope_cv))\n    cat(\"\\nThis substantial variation justifies random slopes.\\n\")\n  }\n}\n\nEvidence: Individual slopes range from 0.0059 to 0.0668 m/s per RIR (CV = 51.5%)\nThis substantial variation justifies random slopes.\nStep 3: Does the design support random slopes?\nRandom slopes require multiple observations per participant at different predictor values. With 406/19 = ~21 observations per athlete across different RIR levels, we have sufficient data to estimate individual slopes reliably.\nDecision: All three conditions are met → Random slopes are appropriate and necessary.\n\n\nThe Finding\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$model_comparison)) {\n  mc &lt;- lmm_results$model_comparison\n\n  # Use comparison_table which contains all models\n  if (!is.null(mc$comparison_table)) {\n    ct &lt;- mc$comparison_table\n\n    cat(\"**Model Comparison Results:**\\n\\n\")\n    cat(\"| Model | AIC | BIC | ΔAIC | R²m | R²c |\\n\")\n    cat(\"|-------|-----|-----|------|-----|-----|\\n\")\n\n    for (i in seq_len(nrow(ct))) {\n      cat(sprintf(\"| %s | %.1f | %.1f | %.1f | %.3f | %.3f |\\n\",\n                  ct$model[i], ct$AIC[i], ct$BIC[i], ct$delta_AIC[i],\n                  ct$R2_marginal[i], ct$R2_conditional[i]))\n    }\n\n    # Find best model\n    best_idx &lt;- which.min(ct$AIC)\n    cat(sprintf(\"\\n**Best Model (lowest AIC):** %s\\n\", ct$model[best_idx]))\n  }\n} else {\n  cat(\"*Model comparison results will be available after running the full analysis pipeline.*\\n\\n\")\n}\n\nModel Comparison Results:\n\n\n\nModel\nAIC\nBIC\nΔAIC\nR²m\nR²c\n\n\n\n\nbase\n-1046.7\n-1030.7\n62.6\n0.339\n0.616\n\n\nrandom_slope\n-1109.3\n-1085.2\n0.0\n0.435\n0.639\n\n\nwith_load\n-1101.1\n-1073.1\n8.1\n0.436\n0.639\n\n\nwith_day\n-1091.5\n-1059.4\n17.8\n0.438\n0.639\n\n\ninteraction\n-1105.6\n-1073.6\n3.6\n0.452\n0.654\n\n\nfull\n-1081.3\n-1037.2\n28.0\n0.458\n0.658\n\n\n\nBest Model (lowest AIC): random_slope\n\n\nBayes Factor Analysis: Quantifying the Evidence\nWhile AIC/BIC differences tell us which model fits better, they don’t directly quantify how much evidence we have. Bayes factors provide an intuitive metric: how many times more likely is the data under one model versus another?\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$model_comparison) &&\n    !is.null(lmm_results$model_comparison$models)) {\n\n  models &lt;- lmm_results$model_comparison$models\n\n  # Compare base vs random slope using ModelComparator\n  if (\"base\" %in% names(models) && \"random_slope\" %in% names(models)) {\n    comparison &lt;- model_comparator$compare(\n      models$base, models$random_slope,\n      model1_name = \"Random Intercept Only\",\n      model2_name = \"Random Intercept + Slope\",\n      nested = TRUE\n    )\n\n    # Calculate Bayes factor from BIC\n    bic_diff &lt;- BIC(models$base) - BIC(models$random_slope)\n    bf &lt;- exp(bic_diff / 2)\n\n    cat(\"**Bayes Factor Analysis (Random Slope vs. Intercept-Only):**\\n\\n\")\n    cat(\"| Metric | Value | Interpretation |\\n\")\n    cat(\"|--------|-------|----------------|\\n\")\n    cat(sprintf(\"| BIC difference | %.1f | %s |\\n\", bic_diff,\n                ifelse(bic_diff &gt; 0, \"Favors random slope\", \"Favors intercept-only\")))\n    cat(sprintf(\"| Bayes Factor | %.1f | %s |\\n\", bf,\n                model_comparator$interpret_bayes_factor(bf)))\n\n    if (!is.null(comparison$lrt_pvalue)) {\n      cat(sprintf(\"| LRT χ² | %.2f (df=%.0f) | p %s |\\n\",\n                  comparison$lrt_chisq, comparison$lrt_df,\n                  ifelse(comparison$lrt_pvalue &lt; 0.001, \"&lt; 0.001\",\n                         sprintf(\"= %.3f\", comparison$lrt_pvalue))))\n    }\n\n    cat(\"\\n**Interpretation Guide:**\\n\\n\")\n    cat(\"- **BF 1-3**: Weak/anecdotal evidence\\n\")\n    cat(\"- **BF 3-10**: Moderate evidence\\n\")\n    cat(\"- **BF 10-30**: Strong evidence\\n\")\n    cat(\"- **BF 30-100**: Very strong evidence\\n\")\n    cat(\"- **BF &gt;100**: Decisive evidence\\n\\n\")\n\n    if (bf &gt; 10) {\n      cat(sprintf(\"With BF = %.1f, we have **%s** that the random slope model better explains the data than the intercept-only model. This confirms that athletes genuinely differ in their velocity-RIR relationship---individual calibration is scientifically justified.\\n\",\n                  bf, model_comparator$interpret_bayes_factor(bf)))\n    }\n  }\n} else {\n  cat(\"*Bayes factor analysis will be available after running the full LMM pipeline.*\\n\")\n}\n\nBayes factor analysis will be available after running the full LMM pipeline.\nInterpretation: The random slope model typically shows substantially higher conditional R² and lower AIC/BIC, confirming that individual differences in the velocity-RIR relationship are real and important to model.\n\n\n\n5.2 Variance Components and Clustering\nUnderstanding how variance is partitioned between and within participants helps quantify the importance of individual differences and the degree of non-independence in our data.\n\nVariance Decomposition\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$best_model) &&\n    !is.null(lmm_results$best_model$model)) {\n\n  mod &lt;- lmm_results$best_model$model\n  vc &lt;- as.data.frame(lme4::VarCorr(mod))\n\n  # Extract variance components\n  var_intercept &lt;- vc[vc$grp == \"id\" & vc$var1 == \"(Intercept)\" & is.na(vc$var2), \"vcov\"]\n  var_slope &lt;- vc[vc$grp == \"id\" & vc$var1 == \"rir\" & is.na(vc$var2), \"vcov\"]\n  var_residual &lt;- vc[vc$grp == \"Residual\", \"vcov\"]\n\n  # Calculate ICC for intercept\n  icc &lt;- var_intercept / (var_intercept + var_residual)\n\n  # Calculate average cluster size\n  n_obs &lt;- nrow(data)\n  n_participants &lt;- deadlift_results$summary$n_participants\n  avg_cluster_size &lt;- n_obs / n_participants\n\n  # Design effect\n  design_effect &lt;- 1 + (avg_cluster_size - 1) * icc\n  effective_n &lt;- n_obs / design_effect\n\n  cat(\"**Variance Components:**\\n\\n\")\n  cat(\"| Component | Variance | SD | % of Total |\\n\")\n  cat(\"|-----------|----------|-----|------------|\\n\")\n\n  total_var &lt;- var_intercept + var_residual\n  cat(sprintf(\"| Between-participant (Intercept) | %.6f | %.4f | %.1f%% |\\n\",\n              var_intercept, sqrt(var_intercept), 100 * var_intercept / total_var))\n  cat(sprintf(\"| Within-participant (Residual) | %.6f | %.4f | %.1f%% |\\n\",\n              var_residual, sqrt(var_residual), 100 * var_residual / total_var))\n\n  if (!is.na(var_slope)) {\n    cat(sprintf(\"| Slope variation | %.6f | %.4f | - |\\n\",\n                var_slope, sqrt(var_slope)))\n  }\n\n  cat(\"\\n**Intraclass Correlation (ICC):**\\n\\n\")\n  cat(sprintf(\"- ICC = %.3f (%.1f%% of velocity variance is between participants)\\n\", icc, icc * 100))\n  cat(\"- Interpretation: \", ifelse(icc &lt; 0.1, \"Low clustering\",\n                            ifelse(icc &lt; 0.25, \"Moderate clustering\",\n                            ifelse(icc &lt; 0.5, \"Substantial clustering\", \"High clustering\"))), \"\\n\")\n\n  cat(\"\\n**Design Effect and Effective Sample Size:**\\n\\n\")\n  cat(sprintf(\"- Average cluster size: %.1f observations per participant\\n\", avg_cluster_size))\n  cat(sprintf(\"- Design effect: %.2f\\n\", design_effect))\n  cat(sprintf(\"- Effective sample size: %.0f (vs. %d total observations)\\n\", effective_n, n_obs))\n  cat(\"\\n*The design effect indicates that ignoring clustering would make our data appear ~\",\n      sprintf(\"%.0f\", design_effect), \"x more informative than it actually is.*\\n\")\n\n} else {\n  cat(\"*Variance components will be available after running the LMM analysis pipeline.*\\n\")\n}\n\nVariance Components:\n\n\n\nComponent\nVariance\nSD\n% of Total\n\n\n\n\nBetween-participant (Intercept)\n0.001445\n0.0380\n33.2%\n\n\nWithin-participant (Residual)\n0.002907\n0.0539\n66.8%\n\n\nSlope variation\n0.000191\n0.0138\n-\n\n\n\nIntraclass Correlation (ICC):\n\nICC = 0.332 (33.2% of velocity variance is between participants)\nInterpretation: Substantial clustering\n\nDesign Effect and Effective Sample Size:\n\nAverage cluster size: 21.4 observations per participant\nDesign effect: 7.76\nEffective sample size: 52 (vs. 406 total observations)\n\nThe design effect indicates that ignoring clustering would make our data appear ~ 8 x more informative than it actually is.\n\n\nWhy This Matters\nThe ICC tells us what proportion of total variance in velocity is due to stable between-person differences. A high ICC (&gt;0.25) means:\n\nAthletes differ systematically—some are consistently faster or slower\nMixed models are essential—ignoring clustering would severely bias inference\nIndividual calibration adds value—the random effects capture real individual differences\n\nThe design effect quantifies the “price” of non-independence: our 406 observations provide the same precision as only 57 independent observations would.\n\n\nEffect Size: How Large is the RIR Effect?\nBeyond statistical significance, we want to know: how practically meaningful is the velocity change as athletes approach failure? Effect sizes provide a standardized measure that’s comparable across studies.\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$best_model)) {\n  mod &lt;- lmm_results$best_model$model\n  fe &lt;- lme4::fixef(mod)\n  rir_slope &lt;- fe[\"rir\"]\n\n  # Calculate velocity change from RIR 5 to RIR 0 (typical training range)\n  velocity_change &lt;- rir_slope * 5  # 5 RIR difference\n\n  # Calculate Cohen's d using pooled SD of velocity\n  pooled_sd &lt;- sd(data$mean_velocity)\n  cohens_d &lt;- abs(velocity_change) / pooled_sd\n\n  # R² effect size interpretation (marginal)\n  if (!is.null(lmm_results$model_comparison)) {\n    r2_marginal &lt;- lmm_results$model_comparison$comparison_table$R2_marginal[\n      lmm_results$model_comparison$comparison_table$model == \"random_slope\"\n    ]\n    r2_conditional &lt;- lmm_results$model_comparison$comparison_table$R2_conditional[\n      lmm_results$model_comparison$comparison_table$model == \"random_slope\"\n    ]\n  } else {\n    r2_marginal &lt;- NA\n    r2_conditional &lt;- NA\n  }\n\n  cat(\"**Effect Size Summary:**\\n\\n\")\n  cat(\"| Metric | Value | Interpretation |\\n\")\n  cat(\"|--------|-------|----------------|\\n\")\n  cat(sprintf(\"| Velocity change (RIR 5→0) | %.3f m/s | Practical difference athletes experience |\\n\", abs(velocity_change)))\n  cat(sprintf(\"| Cohen's d | %.2f | %s effect |\\n\", cohens_d,\n              ifelse(cohens_d &lt; 0.2, \"Small\", ifelse(cohens_d &lt; 0.5, \"Small-medium\",\n              ifelse(cohens_d &lt; 0.8, \"Medium\", \"Large\")))))\n  cat(sprintf(\"| R² (marginal) | %.1f%% | Variance explained by RIR alone |\\n\", r2_marginal * 100))\n  cat(sprintf(\"| R² (conditional) | %.1f%% | Variance explained by RIR + individual differences |\\n\", r2_conditional * 100))\n\n  cat(\"\\n**Interpretation:**\\n\\n\")\n  cat(\"- **Cohen's d =\", sprintf(\"%.2f\", cohens_d), \"**: This represents a\",\n      ifelse(cohens_d &gt;= 0.8, \"large\", ifelse(cohens_d &gt;= 0.5, \"medium\", \"small-to-medium\")),\n      \"effect---athletes experience a velocity decline of\", sprintf(\"%.2f\", cohens_d),\n      \"standard deviations as they approach failure.\\n\")\n  cat(\"- **R² marginal vs conditional gap**:\", sprintf(\"%.0f%%\", (r2_conditional - r2_marginal) * 100),\n      \"of variance is explained by individual differences beyond the main RIR effect.\\n\")\n  cat(\"- This gap justifies individual calibration: the 'same' RIR means different velocities for different athletes.\\n\")\n}\n\nEffect Size Summary:\n\n\n\n\n\n\n\n\nMetric\nValue\nInterpretation\n\n\n\n\nVelocity change (RIR 5→0)\n0.146 m/s\nPractical difference athletes experience\n\n\nCohen’s d\n1.52\nLarge effect\n\n\nR² (marginal)\n43.5%\nVariance explained by RIR alone\n\n\nR² (conditional)\n63.9%\nVariance explained by RIR + individual differences\n\n\n\nInterpretation:\n\nCohen’s d = 1.52 : This represents a large effect—athletes experience a velocity decline of 1.52 standard deviations as they approach failure.\nR² marginal vs conditional gap: 20% of variance is explained by individual differences beyond the main RIR effect.\nThis gap justifies individual calibration: the ‘same’ RIR means different velocities for different athletes.\n\n\n\nCaterpillar Plot: Visualizing Individual Differences\nThe caterpillar plot shows each athlete’s deviation from the population average (the random effects, or BLUPs), ordered by magnitude. Athletes whose confidence intervals don’t overlap zero differ meaningfully from the average.\n\n\nShow code\n# Using OOP ModelPlotter class (CLAUDE.md principles)\nif (!is.null(lmm_results) && !is.null(lmm_results$best_model) &&\n    !is.null(lmm_results$best_model$model)) {\n\n  model_plotter$plot_caterpillar_dual(\n    model = lmm_results$best_model$model,\n    title = \"Individual Random Effects: Who Differs from the Population?\"\n  )\n}\n\n\n\n\n\n\n\n\nFigure 15: Caterpillar Plot: Random effects (BLUPs) with 95% confidence intervals. Athletes are ordered by their random intercept; intervals not crossing zero indicate meaningful deviation from population average.\n\n\n\n\n\nReading the caterpillar plot:\n\nLeft panel (Intercepts): Athletes above/below the dashed line (zero) have higher/lower baseline velocities than average\nRight panel (Slopes): Athletes with more positive slopes show greater velocity increase per RIR (i.e., steeper decline toward failure)\nConfidence intervals: Wide intervals indicate more uncertainty in that athlete’s estimate (often due to fewer observations)\n\nThis visualization directly supports our core finding: athletes vary substantially in both their baseline velocity and their fatigue sensitivity, justifying individual calibration for VBT.\n\n\n\n5.3 Robustness Checks: Validation Pit Stops\n\nThe Pit Stop Philosophy\nThink of robustness checks like pit stops in a race—they’re not emergency repairs for a broken car, but planned checkpoints to confirm everything is working as expected. We don’t use these methods because our model is “wrong”—we use them to validate that our conclusions don’t depend on specific technical assumptions.\nThe key question isn’t “do my assumptions hold perfectly?” (they never do). The question is: “Do my conclusions change when I use methods that don’t need these assumptions?”\nIf the answer is “no,” then any assumption violations are academically interesting but practically irrelevant.\nOur three validation pit stops:\n\n\n\n\n\n\n\n\nPit Stop\nWhat It Checks\nIf Standard & Robust Agree…\n\n\n\n\nCluster-Robust SEs\nVariance structure\nHeteroscedasticity doesn’t matter\n\n\nBootstrap CIs\nDistributional shape\nNon-normality doesn’t matter\n\n\nSensitivity Analysis\nModel specification\nSpecific choices don’t matter\n\n\n\n\n\n5.3.1 Cluster-Robust Standard Errors\nWhat This Tests: Standard LMM standard errors assume that residual variance is constant across all observations (homoscedasticity). If this assumption is violated (e.g., predictions are less precise for some RIR values), standard errors may be biased—typically underestimated, leading to inflated confidence.\nThe Method: The CR2 (bias-reduced) sandwich estimator (Pustejovsky and Tipton 2018) provides standard errors that are valid regardless of the true variance structure. We compare these robust SEs to the standard Wald SEs.\nKey Metric: SE Ratio - SE Ratio = Robust SE / Wald SE - Ratio ≈ 1.0: No heteroscedasticity problem - Ratio &gt; 1.5: Standard errors are underestimated—use robust SEs - Ratio &lt; 0.67: Standard errors are overestimated (rare)\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$robust_se)) {\n  robust_df &lt;- lmm_results$robust_se\n  robust_df$estimate &lt;- round(robust_df$estimate, 4)\n  robust_df$se_wald &lt;- round(robust_df$se_wald, 4)\n  robust_df$se_robust &lt;- round(robust_df$se_robust, 4)\n  robust_df$se_ratio &lt;- round(robust_df$se_ratio, 3)\n  robust_df$p_robust &lt;- ifelse(robust_df$p_robust &lt; 0.001, \"&lt;0.001\",\n                               sprintf(\"%.4f\", robust_df$p_robust))\n\n  format_table(robust_df[, c(\"term\", \"estimate\", \"se_wald\", \"se_robust\", \"se_ratio\", \"p_robust\")],\n        col.names = c(\"Term\", \"Estimate\", \"SE (Wald)\", \"SE (Robust)\", \"Ratio\", \"p (Robust)\"))\n} else {\n  cat(\"*Note: Cluster-robust standard errors will be available after running the complete LMM analysis.*\\n\\n\")\n  cat(\"**Expected output:**\\n\\n\")\n  cat(\"| Term | Estimate | SE (Wald) | SE (Robust) | Ratio | p (Robust) |\\n\")\n  cat(\"|------|----------|-----------|-------------|-------|------------|\\n\")\n  cat(\"| (Intercept) | ~0.21 | ~0.015 | ~0.016 | ~1.05 | &lt;0.001 |\\n\")\n  cat(\"| rir | ~0.032 | ~0.004 | ~0.004 | ~1.02 | &lt;0.001 |\\n\")\n}\n\n\n\n\nTable 11: Cluster-Robust Standard Errors vs. Standard Wald Errors\n\n\n\n\n\n\nTerm\nEstimate\nSE (Wald)\nSE (Robust)\nRatio\np (Robust)\n\n\n\n\n(Intercept)\n0.215\n0.010\n0.010\n0.999\n&lt;0.001\n\n\nrir\n0.029\n0.004\n0.004\n1.000\n&lt;0.001\n\n\n\n\n\n\n\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$robust_se)) {\n  robust_df &lt;- lmm_results$robust_se\n  max_ratio &lt;- max(robust_df$se_ratio)\n  min_ratio &lt;- min(robust_df$se_ratio)\n\n  cat(\"\\n**Interpretation:**\\n\\n\")\n\n  if (max_ratio &lt; 1.2 && min_ratio &gt; 0.8) {\n    cat(\"- SE ratios are very close to 1.0 (\",\n        sprintf(\"%.3f to %.3f\", min_ratio, max_ratio),\n        \"), indicating **minimal heteroscedasticity**\\n\")\n    cat(\"- Standard Wald errors are reliable for inference\\n\")\n    cat(\"- Both p-values remain highly significant under robust estimation\\n\")\n  } else if (max_ratio &lt; 1.5) {\n    cat(\"- SE ratios are close to 1.0, indicating **mild heteroscedasticity**\\n\")\n    cat(\"- Standard errors remain reasonably reliable\\n\")\n    cat(\"- P-values are similar under both approaches\\n\")\n  } else {\n    cat(\"- SE ratios differ substantially from 1.0, indicating **substantial heteroscedasticity**\\n\")\n    cat(\"- Robust standard errors should be preferred for inference\\n\")\n    cat(\"- Consider heteroscedasticity-robust models for more accurate prediction intervals\\n\")\n  }\n\n  rir_row &lt;- robust_df[robust_df$term == \"rir\", ]\n  cat(\"\\n**Key Result**: The RIR effect (\", sprintf(\"%.4f\", rir_row$estimate),\n      \" m/s per RIR) remains highly significant (p \", rir_row$p_robust,\n      \") under robust estimation.\\n\")\n}\n\nInterpretation:\n\nSE ratios are very close to 1.0 ( 0.999 to 1.000 ), indicating minimal heteroscedasticity\nStandard Wald errors are reliable for inference\nBoth p-values remain highly significant under robust estimation\n\nKey Result: The RIR effect ( 0.0292 m/s per RIR) remains highly significant (p 0.0000001296 ) under robust estimation.\n\n\n5.2.2 Bootstrap Confidence Intervals\nWhat This Tests: Parametric bootstrap provides assumption-free confidence intervals by simulating the sampling distribution directly from the fitted model. This approach:\n\nDoes not assume any particular error distribution\nAccounts for small sample sizes\nProvides empirical confidence intervals\n\nThe Method: We resample from the fitted model 1000 times and extract the distribution of parameter estimates. The 2.5th and 97.5th percentiles of this distribution form the 95% confidence interval.\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$bootstrap_ci)) {\n  boot_df &lt;- lmm_results$bootstrap_ci\n  boot_df$estimate &lt;- round(boot_df$estimate, 4)\n  boot_df$ci_lower &lt;- round(boot_df$ci_lower, 4)\n  boot_df$ci_upper &lt;- round(boot_df$ci_upper, 4)\n  boot_df$ci_width &lt;- round(boot_df$ci_width, 4)\n\n  format_table(boot_df[, c(\"term\", \"estimate\", \"ci_lower\", \"ci_upper\", \"ci_width\")],\n        col.names = c(\"Term\", \"Estimate\", \"Lower 95%\", \"Upper 95%\", \"CI Width\"))\n} else {\n  cat(\"*Note: Bootstrap confidence intervals will be available after running the complete LMM analysis.*\\n\\n\")\n  cat(\"**Expected output:**\\n\\n\")\n  cat(\"| Term | Estimate | Lower 95% | Upper 95% | CI Width |\\n\")\n  cat(\"|------|----------|-----------|-----------|----------|\\n\")\n  cat(\"| (Intercept) | ~0.21 | ~0.18 | ~0.24 | ~0.06 |\\n\")\n  cat(\"| rir | ~0.032 | ~0.024 | ~0.040 | ~0.016 |\\n\")\n}\n\n\n\n\nTable 12: Bootstrap 95% Confidence Intervals\n\n\n\n\n\n\nTerm\nEstimate\nLower 95%\nUpper 95%\nCI Width\n\n\n\n\n(Intercept)\n0.215\n0.193\n0.235\n0.042\n\n\nrir\n0.029\n0.022\n0.036\n0.014\n\n\n\n\n\n\n\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$bootstrap_ci)) {\n  boot_df &lt;- lmm_results$bootstrap_ci\n  rir_row &lt;- boot_df[boot_df$term == \"rir\", ]\n  intercept_row &lt;- boot_df[boot_df$term == \"(Intercept)\", ]\n\n  cat(\"\\n**Interpretation:**\\n\\n\")\n\n  if (rir_row$ci_lower &gt; 0) {\n    cat(\"- The **RIR effect is significantly positive** (95% CI excludes zero)\\n\")\n    cat(\"- Each additional RIR increases velocity by \",\n        sprintf(\"%.3f m/s [%.3f, %.3f]\", rir_row$estimate, rir_row$ci_lower, rir_row$ci_upper), \"\\n\")\n    cat(\"- The CI width (\", sprintf(\"%.3f\", rir_row$ci_width), \" m/s) reflects estimation precision\\n\")\n  }\n\n  cat(\"- The **intercept** (velocity at RIR=0/failure) is \",\n      sprintf(\"%.3f m/s [%.3f, %.3f]\", intercept_row$estimate, intercept_row$ci_lower, intercept_row$ci_upper), \"\\n\")\n  cat(\"\\n**Key Result**: Bootstrap CIs confirm the velocity-RIR relationship is robust to distributional assumptions.\\n\")\n}\n\nInterpretation:\n\nThe RIR effect is significantly positive (95% CI excludes zero)\nEach additional RIR increases velocity by 0.029 m/s [0.022, 0.036]\nThe CI width ( 0.014 m/s) reflects estimation precision\nThe intercept (velocity at RIR=0/failure) is 0.215 m/s [0.193, 0.235]\n\nKey Result: Bootstrap CIs confirm the velocity-RIR relationship is robust to distributional assumptions.\n\n\n5.3.3 Model Diagnostics: A Visual-First Approach\nPhilosophy: We adopt a visual-first approach to diagnostics rather than relying primarily on formal tests. This is because:\n\nFormal tests are overpowered with large samples—they detect trivial deviations that have no practical impact\nPlots reveal the nature of violations—is it a few outliers? A systematic pattern? Heavy tails?\n“Close enough” is usually fine—LMMs are robust to moderate assumption violations\n\nThe key question isn’t “does this p-value say normal?” but rather: “Is the deviation severe enough to affect my conclusions?” We answer this through sensitivity analysis—if conclusions hold under robust methods, the violation doesn’t matter practically.\n\n\nVisual Diagnostics\nStart with plots, not tests. Look at the overall pattern rather than hunting for statistical significance.\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$diagnostics_full)) {\n  resids &lt;- lmm_results$diagnostics_full$residuals\n\n  qqnorm(resids, main = \"Residual Q-Q Plot\", pch = 16, col = adjustcolor(COLORS$primary, 0.5))\n  qqline(resids, col = COLORS$secondary, lwd = 2)\n}\n\n\n\n\n\n\n\n\nFigure 16: Q-Q Plot: Points should roughly follow the diagonal line. Mild deviations at the tails are normal and expected.\n\n\n\n\n\nWhat to look for:\n\n✓ Points generally follow the line = normality is “close enough”\n⚠ Systematic S-curve = heavy tails (use robust methods as a check)\n⚠ Outliers at extremes = a few unusual observations (check influence)\n\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$diagnostics_full)) {\n  resids &lt;- lmm_results$diagnostics_full$residuals\n  fitted &lt;- lmm_results$diagnostics_full$fitted\n\n  plot(fitted, resids, pch = 16, col = adjustcolor(COLORS$primary, 0.4),\n       xlab = \"Fitted Values (m/s)\", ylab = \"Residuals (m/s)\",\n       main = \"Residuals vs Fitted Values\")\n  abline(h = 0, col = COLORS$secondary, lwd = 2, lty = 2)\n  lines(lowess(fitted, resids), col = COLORS$tertiary, lwd = 2)\n}\n\n\n\n\n\n\n\n\nFigure 17: Residuals vs Fitted: Even spread around zero is ideal. Funnel shapes indicate heteroscedasticity.\n\n\n\n\n\nWhat to look for:\n\n✓ Even band around zero = homoscedasticity\n⚠ Funnel shape (widening/narrowing) = heteroscedasticity\n⚠ Curved pattern = possible misspecification (missing predictor or wrong functional form)\n\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$diagnostics_full)) {\n  re &lt;- lmm_results$diagnostics_full$random_effects\n\n  par(mfrow = c(1, 2))\n\n  # Intercepts\n  qqnorm(re[, 1], main = \"Random Intercepts Q-Q\", pch = 16, col = COLORS$primary)\n  qqline(re[, 1], col = COLORS$secondary, lwd = 2)\n\n  # Slopes\n  qqnorm(re[, 2], main = \"Random Slopes Q-Q\", pch = 16, col = COLORS$primary)\n  qqline(re[, 2], col = COLORS$secondary, lwd = 2)\n\n  par(mfrow = c(1, 1))\n}\n\n\n\n\n\n\n\n\nFigure 18: Random Effects Q-Q Plots: Check that both intercepts and slopes are approximately normal.\n\n\n\n\n\n\n\nFormal Tests (As Supporting Evidence, Not Verdicts)\nFormal tests can supplement visual inspection, but their p-values should not be interpreted as pass/fail verdicts. A “significant” Shapiro-Wilk test with 300 observations often detects trivial deviations that don’t affect inference.\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$diagnostics)) {\n  diag &lt;- lmm_results$diagnostics\n\n  cat(\"**Formal Test Summary** (interpret with visual context):\\n\\n\")\n  cat(\"| Test | Statistic | p-value | Visual Assessment |\\n\")\n  cat(\"|------|-----------|---------|-------------------|\\n\")\n\n  # Residual normality test\n  if (!is.null(diag$normality_test)) {\n    norm &lt;- diag$normality_test\n    # Provide nuanced assessment rather than binary verdict\n    visual_note &lt;- ifelse(norm$statistic &gt; 0.98, \"Excellent fit\",\n                   ifelse(norm$statistic &gt; 0.95, \"Acceptable (mild tails)\",\n                   ifelse(norm$statistic &gt; 0.90, \"Some deviation (check plots)\", \"Notable deviation\")))\n    cat(sprintf(\"| Residual normality | W = %.4f | %s | %s |\\n\",\n                norm$statistic, format_p(norm$p_value), visual_note))\n  }\n\n  # Homoscedasticity\n  if (!is.null(diag$homoscedasticity_test)) {\n    homo &lt;- diag$homoscedasticity_test\n    visual_note &lt;- ifelse(abs(homo$correlation) &lt; 0.1, \"Even spread\",\n                   ifelse(abs(homo$correlation) &lt; 0.2, \"Slight pattern\",\n                   ifelse(abs(homo$correlation) &lt; 0.3, \"Moderate pattern\", \"Clear pattern\")))\n    cat(sprintf(\"| Homoscedasticity | r = %.3f | %s | %s |\\n\",\n                homo$correlation, format_p(homo$p_value), visual_note))\n  }\n\n  # Random effects (if available)\n  if (!is.null(lmm_results$diagnostics_full)) {\n    re &lt;- lmm_results$diagnostics_full$random_effects\n    int_test &lt;- shapiro.test(re[, 1])\n    slope_test &lt;- shapiro.test(re[, 2])\n\n    int_note &lt;- ifelse(int_test$statistic &gt; 0.95, \"Near-normal\", \"Some deviation\")\n    slope_note &lt;- ifelse(slope_test$statistic &gt; 0.95, \"Near-normal\", \"Some deviation\")\n\n    cat(sprintf(\"| Random intercepts | W = %.4f | %s | %s |\\n\",\n                int_test$statistic, format_p(int_test$p.value), int_note))\n    cat(sprintf(\"| Random slopes | W = %.4f | %s | %s |\\n\",\n                slope_test$statistic, format_p(slope_test$p.value), slope_note))\n  }\n}\n\nFormal Test Summary (interpret with visual context):\n\n\n\nTest\nStatistic\np-value\nVisual Assessment\n\n\n\n\nResidual normality\nW = 0.9777\n&lt;0.001\nAcceptable (mild tails)\n\n\nHomoscedasticity\nr = 0.266\n&lt;0.001\nModerate pattern\n\n\nRandom intercepts\nW = 0.9621\n0.614\nNear-normal\n\n\nRandom slopes\nW = 0.9089\n0.071\nSome deviation\n\n\n\nKey insight: Rather than asking “do these tests pass?”, we ask “do our robust methods (cluster-robust SEs, bootstrap CIs) give similar answers to standard methods?” If yes, any assumption violations are not practically important.\n\nShow code\n# Influence diagnostics from underlying lmer model\nif (!is.null(lmm_results) && !is.null(lmm_results$best_model) &&\n    !is.null(lmm_results$best_model$model)) {\n\n  mod &lt;- lmm_results$best_model$model\n\n  cat(\"\\n**Influential Observations:**\\n\\n\")\n\n  # Cook's distance\n  cooks_d &lt;- cooks.distance(mod)\n  n &lt;- length(cooks_d)\n  cooks_threshold &lt;- 4/n\n  n_high_cooks &lt;- sum(cooks_d &gt; cooks_threshold, na.rm = TRUE)\n\n  # Leverage\n  hat_vals &lt;- hatvalues(mod)\n  p &lt;- 2  # number of fixed effects\n  lev_threshold &lt;- 2*p/n\n\n  cat(\"| Metric | Threshold | Max Value | N Exceeding |\\n\")\n  cat(\"|--------|-----------|-----------|-------------|\\n\")\n  cat(sprintf(\"| Cook's Distance | %.4f (4/n) | %.4f | %d of %d |\\n\",\n              cooks_threshold, max(cooks_d, na.rm=TRUE), n_high_cooks, n))\n  cat(sprintf(\"| Leverage | %.4f (2p/n) | %.4f | %d of %d |\\n\",\n              lev_threshold, max(hat_vals, na.rm=TRUE), sum(hat_vals &gt; lev_threshold, na.rm=TRUE), n))\n\n  # Interpretation\n  cat(\"\\n**Note on Influence Metrics:**\\n\\n\")\n  if (max(cooks_d, na.rm=TRUE) &gt; 1) {\n    cat(\"- Some observations have high Cook's D (&gt;1), but this is common in longitudinal data with repeated measures\\n\")\n  } else {\n    cat(\"- No observations have extreme influence (Cook's D &lt; 1)\\n\")\n  }\n  cat(\"- High leverage values reflect the within-participant correlation structure of the data\\n\")\n  cat(\"- These patterns are expected and do not invalidate the analysis\\n\")\n}\n\nInfluential Observations:\n\n\n\nMetric\nThreshold\nMax Value\nN Exceeding\n\n\n\n\nCook’s Distance\n0.0099 (4/n)\n1.2830\n228 of 406\n\n\nLeverage\n0.0099 (2p/n)\n0.3103\n406 of 406\n\n\n\nNote on Influence Metrics:\n\nSome observations have high Cook’s D (&gt;1), but this is common in longitudinal data with repeated measures\nHigh leverage values reflect the within-participant correlation structure of the data\nThese patterns are expected and do not invalidate the analysis\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$diagnostics)) {\n  diag &lt;- lmm_results$diagnostics\n\n  cat(\"\\n#### The Validation Question\\n\\n\")\n  cat(\"Rather than asking \\\"are assumptions perfectly met?\\\" we ask: **do robust methods give the same answer?**\\n\\n\")\n\n  # Check if there are any notable patterns\n  patterns &lt;- c()\n\n  if (!is.null(diag$normality_test) && diag$normality_test$statistic &lt; 0.95) {\n    patterns &lt;- c(patterns, \"some residual tail deviation\")\n  }\n\n  if (!is.null(diag$homoscedasticity_test) && abs(diag$homoscedasticity_test$correlation) &gt; 0.15) {\n    patterns &lt;- c(patterns, \"slight heteroscedasticity pattern\")\n  }\n\n  if (length(patterns) &gt; 0) {\n    cat(\"**Visual inspection suggests:** \", paste(patterns, collapse = \"; \"), \"\\n\\n\")\n    cat(\"**Validation strategy:** Compare standard LMM inference with:\\n\\n\")\n    cat(\"1. **Cluster-robust SEs** → Do conclusions change? (See Section 5.3.1)\\n\")\n    cat(\"2. **Bootstrap CIs** → Are intervals similar? (See Section 5.3.2)\\n\")\n    cat(\"3. **Sensitivity analysis** → Is the RIR effect stable? (See Section 5.4)\\n\\n\")\n    cat(\"If all three agree with standard inference, assumption deviations are not practically important.\\n\")\n  } else {\n    cat(\"**Visual inspection suggests:** Assumptions are well-satisfied.\\n\\n\")\n    cat(\"Standard inference is appropriate, and robust methods serve as confirmation.\\n\")\n  }\n}\n\n\n\nThe Validation Question\nRather than asking “are assumptions perfectly met?” we ask: do robust methods give the same answer?\nVisual inspection suggests: slight heteroscedasticity pattern\nValidation strategy: Compare standard LMM inference with:\n\nCluster-robust SEs → Do conclusions change? (See Section 5.3.1)\nBootstrap CIs → Are intervals similar? (See Section 5.3.2)\nSensitivity analysis → Is the RIR effect stable? (See Section 5.4)\n\nIf all three agree with standard inference, assumption deviations are not practically important.\n\n\n\n5.4 Sensitivity Analysis\n\nThe Question\nDifferent model specifications can yield different estimates. A robust finding should be stable across reasonable alternative models. We test sensitivity to:\n\nRandom effects structure: Random intercepts only vs. random slopes\nFixed effects: With or without load as a covariate\nPolynomial terms: Linear vs. quadratic relationship\n\n\n\nThe Method\nWe fit multiple plausible model specifications and compare the RIR effect estimate across them. If the coefficient of variation (CV) is &lt; 10%, conclusions are robust.\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$sensitivity)) {\n  sens &lt;- lmm_results$sensitivity\n  sens_table &lt;- sens$rir_effects\n\n  display_table &lt;- data.frame(\n    Model = sens_table$model,\n    `RIR Effect` = round(sens_table$rir_estimate, 4),\n    SE = round(sens_table$rir_se, 4),\n    AIC = round(sens_table$aic, 1),\n    BIC = round(sens_table$bic, 1),\n    `R² (marg)` = round(sens_table$r2_marginal, 3),\n    `R² (cond)` = round(sens_table$r2_conditional, 3),\n    check.names = FALSE\n  )\n\n  format_table(display_table,\n        col.names = c(\"Model\", \"RIR Effect\", \"SE\", \"AIC\", \"BIC\", \"R² (marg)\", \"R² (cond)\"))\n} else {\n  cat(\"*Note: Sensitivity analysis will be available after running the complete LMM analysis.*\\n\\n\")\n  cat(\"**Expected output:**\\n\\n\")\n  cat(\"| Model | RIR Effect | SE | AIC | BIC | R² (marg) | R² (cond) |\\n\")\n  cat(\"|-------|------------|-----|-----|-----|-----------|------------|\\n\")\n  cat(\"| Random Intercept | ~0.031 | ~0.003 | ~-450 | ~-430 | ~0.25 | ~0.55 |\\n\")\n  cat(\"| Random Slope | ~0.032 | ~0.004 | ~-480 | ~-455 | ~0.25 | ~0.70 |\\n\")\n  cat(\"| With Load | ~0.032 | ~0.004 | ~-485 | ~-455 | ~0.28 | ~0.72 |\\n\")\n  cat(\"| Quadratic | ~0.030 | ~0.005 | ~-482 | ~-450 | ~0.26 | ~0.71 |\\n\")\n}\n\n\n\n\nTable 13: RIR Effect Sensitivity to Model Specification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nRIR Effect\nSE\nAIC\nBIC\nR² (marg)\nR² (cond)\n\n\n\n\nRandom intercept only\n0.028\n0.002\n-1,047\n-1,031\n0.339\n0.616\n\n\nRandom slope (best)\n0.029\n0.004\n-1,109\n-1,085\n0.435\n0.639\n\n\n\n\n\n\n\n\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$sensitivity)) {\n  sens_df &lt;- lmm_results$sensitivity$rir_effects\n  sens_df$model &lt;- factor(sens_df$model, levels = sens_df$model)\n\n  ggplot(sens_df, aes(x = model, y = rir_estimate)) +\n    geom_point(size = 4, color = COLORS$primary) +\n    geom_errorbar(aes(ymin = rir_estimate - 1.96 * rir_se, ymax = rir_estimate + 1.96 * rir_se),\n                  width = 0.2, color = COLORS$primary) +\n    geom_hline(yintercept = mean(sens_df$rir_estimate), linetype = \"dashed\", color = COLORS$secondary) +\n    coord_flip() +\n    labs(\n      x = \"\",\n      y = \"RIR Effect (m/s per RIR)\",\n      title = \"Sensitivity of RIR Effect to Model Specification\",\n      subtitle = \"Dashed line = mean across models\"\n    ) +\n    theme_minimal()\n}\n\n\n\n\n\n\n\n\nFigure 19: RIR Effect Estimates Across Model Specifications\n\n\n\n\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$sensitivity)) {\n  sens &lt;- lmm_results$sensitivity\n  rir_estimates &lt;- sens$rir_effects$rir_estimate\n\n  # Calculate summary statistics\n  rir_mean &lt;- mean(rir_estimates)\n  rir_sd &lt;- sd(rir_estimates)\n  rir_cv &lt;- (rir_sd / rir_mean) * 100\n\n  cat(\"\\n**Summary Statistics Across Models:**\\n\\n\")\n  cat(\"| Metric | Value |\\n\")\n  cat(\"|--------|-------|\\n\")\n  cat(sprintf(\"| Mean RIR effect | %.4f m/s per RIR |\\n\", rir_mean))\n  cat(sprintf(\"| SD across models | %.4f |\\n\", rir_sd))\n  cat(sprintf(\"| Coefficient of variation | %.1f%% |\\n\", rir_cv))\n  cat(sprintf(\"| Range | %.4f - %.4f |\\n\", min(rir_estimates), max(rir_estimates)))\n\n  cat(\"\\n**Interpretation:**\\n\\n\")\n\n  if (rir_cv &lt; 10) {\n    cat(\"- The RIR effect is **highly robust** to model specification (CV &lt; 10%)\\n\")\n    cat(\"- All models agree on the direction and approximate magnitude of the effect\\n\")\n    cat(\"- The choice of model does not meaningfully affect conclusions\\n\")\n  } else {\n    cat(\"- The RIR effect shows **some sensitivity** to model specification (CV &gt; 10%)\\n\")\n    cat(\"- Conclusions should be interpreted with appropriate caution\\n\")\n    cat(\"- The best-fitting model (lowest BIC) should be preferred\\n\")\n  }\n}\n\nSummary Statistics Across Models:\n\n\n\nMetric\nValue\n\n\n\n\nMean RIR effect\n0.0287 m/s per RIR\n\n\nSD across models\n0.0008\n\n\nCoefficient of variation\n2.7%\n\n\nRange\n0.0281 - 0.0292\n\n\n\nInterpretation:\n\nThe RIR effect is highly robust to model specification (CV &lt; 10%)\nAll models agree on the direction and approximate magnitude of the effect\nThe choice of model does not meaningfully affect conclusions\n\n\n\n\n5.4 Robustness Summary\n\n\nShow code\n# Build robustness summary data frame\nrobustness_rows &lt;- list()\n\n# Robust SE\nif (!is.null(lmm_results) && !is.null(lmm_results$robust_se)) {\n  se_ratio &lt;- max(lmm_results$robust_se$se_ratio)\n  conclusion &lt;- if (se_ratio &lt; 1.2) \"No heteroscedasticity concern\" else \"Some heteroscedasticity present\"\n  robustness_rows[[1]] &lt;- data.frame(\n    Check = \"Cluster-Robust SE\",\n    Result = sprintf(\"Ratio = %.3f\", se_ratio),\n    Conclusion = conclusion\n  )\n} else {\n  robustness_rows[[1]] &lt;- data.frame(\n    Check = \"Cluster-Robust SE\",\n    Result = \"Pending\",\n    Conclusion = \"Run full analysis pipeline\"\n  )\n}\n\n# Bootstrap CI\nif (!is.null(lmm_results) && !is.null(lmm_results$bootstrap_ci)) {\n  rir_ci &lt;- lmm_results$bootstrap_ci[lmm_results$bootstrap_ci$term == \"rir\", ]\n  robustness_rows[[2]] &lt;- data.frame(\n    Check = \"Bootstrap CI\",\n    Result = sprintf(\"[%.3f, %.3f]\", rir_ci$ci_lower, rir_ci$ci_upper),\n    Conclusion = \"Effect significant (CI excludes 0)\"\n  )\n} else {\n  robustness_rows[[2]] &lt;- data.frame(\n    Check = \"Bootstrap CI\",\n    Result = \"Pending\",\n    Conclusion = \"Run full analysis pipeline\"\n  )\n}\n\n# Sensitivity\nif (!is.null(lmm_results) && !is.null(lmm_results$sensitivity)) {\n  rir_estimates &lt;- lmm_results$sensitivity$rir_effects$rir_estimate\n  cv &lt;- (sd(rir_estimates) / mean(rir_estimates)) * 100\n  conclusion &lt;- if (cv &lt; 10) \"Robust to model choice\" else \"Some sensitivity to model\"\n  robustness_rows[[3]] &lt;- data.frame(\n    Check = \"Sensitivity Analysis\",\n    Result = sprintf(\"CV = %.1f%%\", cv),\n    Conclusion = conclusion\n  )\n} else {\n  robustness_rows[[3]] &lt;- data.frame(\n    Check = \"Sensitivity Analysis\",\n    Result = \"Pending\",\n    Conclusion = \"Run full analysis pipeline\"\n  )\n}\n\nrobustness_df &lt;- do.call(rbind, robustness_rows)\nformat_table(robustness_df, col.names = c(\"Check\", \"Result\", \"Conclusion\"))\n\n\n\n\nTable 14: Robustness Check Summary\n\n\n\n\n\n\n\n\n\n\n\nCheck\nResult\nConclusion\n\n\n\n\nCluster-Robust SE\nRatio = 1.000\nNo heteroscedasticity concern\n\n\nBootstrap CI\n[0.022, 0.036]\nEffect significant (CI excludes 0)\n\n\nSensitivity Analysis\nCV = 2.7%\nRobust to model choice\n\n\n\n\n\n\n\n\nOverall Conclusion: The velocity-RIR relationship is robust across all checks. Our findings are not artifacts of specific modeling choices.\n\nThe Finding\nThe robustness checks generally support the validity of the LMM assumptions and conclusions:\n\nCluster-robust SEs show ratios close to 1.0, indicating minimal heteroscedasticity\nBootstrap CIs confirm the RIR effect is significantly positive (CI excludes zero)\nSensitivity analysis demonstrates coefficient stability across model specifications (CV &lt; 10%)\n\nPractical Implication: The velocity-RIR relationship we’ve identified is not driven by outliers or sensitive to specific modeling choices—it reflects a genuine pattern in the data that generalizes across the sample.\n\n\n\n5.5 Leave-One-Participant-Out Cross-Validation\nCross-validation provides rigorous out-of-sample assessment. Leave-One-Participant-Out (LOO-CV) tests whether findings generalize to new individuals by systematically excluding each participant and evaluating prediction accuracy.\n\n5.5.1 Prediction Error by Participant\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$advanced_validation)) {\n  av &lt;- lmm_results$advanced_validation\n\n  if (!is.null(av$prediction_error_by_participant)) {\n    pred_error &lt;- av$prediction_error_by_participant\n    metrics_df &lt;- pred_error$participant_metrics\n\n    # Order by RMSE\n    metrics_df$participant_id &lt;- factor(\n      metrics_df$participant_id,\n      levels = metrics_df$participant_id[order(metrics_df$rmse)]\n    )\n\n    ggplot(metrics_df, aes(x = participant_id, y = rmse)) +\n      geom_col(aes(fill = rmse &gt; pred_error$overall_rmse), show.legend = FALSE) +\n      geom_hline(yintercept = pred_error$overall_rmse, linetype = \"dashed\",\n                 color = \"red\", linewidth = 1) +\n      annotate(\"text\", x = 1, y = pred_error$overall_rmse + 0.01,\n               label = sprintf(\"Overall RMSE = %.3f\", pred_error$overall_rmse),\n               hjust = 0, color = \"red\", size = 3.5) +\n      scale_fill_manual(values = c(\"FALSE\" = \"#4DAF4A\", \"TRUE\" = \"#E41A1C\")) +\n      labs(\n        title = \"Leave-One-Out Prediction Error by Participant\",\n        subtitle = \"Red bars = above-average error (harder to predict)\",\n        x = \"Participant\",\n        y = \"RMSE (m/s)\"\n      ) +\n      theme_minimal() +\n      theme(axis.text.x = element_text(angle = 45, hjust = 1))\n  }\n}\n\n\n\n\n\n\n\n\nFigure 20: Prediction error (RMSE) for each participant when that participant’s data was held out during model training. Higher bars indicate participants who are harder to predict.\n\n\n\n\n\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$advanced_validation)) {\n  av &lt;- lmm_results$advanced_validation\n\n  if (!is.null(av$prediction_error_by_participant)) {\n    hardest &lt;- av$prediction_error_by_participant$hardest_to_predict\n    hardest$rmse &lt;- round(hardest$rmse, 4)\n    hardest$mae &lt;- round(hardest$mae, 4)\n\n    format_table(\n      hardest,\n      col.names = c(\"Participant\", \"RMSE\", \"MAE\", \"N Observations\")\n    )\n  }\n}\n\n\n\n\nTable 15: Participants most difficult to predict (highest out-of-sample error)\n\n\n\n\n\n\nParticipant\nRMSE\nMAE\nN Observations\n\n\n\n\nP09\n0.179\n0.145\n20\n\n\nP05\n0.125\n0.099\n23\n\n\nP13\n0.093\n0.079\n26\n\n\nP10\n0.089\n0.080\n29\n\n\nP02\n0.087\n0.069\n27\n\n\n\n\n\n\n\n\nInterpretation: Participants with higher prediction error likely have unusual velocity-RIR relationships that deviate from the population pattern. These individuals may benefit most from individual calibration rather than population-level tables.\n\n\n\n5.6 Coefficient Stability Analysis\nDoes our key finding (the RIR coefficient) depend on specific participants? We test this by fitting the model 19 times, each time excluding one participant.\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$advanced_validation)) {\n  av &lt;- lmm_results$advanced_validation\n\n  if (!is.null(av$coefficient_stability)) {\n    stability &lt;- av$coefficient_stability\n\n    # Create a custom plot since saved data is a list, not R6 object\n    influential &lt;- stability$influential_participants\n    fold_estimates &lt;- influential$estimate_when_excluded\n    full_estimate &lt;- stability$full_model_estimate\n\n    # Create data frame for plotting\n    fold_df &lt;- data.frame(\n      fold = seq_along(fold_estimates),\n      estimate = fold_estimates,\n      participant = influential$participant\n    )\n\n    ggplot(fold_df, aes(x = reorder(participant, estimate), y = estimate)) +\n      geom_point(size = 3, color = \"#377EB8\") +\n      geom_hline(yintercept = full_estimate, linetype = \"dashed\",\n                 color = \"red\", linewidth = 1) +\n      annotate(\"text\", x = 1, y = full_estimate + 0.0005,\n               label = sprintf(\"Full model: %.4f\", full_estimate),\n               hjust = 0, color = \"red\", size = 3.5) +\n      labs(\n        title = \"RIR Coefficient Stability Across LOO Folds\",\n        subtitle = sprintf(\"CV = %.1f%% | Range: %.4f - %.4f\",\n                           stability$cv_percent, stability$range[1], stability$range[2]),\n        x = \"Participant Excluded\",\n        y = \"RIR Coefficient (m/s per RIR)\"\n      ) +\n      theme_minimal() +\n      theme(axis.text.x = element_text(angle = 45, hjust = 1))\n  }\n}\n\n\n\n\n\n\n\n\nFigure 21: Distribution of RIR coefficient estimates across leave-one-out folds. The red dashed line shows the full-sample estimate. Low variability (CV &lt; 10%) indicates a robust finding.\n\n\n\n\n\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$advanced_validation)) {\n  av &lt;- lmm_results$advanced_validation\n\n  if (!is.null(av$coefficient_stability)) {\n    stability &lt;- av$coefficient_stability\n\n    stability_df &lt;- data.frame(\n      Metric = c(\n        \"Full Model Estimate\",\n        \"Mean Across Folds\",\n        \"Coefficient of Variation\",\n        \"Range (Min - Max)\",\n        \"Stability Assessment\"\n      ),\n      Value = c(\n        sprintf(\"%.4f m/s per RIR\", stability$full_model_estimate),\n        sprintf(\"%.4f m/s per RIR\", stability$mean_estimate),\n        sprintf(\"%.1f%%\", stability$cv_percent),\n        sprintf(\"%.4f - %.4f\", stability$range[1], stability$range[2]),\n        if (stability$is_stable) \"Stable (CV &lt; 10%)\" else \"Variable (CV &gt;= 10%)\"\n      )\n    )\n\n    format_table(stability_df, col.names = c(\"Metric\", \"Value\"))\n  }\n}\n\n\n\n\nTable 16: Coefficient stability metrics for the RIR effect\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\nFull Model Estimate\n0.0292 m/s per RIR\n\n\nMean Across Folds\n0.0292 m/s per RIR\n\n\nCoefficient of Variation\n3.0%\n\n\nRange (Min - Max)\n0.0268 - 0.0306\n\n\nStability Assessment\nStable (CV &lt; 10%)\n\n\n\n\n\n\n\n\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$advanced_validation)) {\n  av &lt;- lmm_results$advanced_validation\n\n  if (!is.null(av$coefficient_stability)) {\n    influential &lt;- av$coefficient_stability$influential_participants\n    influential$estimate_when_excluded &lt;- round(influential$estimate_when_excluded, 4)\n    influential$deviation &lt;- round(influential$deviation, 4)\n\n    format_table(\n      influential,\n      col.names = c(\"Participant\", \"Estimate When Excluded\", \"Deviation from Full Model\")\n    )\n  }\n}\n\n\n\n\nTable 17: Most influential participants on the RIR coefficient\n\n\n\n\n\n\nParticipant\nEstimate When Excluded\nDeviation from Full Model\n\n\n\n\nP05\n0.027\n0.002\n\n\nP19\n0.028\n0.002\n\n\nP12\n0.031\n0.001\n\n\nP09\n0.028\n0.001\n\n\nP04\n0.030\n0.001\n\n\n\n\n\n\n\n\nInterpretation: A coefficient of variation &lt; 10% indicates that the RIR effect is robust—no single participant drives the conclusion. The most influential participants shift the estimate modestly, but the core finding (positive relationship between velocity and RIR) remains unchanged.\n\n\n5.7 Model Selection Stability\nDoes the best model change when we exclude participants? We compare model selection (via BIC) across all LOO folds.\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$advanced_validation)) {\n  av &lt;- lmm_results$advanced_validation\n\n  if (!is.null(av$model_selection_stability)) {\n    ms &lt;- av$model_selection_stability\n\n    # Create vote counts table\n    vote_df &lt;- data.frame(\n      Model = names(ms$vote_counts),\n      Votes = as.integer(unlist(ms$vote_counts)),\n      Percentage = sprintf(\"%.0f%%\", 100 * as.integer(unlist(ms$vote_counts)) / 19)\n    )\n\n    format_table(vote_df, col.names = c(\"Model\", \"Folds Won\", \"Percentage\"))\n  }\n}\n\n\n\n\nTable 18: Model selection stability across leave-one-out folds\n\n\n\n\n\n\nModel\nFolds Won\nPercentage\n\n\n\n\nrandom_slope\n19\n100%\n\n\n\n\n\n\n\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$advanced_validation)) {\n  av &lt;- lmm_results$advanced_validation\n\n  if (!is.null(av$model_selection_stability)) {\n    ms &lt;- av$model_selection_stability\n\n    cat(\"**Summary:**\\n\\n\")\n    cat(sprintf(\"- Full data winner: **%s**\\n\", ms$full_data_winner))\n    cat(sprintf(\"- Consensus model: **%s**\\n\", ms$consensus_model))\n    cat(sprintf(\"- Stability: **%.0f%%** of folds agree with full data\\n\", ms$stability_percent))\n    cat(sprintf(\"- Selection criterion: %s\\n\", ms$criterion))\n\n    if (ms$stability_percent &gt;= 90) {\n      cat(\"\\n✓ **Excellent stability**: The same model wins almost every fold, confirming our model choice.\\n\")\n    } else if (ms$stability_percent &gt;= 70) {\n      cat(\"\\n⚠ **Good stability**: Most folds agree, though some variability exists.\\n\")\n    } else {\n      cat(\"\\n⚠ **Moderate stability**: Model selection shows sensitivity to data composition.\\n\")\n    }\n  }\n}\n\nSummary:\n\nFull data winner: random_slope\nConsensus model: random_slope\nStability: 100% of folds agree with full data\nSelection criterion: BIC\n\n✓ Excellent stability: The same model wins almost every fold, confirming our model choice.\n\n\n5.8 Anomaly Detection with Isolation Forest\nWhy Isolation Forest over Cook’s Distance?\nIsolation Forest is a multivariate anomaly detection algorithm that identifies unusual observations based on their isolation in feature space. We chose this approach over Cook’s distance for several reasons:\n\nMultivariate sensitivity: Cook’s distance measures influence on regression coefficients, but an observation can be unusual in ways that don’t strongly affect the linear model (e.g., unusual velocity-RIR combinations that still fall near the regression line)\nFeature-rich detection: By using 23+ engineered features (z-scores, ratios, residuals, interactions), we capture complex patterns that univariate or bivariate methods miss\nNo distributional assumptions: Isolation Forest doesn’t assume normality, making it robust for velocity data which may have non-normal tails\nInterpretable output: The permutation-based feature contributions provide SHAP-like explanations for why each observation was flagged\n\n\n5.8.1 Observation-Level Anomaly Detection\nWe train a dedicated Isolation Forest model using ALL engineered features (~23 features) rather than just raw metrics. This captures complex patterns at the rep-level that raw velocity, RIR, and load alone would miss:\nFeature Categories:\n\nZ-score features: Global, within-participant, and within-set standardized velocities\nPosition features: Rep position in set, first/last rep flags, velocity decay from set start\nRatio features: Velocity ratios to set max, first rep, global mean\nResidual features: Velocity residual from RIR prediction, standardized residuals\nVolume features: Rep number in session, cumulative volume\nInteraction features: RIR×Load, Position×RIR, Volume×Velocity\nCross-level features: Z-scores vs population at same load or RIR level\n\nThreshold Selection Strategy:\nRather than using an arbitrary contamination rate (e.g., 5%), we select the threshold based on visual inspection of the score distribution. The goal is to identify observations that naturally “cluster away” from the main distribution—where the anomaly score curve shows a clear separation or inflection point. This data-driven approach ensures we flag only truly unusual observations rather than forcing a predetermined percentage.\nWe use Leave-One-Out Cross-Validation (LOO-CV) to obtain robust anomaly scores that aren’t biased by the observation being scored.\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$data)) {\n  data &lt;- lmm_results$data\n\n  # Use FeatureEngineer to create comprehensive features\n  fe &lt;- anomaly_detector$get_feature_engineer()$new()\n  feature_result &lt;- fe$engineer_features(data)\n\n  # Get ALL observation-level features (not just the 10-feature subset)\n  obs_features &lt;- feature_result$observation_features\n  all_obs_feature_names &lt;- feature_result$feature_names$observation  # ALL features\n\n  # Use all engineered features that exist in the data\n  available_features &lt;- intersect(all_obs_feature_names, names(obs_features))\n  # Filter to only numeric features (exclude boolean flags if problematic)\n  numeric_mask &lt;- sapply(obs_features[, available_features, drop = FALSE], is.numeric)\n  available_features &lt;- available_features[numeric_mask]\n\n  cat(sprintf(\"**Observation-Level Feature Engineering:**\\n\\n\"))\n  cat(sprintf(\"- Total engineered features: **%d**\\n\", length(available_features)))\n  cat(sprintf(\"- Feature categories:\\n\"))\n  cat(sprintf(\"  - Z-scores: velocity_z_global, velocity_z_within_participant, velocity_z_within_set, rir_z_global\\n\"))\n  cat(sprintf(\"  - Position: set_position_normalized, velocity_decay_from_start, velocity_rank_in_set\\n\"))\n  cat(sprintf(\"  - Ratios: velocity_ratio_to_global, velocity_ratio_to_set_max, velocity_ratio_to_set_first, velocity_ratio_to_expected\\n\"))\n  cat(sprintf(\"  - Residuals: velocity_residual_from_rir, standardized_residual_rir\\n\"))\n  cat(sprintf(\"  - Volume: rep_number_in_session, cumulative_volume\\n\"))\n  cat(sprintf(\"  - Interactions: rir_x_load_interaction, set_position_x_rir, cumulative_volume_x_velocity\\n\"))\n  cat(sprintf(\"  - Cross-level: pl_z_vs_population_at_load, pr_z_vs_population_at_rir, etc.\\n\\n\"))\n\n  # Cross-validated detection with ALL engineered features\n  # Use initial contamination to get scores, then refine threshold visually\n  cv_result &lt;- anomaly_detector$detect_anomalies_cv(\n    obs_features, available_features, contamination = 0.05\n  )\n\n  # Refine threshold based on visual gap detection\n  # Looking at the score distribution, observations above 0.60 clearly separate from the main cluster\n  sorted_scores &lt;- sort(cv_result$scores, decreasing = TRUE)\n\n  # Find natural gap: where consecutive score differences are largest\n  score_diffs &lt;- diff(sorted_scores)\n  # Look for gap in top 10% of scores\n  n_check &lt;- max(10, ceiling(0.10 * length(sorted_scores)))\n  top_diffs &lt;- score_diffs[1:n_check]\n  gap_idx &lt;- which.max(abs(top_diffs))\n\n  # The threshold is the score just below the gap\n  visual_threshold &lt;- sorted_scores[gap_idx + 1]\n\n  # Apply the visually-selected threshold\n  cv_result$threshold &lt;- visual_threshold\n  cv_result$is_anomaly &lt;- cv_result$scores &gt;= visual_threshold\n  cv_result$anomaly_indices &lt;- which(cv_result$is_anomaly)\n  cv_result$n_anomalies &lt;- sum(cv_result$is_anomaly)\n\n  # Store for later use\n  cv_anomaly_result &lt;- cv_result\n  cv_obs_features &lt;- obs_features\n  cv_available_features &lt;- available_features\n  cv_feature_result &lt;- feature_result\n\n  cat(sprintf(\"**Observation-Level Detection Results:**\\n\\n\"))\n  cat(sprintf(\"- Observations analyzed: %d\\n\", length(cv_result$scores)))\n  cat(sprintf(\"- Anomalies flagged: %d (%.1f%%)\\n\",\n              cv_result$n_anomalies,\n              100 * cv_result$n_anomalies / length(cv_result$scores)))\n  cat(sprintf(\"- Threshold (visual gap detection): %.3f\\n\", cv_result$threshold))\n  cat(sprintf(\"\\n**Threshold Rationale:** The threshold was selected by identifying where anomaly scores show a natural separation—observations above this threshold cluster distinctly away from the main distribution, indicating genuinely unusual patterns rather than arbitrary percentile cutoffs.\\n\"))\n}\n\n\n**Observation-Level Feature Engineering:**\n\n- Total engineered features: **23**\n- Feature categories:\n  - Z-scores: velocity_z_global, velocity_z_within_participant, velocity_z_within_set, rir_z_global\n  - Position: set_position_normalized, velocity_decay_from_start, velocity_rank_in_set\n  - Ratios: velocity_ratio_to_global, velocity_ratio_to_set_max, velocity_ratio_to_set_first, velocity_ratio_to_expected\n  - Residuals: velocity_residual_from_rir, standardized_residual_rir\n  - Volume: rep_number_in_session, cumulative_volume\n  - Interactions: rir_x_load_interaction, set_position_x_rir, cumulative_volume_x_velocity\n  - Cross-level: pl_z_vs_population_at_load, pr_z_vs_population_at_rir, etc.\n\n**Observation-Level Detection Results:**\n\n- Observations analyzed: 406\n- Anomalies flagged: 5 (1.2%)\n- Threshold (visual gap detection): 0.606\n\n**Threshold Rationale:** The threshold was selected by identifying where anomaly scores show a natural separation—observations above this threshold cluster distinctly away from the main distribution, indicating genuinely unusual patterns rather than arbitrary percentile cutoffs.\n\n\n\n\nShow code\nif (exists(\"cv_anomaly_result\")) {\n  anomaly_detector$plot_score_distribution_with_threshold(\n    cv_anomaly_result,\n    show_quantiles = TRUE,\n    title = sprintf(\"Observation-Level Anomaly Scores (%d Engineered Features)\",\n                    length(cv_available_features))\n  )\n}\n\n\n\n\n\n\n\n\nFigure 22: Distribution of cross-validated anomaly scores for observation-level model using all engineered features. The threshold is set at the natural gap where high-scoring observations separate from the main cluster.\n\n\n\n\n\n\n\n5.8.2 Anomalous Observation Details\nFor each flagged observation, we provide: 1. Original values: The raw measurements (velocity, RIR, load) for interpretation 2. SHAP-like contributions: Which engineered features drove the anomaly score 3. Narrative explanation: A human-readable interpretation of why the observation is unusual\n\n\nShow code\nif (exists(\"cv_anomaly_result\") && cv_anomaly_result$n_anomalies &gt; 0) {\n  data &lt;- lmm_results$data\n\n  # Get anomaly indices\n  anomaly_idx &lt;- cv_anomaly_result$anomaly_indices\n\n  # Build table with original values\n  anomaly_data &lt;- data.frame(\n    Row = anomaly_idx,\n    Participant = data$id[anomaly_idx],\n    Velocity = sprintf(\"%.3f\", data$mean_velocity[anomaly_idx]),\n    RIR = data$rir[anomaly_idx],\n    Load = data$load_percentage[anomaly_idx],\n    Day = data$day[anomaly_idx],\n    Set = sapply(strsplit(as.character(data$set_id[anomaly_idx]), \"_\"), function(x) tail(x, 1)),\n    Score = sprintf(\"%.3f\", cv_anomaly_result$scores[anomaly_idx])\n  )\n\n  # Order by score (most anomalous first)\n  anomaly_data &lt;- anomaly_data[order(cv_anomaly_result$scores[anomaly_idx], decreasing = TRUE), ]\n\n  format_table(head(anomaly_data, 15), col.names = c(\"Row\", \"ID\", \"Velocity (m/s)\", \"RIR\", \"Load\", \"Day\", \"Set\", \"CV Score\"))\n} else {\n  cat(\"No anomalies detected with the CV method.\")\n}\n\n\n\n\nTable 19: Original feature values for anomalous observations. These raw measurements help interpret why each observation was flagged as unusual.\n\n\n\n\n\n\n\nRow\nID\nVelocity (m/s)\nRIR\nLoad\nDay\nSet\nCV Score\n\n\n\n\n4\n122\nP09\n0.670\n7\n80%\nDay 2\nS2\n0.646\n\n\n1\n37\nP05\n0.653\n7\n80%\nDay 2\nS2\n0.640\n\n\n2\n61\nP02\n0.578\n5\n90%\nDay 1\nS1\n0.631\n\n\n5\n123\nP09\n0.675\n6\n80%\nDay 2\nS2\n0.622\n\n\n3\n111\nP09\n0.200\n0\n90%\nDay 1\nS1\n0.606\n\n\n\n\n\n\n\n\n\n\n5.8.3 SHAP-like Feature Contributions\nFor each anomalous observation, we compute permutation-based feature contributions. This SHAP-like approach measures how much each engineered feature contributes to the anomaly score by replacing feature values with population medians.\nPositive contributions indicate features that increase the anomaly score (make the observation more unusual). Negative contributions indicate features that decrease the score (make it less unusual).\n\n\nShow code\nif (exists(\"cv_anomaly_result\") && cv_anomaly_result$n_anomalies &gt; 0) {\n  # Ensure id column is present in features\n  if (!\"id\" %in% names(cv_obs_features)) {\n    cv_obs_features$id &lt;- lmm_results$data$id\n  }\n\n  # Compute SHAP-like explanations using engineered features\n  obs_shap &lt;- anomaly_detector$explain_anomalies_shap(\n    cv_anomaly_result,\n    cv_obs_features,\n    cv_available_features,\n    participant_col = \"id\",\n    max_anomalies = 20\n  )\n\n  # Store for later use\n  cv_shap_result &lt;- obs_shap\n\n  if (obs_shap$n_anomalies &gt; 0) {\n    # Generate digestible summary table\n    summary_df &lt;- anomaly_detector$summarize_shap_contributions(obs_shap, top_k = 3)\n\n    format_table(\n      head(summary_df, 15),\n      col.names = c(\"Participant\", \"Obs #\", \"Score\", \"Rank\",\n                   \"Top Feature\", \"2nd Feature\", \"3rd Feature\")\n    )\n  }\n} else {\n  cat(\"No anomalies detected with CV method.\")\n}\n\n\n\n\nTable 20: Digestible summary of anomaly explanations showing top 3 contributing features per observation. Arrows indicate direction (↑ increases anomaly score, ↓ decreases it).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParticipant\nObs #\nScore\nRank\nTop Feature\n2nd Feature\n3rd Feature\n\n\n\n\nc(“P05”, “P02”, “P09”, “P09”, “P09”)\n37\n0.640\n1\npr_ratio_to_global_at_rir ↑0.038\nvelocity_deviation_from_participant ↑0.026\nvelocity_z_global ↑0.021\n\n\n\n61\n0.631\n2\nvelocity_residual_from_rir ↑0.028\nvelocity_ratio_to_expected ↑0.026\nstandardized_residual_rir ↑0.024\n\n\n\n111\n0.606\n3\npr_z_vs_population_at_rir ↑0.015\npl_z_vs_population_at_load ↑0.013\npr_ratio_to_global_at_rir ↑0.012\n\n\n\n122\n0.646\n4\npr_ratio_to_global_at_rir ↑0.037\nvelocity_deviation_from_participant ↑0.021\nvelocity_z_global ↑0.019\n\n\n\n123\n0.622\n5\npr_ratio_to_global_at_rir ↑0.036\nvelocity_z_global ↑0.020\npr_z_vs_population_at_rir ↑0.019\n\n\n\n\n\n\n\n\n\nShow code\nif (exists(\"cv_anomaly_result\") && cv_anomaly_result$n_anomalies &gt; 0 && exists(\"cv_shap_result\")) {\n  data &lt;- lmm_results$data\n  anomaly_idx &lt;- cv_anomaly_result$anomaly_indices\n  scores &lt;- cv_anomaly_result$scores[anomaly_idx]\n\n  cat(\"**Narrative Interpretation of Top Anomalies:**\\n\\n\")\n  cat(\"Below we interpret each flagged observation, explaining *why* it's unusual based on the engineered features and original measurements.\\n\\n\")\n\n  # Get top 5 anomalies for interpretation\n  top_idx &lt;- head(order(scores, decreasing = TRUE), 5)\n\n  for (i in seq_along(top_idx)) {\n    rank &lt;- i\n    idx &lt;- anomaly_idx[top_idx[i]]\n    obs &lt;- data[idx, ]\n    score &lt;- round(scores[top_idx[i]], 3)\n\n    # Build interpretation using original values\n    velocity &lt;- round(obs$mean_velocity, 3)\n    rir &lt;- obs$rir\n    load &lt;- obs$load_percentage\n    pid &lt;- obs$id\n    day &lt;- obs$day\n\n    # Get engineered feature values for this observation\n    velocity_z_global &lt;- round(cv_obs_features$velocity_z_global[idx], 2)\n    velocity_z_participant &lt;- round(cv_obs_features$velocity_z_within_participant[idx], 2)\n\n    # Compute population stats for context\n    velocity_mean &lt;- round(mean(data$mean_velocity, na.rm = TRUE), 3)\n    velocity_for_rir &lt;- round(mean(data$mean_velocity[data$rir == rir], na.rm = TRUE), 3)\n    velocity_for_load &lt;- round(mean(data$mean_velocity[data$load_percentage == load], na.rm = TRUE), 3)\n\n    # Determine primary anomaly type from engineered features\n    if (abs(velocity_z_global) &gt; 2) {\n      anomaly_type &lt;- if (velocity_z_global &gt; 0) \"globally fast\" else \"globally slow\"\n    } else if (abs(velocity_z_participant) &gt; 2) {\n      anomaly_type &lt;- if (velocity_z_participant &gt; 0) \"unusually fast for this participant\" else \"unusually slow for this participant\"\n    } else if (velocity &lt; velocity_for_rir * 0.85) {\n      anomaly_type &lt;- sprintf(\"slower than typical for RIR %d (expected ~%.3f m/s)\", rir, velocity_for_rir)\n    } else if (velocity &gt; velocity_for_rir * 1.15) {\n      anomaly_type &lt;- sprintf(\"faster than typical for RIR %d (expected ~%.3f m/s)\", rir, velocity_for_rir)\n    } else {\n      anomaly_type &lt;- \"atypical multivariate pattern\"\n    }\n\n    cat(sprintf(\"%d. **%s** (Row %d, CV Score = %.3f)\\n\", rank, pid, idx, score))\n    cat(sprintf(\"   - **Measurements**: Velocity = **%.3f m/s** at RIR = **%d** with **%s** load (%s)\\n\",\n                velocity, rir, load, day))\n    cat(sprintf(\"   - **Comparison**: Population mean = %.3f m/s | RIR %d mean = %.3f m/s | %s mean = %.3f m/s\\n\",\n                velocity_mean, rir, velocity_for_rir, load, velocity_for_load))\n    cat(sprintf(\"   - **Z-scores**: Global = %.2f | Within-participant = %.2f\\n\",\n                velocity_z_global, velocity_z_participant))\n    cat(sprintf(\"   - **Interpretation**: This observation is **%s**.\\n\\n\", anomaly_type))\n  }\n\n  cat(\"\\n*Note: These interpretations combine raw measurements with engineered features (z-scores, residuals, interactions) to explain the multivariate anomaly pattern.*\\n\")\n}\n\nNarrative Interpretation of Top Anomalies:\nBelow we interpret each flagged observation, explaining why it’s unusual based on the engineered features and original measurements.\n\nP09 (Row 122, CV Score = 0.646)\n\nMeasurements: Velocity = 0.670 m/s at RIR = 7 with 80% load (Day 2)\nComparison: Population mean = 0.288 m/s | RIR 7 mean = 0.371 m/s | 80% mean = 0.304 m/s\nZ-scores: Global = 3.99 | Within-participant = 1.69\nInterpretation: This observation is globally fast.\n\nP05 (Row 37, CV Score = 0.640)\n\nMeasurements: Velocity = 0.653 m/s at RIR = 7 with 80% load (Day 2)\nComparison: Population mean = 0.288 m/s | RIR 7 mean = 0.371 m/s | 80% mean = 0.304 m/s\nZ-scores: Global = 3.81 | Within-participant = 1.91\nInterpretation: This observation is globally fast.\n\nP02 (Row 61, CV Score = 0.631)\n\nMeasurements: Velocity = 0.578 m/s at RIR = 5 with 90% load (Day 1)\nComparison: Population mean = 0.288 m/s | RIR 5 mean = 0.350 m/s | 90% mean = 0.261 m/s\nZ-scores: Global = 3.02 | Within-participant = 2.70\nInterpretation: This observation is globally fast.\n\nP09 (Row 123, CV Score = 0.622)\n\nMeasurements: Velocity = 0.675 m/s at RIR = 6 with 80% load (Day 2)\nComparison: Population mean = 0.288 m/s | RIR 6 mean = 0.373 m/s | 80% mean = 0.304 m/s\nZ-scores: Global = 4.04 | Within-participant = 1.72\nInterpretation: This observation is globally fast.\n\nP09 (Row 111, CV Score = 0.606)\n\nMeasurements: Velocity = 0.200 m/s at RIR = 0 with 90% load (Day 1)\nComparison: Population mean = 0.288 m/s | RIR 0 mean = 0.206 m/s | 90% mean = 0.261 m/s\nZ-scores: Global = -0.92 | Within-participant = -1.46\nInterpretation: This observation is atypical multivariate pattern.\n\n\nNote: These interpretations combine raw measurements with engineered features (z-scores, residuals, interactions) to explain the multivariate anomaly pattern.\n\n\nShow code\nif (exists(\"cv_shap_result\") && cv_shap_result$n_anomalies &gt; 0) {\n  anomaly_detector$plot_observation_contributions(\n    cv_shap_result,\n    max_display = 10,\n    top_k = 8,  # Show top 8 features per observation\n    title = \"Feature Contributions for Anomalous Observations (Engineered Features)\"\n  )\n} else {\n  cat(\"No SHAP results available for plotting.\")\n}\n\n\n\n\n\n\n\n\nFigure 23: SHAP-like feature contributions for anomalous observations using all 10 engineered features. Each bar shows how much a feature contributed to the observation’s anomaly score. Red bars indicate features that increased the anomaly score, blue bars indicate features that decreased it.\n\n\n\n\n\n\n\nShow code\nif (exists(\"cv_shap_result\") && cv_shap_result$n_anomalies &gt; 0) {\n  anomaly_detector$plot_observation_aggregate_importance(\n    cv_shap_result,\n    top_n = 10,\n    title = \"Observation-Level Engineered Feature Importance\"\n  )\n} else {\n  cat(\"No SHAP results available for importance plot.\")\n}\n\n\n\n\n\n\n\n\nFigure 24: Aggregate feature importance across all anomalous observations using engineered features. Shows which features most commonly contribute to the anomaly scores.\n\n\n\n\n\nInterpretation: The SHAP-like contributions use ALL engineered features (~25 features) including:\n\nVelocity z-scores (global, within-participant, within-set): Identify observations that deviate from expected velocities\nPosition features (normalized position, decay from start, rank): Capture fatigue patterns within sets\nRatio features (to set max, to first rep, to global): Compare velocity to references\nResidual features (RIR residual, standardized residual): Identify deviations from the velocity-RIR relationship\nVolume features (rep number, cumulative volume): Track session progression\nInteraction features (RIR×load, position×RIR, volume×velocity): Capture complex multivariate patterns\nCross-level features (population comparisons at same load/RIR): Compare to similar conditions\n\nPositive contributions indicate that a feature’s value increases the anomaly score (makes the observation more unusual), while negative contributions indicate the opposite. This permutation-based approach directly measures each feature’s impact on the Isolation Forest’s anomaly scoring.\n\n\n5.8.4 Participant-Level Anomaly Detection\nIn addition to observation-level anomalies, we train a separate Isolation Forest model for participant-level anomalies using participant-aggregated features (~40 features). This model identifies participants whose overall patterns are unusual compared to the population.\nParticipant-Level Features:\n\nVelocity profile: Mean, SD, CV, range, IQR, skewness, kurtosis, percentiles\nRIR relationship: Slope, intercept, R², range sensitivity\nSet behavior: Number of sets, mean reps per set, rep variability\nLoad sensitivity: Velocity change across loads, number of loads tested\nPopulation comparisons: Z-scores, percentile ranks, ratio to population norms\n\n\n\nShow code\nif (exists(\"cv_feature_result\")) {\n  # Get participant-level features\n  part_features &lt;- cv_feature_result$participant_features\n  all_part_feature_names &lt;- cv_feature_result$feature_names$participant\n\n  # Use all participant features that exist and are numeric\n  available_part_features &lt;- intersect(all_part_feature_names, names(part_features))\n  numeric_mask &lt;- sapply(part_features[, available_part_features, drop = FALSE], is.numeric)\n  available_part_features &lt;- available_part_features[numeric_mask]\n\n  cat(sprintf(\"**Participant-Level Feature Engineering:**\\n\\n\"))\n  cat(sprintf(\"- Total engineered features: **%d**\\n\", length(available_part_features)))\n  cat(sprintf(\"- Participants: %d\\n\\n\", nrow(part_features)))\n\n  # Standard detection (not CV since we have few participants)\n  part_result &lt;- anomaly_detector$detect_raw_data_anomalies(\n    part_features, available_part_features, contamination = 0.10  # Higher threshold for small N\n  )\n\n  # Store for later\n  part_anomaly_result &lt;- part_result\n  part_available_features &lt;- available_part_features\n\n  cat(sprintf(\"**Participant-Level Detection Results:**\\n\\n\"))\n  cat(sprintf(\"- Participants analyzed: %d\\n\", length(part_result$scores)))\n  cat(sprintf(\"- Anomalous participants: %d (%.1f%%)\\n\",\n              part_result$n_anomalies,\n              100 * part_result$n_anomalies / length(part_result$scores)))\n  cat(sprintf(\"- Threshold (10%% contamination): %.3f\\n\", part_result$threshold))\n\n  if (part_result$n_anomalies &gt; 0) {\n    # Show which participants are anomalous\n    anomalous_idx &lt;- part_result$anomaly_indices\n    cat(\"\\n**Anomalous Participants:**\\n\")\n    for (i in anomalous_idx) {\n      pid &lt;- part_features$id[i]\n      score &lt;- round(part_result$scores[i], 3)\n      cat(sprintf(\"- %s (score: %.3f)\\n\", pid, score))\n    }\n  }\n}\n\n\n**Participant-Level Feature Engineering:**\n\n- Total engineered features: **41**\n- Participants: 19\n\n\n**Participant-Level Detection Results:**\n\n- Participants analyzed: 19\n- Anomalous participants: 2 (10.5%)\n- Threshold (10% contamination): 0.540\n\n**Anomalous Participants:**\n- P05 (score: 0.597)\n- P09 (score: 0.569)\n\n\n\n\nShow code\nif (exists(\"part_anomaly_result\") && exists(\"cv_feature_result\")) {\n  part_features &lt;- cv_feature_result$participant_features\n\n  # Create bar plot of participant scores\n  score_df &lt;- data.frame(\n    participant = part_features$id,\n    score = part_anomaly_result$scores,\n    is_anomaly = part_anomaly_result$is_anomaly\n  )\n  score_df &lt;- score_df[order(score_df$score, decreasing = TRUE), ]\n  score_df$participant &lt;- factor(score_df$participant, levels = score_df$participant)\n\n  ggplot2::ggplot(score_df, ggplot2::aes(x = participant, y = score, fill = is_anomaly)) +\n    ggplot2::geom_col() +\n    ggplot2::geom_hline(yintercept = part_anomaly_result$threshold, linetype = \"dashed\", color = \"red\") +\n    ggplot2::scale_fill_manual(values = c(\"FALSE\" = \"#2E86AB\", \"TRUE\" = \"#E63946\"),\n                               labels = c(\"Normal\", \"Anomalous\")) +\n    ggplot2::labs(\n      title = sprintf(\"Participant-Level Anomaly Scores (%d Features)\", length(part_available_features)),\n      subtitle = sprintf(\"Threshold = %.3f (10%% contamination)\", part_anomaly_result$threshold),\n      x = \"Participant\", y = \"Anomaly Score\", fill = \"Status\"\n    ) +\n    ggplot2::theme_minimal() +\n    ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 45, hjust = 1))\n}\n\n\n\n\n\n\n\n\nFigure 25: Participant-level anomaly scores using all participant-aggregated features. Each bar represents a participant’s anomaly score from the Isolation Forest model.\n\n\n\n\n\n\n\n5.8.5 Random Effects Anomalies\nWe also screen participants based on their random effects (individual intercepts and slopes) to identify individuals with unusual velocity-RIR patterns.\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$advanced_validation)) {\n  av &lt;- lmm_results$advanced_validation\n\n  if (!is.null(av$anomalies) && !is.null(av$anomalies$random_effects)) {\n    re &lt;- av$anomalies$random_effects\n\n    re_df &lt;- data.frame(\n      Category = c(\"Total Participants\", \"Anomalous\", \"Borderline\", \"Normal\"),\n      Count = c(re$n_participants, re$n_anomalous, re$n_borderline, re$n_normal),\n      Interpretation = c(\n        \"\",\n        \"Unusual velocity-RIR relationship\",\n        \"Some deviation from population\",\n        \"Typical relationship\"\n      )\n    )\n\n    format_table(re_df, col.names = c(\"Category\", \"Count\", \"Interpretation\"))\n  }\n}\n\n\n\n\nTable 21: Random effects anomaly detection by participant\n\n\n\n\n\n\nCategory\nCount\nInterpretation\n\n\n\n\nTotal Participants\n19\n\n\n\nAnomalous\n2\nUnusual velocity-RIR relationship\n\n\nBorderline\n15\nSome deviation from population\n\n\nNormal\n2\nTypical relationship\n\n\n\n\n\n\n\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$advanced_validation)) {\n  av &lt;- lmm_results$advanced_validation\n\n  if (!is.null(av$anomalies) && !is.null(av$anomalies$random_effects)) {\n    re &lt;- av$anomalies$random_effects\n\n    if (length(re$anomalous_ids) &gt; 0) {\n      cat(\"**Anomalous Participants:** \", paste(re$anomalous_ids, collapse = \", \"), \"\\n\\n\")\n      cat(\"These participants have velocity-RIR relationships that differ substantially from the population pattern. This is not necessarily problematic---it confirms the need for individual calibration.\\n\")\n    } else {\n      cat(\"No participants were classified as anomalous based on their random effects.\\n\")\n    }\n  }\n}\n\nAnomalous Participants: P05, P09\nThese participants have velocity-RIR relationships that differ substantially from the population pattern. This is not necessarily problematic—it confirms the need for individual calibration.\n\n\n\n5.9 Why Isolation Forest Over Cook’s Distance\nWe choose Isolation Forest as our primary anomaly detection method for several reasons:\nAdvantages of Isolation Forest:\n\nNo distributional assumptions: Unlike Cook’s distance (which assumes normality and linear relationships), Isolation Forest works with any data distribution\nMultivariate detection: Identifies unusual combinations of features, not just univariate outliers\nFeature explainability: With SHAP-like permutation analysis, we can explain why each observation is flagged\nData-driven thresholds: Elbow detection finds natural breaks in anomaly scores rather than arbitrary cutoffs\nScales to high dimensions: Works well with our 50+ engineered features\n\nLimitations of Cook’s Distance:\n\nMeasures influence on regression coefficients, not inherent data quality\nTends to over-flag observations with high leverage (extreme X values)\nThe 4/n threshold is arbitrary and often flags too many observations\nCannot explain why an observation is flagged\n\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$advanced_validation)) {\n  av &lt;- lmm_results$advanced_validation\n\n  if (!is.null(av$if_vs_cooks_comparison)) {\n    comp &lt;- av$if_vs_cooks_comparison\n\n    comparison_df &lt;- data.frame(\n      Metric = c(\n        \"Observations flagged by Isolation Forest\",\n        \"Observations flagged by Cook's Distance\",\n        \"Agreement rate\"\n      ),\n      Value = c(\n        comp$both_flagged + comp$only_isolation_forest,\n        comp$both_flagged + comp$only_cooks_distance,\n        sprintf(\"%.1f%%\", comp$agreement_rate * 100)\n      )\n    )\n\n    format_table(comparison_df, col.names = c(\"Metric\", \"Value\"))\n  }\n}\n\n\n\n\nTable 22: Comparison between Isolation Forest and Cook’s Distance flagging (for reference)\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\nObservations flagged by Isolation Forest\n22\n\n\nObservations flagged by Cook’s Distance\n228\n\n\nAgreement rate\n48.8%\n\n\n\n\n\n\n\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$advanced_validation)) {\n  av &lt;- lmm_results$advanced_validation\n\n  if (!is.null(av$if_vs_cooks_comparison)) {\n    comp &lt;- av$if_vs_cooks_comparison\n\n    if (comp$only_cooks_distance &gt; comp$only_isolation_forest * 3) {\n      cat(\"**Note:** Cook's distance flags significantly more observations than Isolation Forest. \",\n          \"This is expected because Cook's distance is sensitive to leverage (extreme RIR or load values) \",\n          \"rather than truly anomalous patterns. We focus on Isolation Forest results for data quality assessment.\\n\")\n    }\n  }\n}\n\nNote: Cook’s distance flags significantly more observations than Isolation Forest. This is expected because Cook’s distance is sensitive to leverage (extreme RIR or load values) rather than truly anomalous patterns. We focus on Isolation Forest results for data quality assessment.\n\n5.9.1 Cook’s Distance for Final LMER Model\nWhile Isolation Forest is better for general data quality screening, Cook’s distance remains valuable for assessing influence on our final model. Here we apply Cook’s distance specifically to the best LMER model specification to identify observations that disproportionately affect the model coefficients.\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$best_model) &&\n    !is.null(lmm_results$best_model$model)) {\n\n  model &lt;- lmm_results$best_model$model\n  data &lt;- lmm_results$data\n\n  # Compute Cook's distance using influence.ME package\n  if (requireNamespace(\"influence.ME\", quietly = TRUE)) {\n    # Get influence measures (can be slow for large models)\n    cooks_d &lt;- tryCatch({\n      influence.ME::cooks.distance.estex(\n        influence.ME::influence(model, obs = TRUE)\n      )\n    }, error = function(e) NULL)\n\n    if (!is.null(cooks_d)) {\n      n &lt;- length(cooks_d)\n      threshold &lt;- 4 / n\n      influential &lt;- cooks_d &gt; threshold\n\n      # Summary table\n      cooks_df &lt;- data.frame(\n        Metric = c(\n          \"Total Observations\",\n          \"Threshold (4/n)\",\n          \"Influential Observations\",\n          \"Percentage Influential\",\n          \"Max Cook's D\",\n          \"Mean Cook's D\"\n        ),\n        Value = c(\n          n,\n          sprintf(\"%.4f\", threshold),\n          sum(influential),\n          sprintf(\"%.1f%%\", 100 * sum(influential) / n),\n          sprintf(\"%.4f\", max(cooks_d)),\n          sprintf(\"%.4f\", mean(cooks_d))\n        )\n      )\n\n      format_table(cooks_df, col.names = c(\"Metric\", \"Value\"))\n    }\n  } else {\n    cat(\"Note: influence.ME package not available for Cook's distance computation.\\n\")\n  }\n}\n\n\n\n\nTable 23: Cook’s distance diagnostics for the final LMER model. Observations with Cook’s D &gt; 4/n are flagged as influential.\n\n\n\nNote: influence.ME package not available for Cook's distance computation.\n\n\n\n\n\n\n\nTable 24: Details of observations with high Cook’s distance (influence on model coefficients)\n\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$best_model) &&\n    !is.null(lmm_results$best_model$model)) {\n\n  model &lt;- lmm_results$best_model$model\n  data &lt;- lmm_results$data\n\n  if (requireNamespace(\"influence.ME\", quietly = TRUE)) {\n    cooks_d &lt;- tryCatch({\n      influence.ME::cooks.distance.estex(\n        influence.ME::influence(model, obs = TRUE)\n      )\n    }, error = function(e) NULL)\n\n    if (!is.null(cooks_d)) {\n      n &lt;- length(cooks_d)\n      threshold &lt;- 4 / n\n      influential_idx &lt;- which(cooks_d &gt; threshold)\n\n      if (length(influential_idx) &gt; 0) {\n        # Get top 15 most influential\n        top_idx &lt;- influential_idx[order(cooks_d[influential_idx], decreasing = TRUE)]\n        top_idx &lt;- head(top_idx, 15)\n\n        influential_obs &lt;- data[top_idx, c(\"id\", \"mean_velocity\", \"rir\", \"load_percentage\", \"day\")]\n        influential_obs$cooks_d &lt;- round(cooks_d[top_idx], 4)\n        influential_obs$velocity &lt;- round(influential_obs$mean_velocity, 3)\n        influential_obs &lt;- influential_obs[, c(\"id\", \"velocity\", \"rir\", \"load_percentage\", \"day\", \"cooks_d\")]\n\n        format_table(\n          influential_obs,\n          col.names = c(\"Participant\", \"Velocity (m/s)\", \"RIR\", \"Load\", \"Day\", \"Cook's D\")\n        )\n      } else {\n        cat(\"No observations exceed the Cook's distance threshold.\\n\")\n      }\n    }\n  }\n}\n\n\n\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$best_model) &&\n    !is.null(lmm_results$best_model$model)) {\n\n  model &lt;- lmm_results$best_model$model\n  data &lt;- lmm_results$data\n\n  if (requireNamespace(\"influence.ME\", quietly = TRUE)) {\n    cooks_d &lt;- tryCatch({\n      influence.ME::cooks.distance.estex(\n        influence.ME::influence(model, obs = TRUE)\n      )\n    }, error = function(e) NULL)\n\n    if (!is.null(cooks_d)) {\n      n &lt;- length(cooks_d)\n      threshold &lt;- 4 / n\n      influential_idx &lt;- which(cooks_d &gt; threshold)\n\n      cat(\"**Interpretation of Cook's Distance Results:**\\n\\n\")\n\n      if (length(influential_idx) == 0) {\n        cat(\"No observations have disproportionate influence on the model coefficients. \",\n            \"This suggests the model estimates are robust and not driven by individual observations.\\n\")\n      } else {\n        pct &lt;- round(100 * length(influential_idx) / n, 1)\n        cat(sprintf(\"**%d observations (%.1f%%)** have Cook's distance &gt; 4/n, indicating they influence the model coefficients. \",\n                    length(influential_idx), pct))\n\n        # Characterize influential observations\n        infl_data &lt;- data[influential_idx, ]\n        mean_rir &lt;- mean(infl_data$rir)\n        mean_vel &lt;- mean(infl_data$mean_velocity)\n        global_mean_vel &lt;- mean(data$mean_velocity)\n\n        if (mean_vel &lt; global_mean_vel * 0.9) {\n          cat(\"These observations tend to have **lower velocities** than average, \")\n        } else if (mean_vel &gt; global_mean_vel * 1.1) {\n          cat(\"These observations tend to have **higher velocities** than average, \")\n        }\n\n        if (mean_rir &lt; 2) {\n          cat(\"and are concentrated near **failure (RIR 0-1)**. \")\n        } else if (mean_rir &gt; 4) {\n          cat(\"and occur at **high RIR values**. \")\n        }\n\n        cat(\"\\n\\n\")\n        cat(\"**Recommendation:** These observations warrant review but should not be automatically removed. \",\n            \"High influence may reflect genuine individual variation rather than data quality issues. \",\n            \"Compare with Isolation Forest flags---observations flagged by both methods may be true anomalies.\\n\")\n      }\n    }\n  }\n}\n\n\n\n\n5.10 Enhanced Multi-Level Anomaly Detection\nThe previous section used a fixed 95th percentile threshold. Here we compare multiple data-driven threshold selection strategies and extend anomaly detection to the participant level using aggregate behavior patterns.\n\n5.10.1 Threshold Strategy Comparison\nDifferent threshold selection strategies may identify different thresholds depending on the score distribution characteristics.\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$advanced_validation)) {\n  av &lt;- lmm_results$advanced_validation\n\n  if (!is.null(av$anomalies) && !is.null(av$anomalies$threshold_comparison)) {\n    tc &lt;- av$anomalies$threshold_comparison\n\n    tc_display &lt;- data.frame(\n      Strategy = tc$strategy,\n      Threshold = sprintf(\"%.4f\", tc$threshold),\n      `N Flagged` = tc$n_flagged,\n      `Pct Flagged` = sprintf(\"%.1f%%\", (tc$n_flagged / max(tc$n_total, 1)) * 100),\n      check.names = FALSE\n    )\n\n    format_table(tc_display)\n  }\n}\n\n\n\n\nTable 25: Comparison of threshold selection strategies for Isolation Forest scores\n\n\n\n\n\n\nfixed_percentile\n0.6026\n22\n2200.0%\n\n\nelbow_detection\n0.5143\n83\n8300.0%\n\n\ngap_statistics\n0.6568\n8\n800.0%\n\n\nknee_point\n0.7197\n1\n100.0%\n\n\nmad_based\n0.5972\n23\n2300.0%\n\n\n\n\n\n\n\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$advanced_validation)) {\n  av &lt;- lmm_results$advanced_validation\n\n  if (!is.null(av$anomalies) && !is.null(av$anomalies$threshold_comparison)) {\n    tc &lt;- av$anomalies$threshold_comparison\n\n    # Find the elbow and fixed strategies\n    elbow_row &lt;- tc[tc$strategy == \"elbow_detection\", ]\n    fixed_row &lt;- tc[tc$strategy == \"fixed_percentile\", ]\n\n    if (nrow(elbow_row) &gt; 0 && nrow(fixed_row) &gt; 0) {\n      cat(sprintf(\n        \"**Interpretation:** The elbow detection method identifies a threshold of %.4f (flagging %d observations), compared to the fixed 95th percentile threshold of %.4f (flagging %d observations). \",\n        elbow_row$threshold, elbow_row$n_flagged,\n        fixed_row$threshold, fixed_row$n_flagged\n      ))\n\n      if (elbow_row$threshold &gt; fixed_row$threshold) {\n        cat(\"The higher elbow threshold suggests there is a clear separation between the normal score cluster and anomalous observations, supporting a more conservative flagging approach.\\n\")\n      } else {\n        cat(\"The thresholds are similar, indicating the 95th percentile is appropriate for this dataset.\\n\")\n      }\n    }\n  }\n}\n\nInterpretation: The elbow detection method identifies a threshold of 0.5143 (flagging 83 observations), compared to the fixed 95th percentile threshold of 0.6026 (flagging 22 observations). The thresholds are similar, indicating the 95th percentile is appropriate for this dataset.\n\n\n5.10.2 Elbow Detection Results\nUsing the elbow detection method, which identifies the natural cluster boundary in the score distribution:\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$advanced_validation)) {\n  av &lt;- lmm_results$advanced_validation\n\n  if (!is.null(av$anomalies) && !is.null(av$anomalies$raw_data_v2)) {\n    v2 &lt;- av$anomalies$raw_data_v2\n\n    elbow_df &lt;- data.frame(\n      Metric = c(\n        \"Total Observations\",\n        \"Anomalies Detected\",\n        \"Anomaly Rate\",\n        \"Elbow Threshold\"\n      ),\n      Value = c(\n        v2$n_total,\n        v2$n_anomalies,\n        sprintf(\"%.1f%%\", v2$anomaly_rate * 100),\n        sprintf(\"%.4f\", v2$threshold)\n      )\n    )\n\n    format_table(elbow_df, col.names = c(\"Metric\", \"Value\"))\n  }\n}\n\n\n\n\nTable 26: Anomaly detection using elbow-based threshold selection\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\nTotal Observations\n406\n\n\nAnomalies Detected\n83\n\n\nAnomaly Rate\n20.4%\n\n\nElbow Threshold\n0.5143\n\n\n\n\n\n\n\n\n\n\n5.10.3 Participant Behavior Profiling\nBeyond individual observations, we profile participants based on their aggregate behavior patterns to identify those with unusual overall performance characteristics.\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$advanced_validation)) {\n  av &lt;- lmm_results$advanced_validation\n\n  if (!is.null(av$anomalies) && !is.null(av$anomalies$participant_profile)) {\n    pp &lt;- av$anomalies$participant_profile\n\n    profile_df &lt;- data.frame(\n      Metric = c(\n        \"Total Participants\",\n        \"Anomalous Participants\",\n        \"Anomaly Rate\",\n        \"Threshold\",\n        \"Detection Method\"\n      ),\n      Value = c(\n        pp$n_participants,\n        pp$n_anomalous,\n        sprintf(\"%.1f%%\", (pp$n_anomalous / pp$n_participants) * 100),\n        sprintf(\"%.4f\", pp$threshold),\n        pp$method\n      )\n    )\n\n    format_table(profile_df, col.names = c(\"Metric\", \"Value\"))\n  }\n}\n\n\n\n\nTable 27: Participant-level anomaly detection based on aggregate behavior patterns\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\nTotal Participants\n19\n\n\nAnomalous Participants\n4\n\n\nAnomaly Rate\n21.1%\n\n\nThreshold\n0.5113\n\n\nDetection Method\nisolation_forest\n\n\n\n\n\n\n\n\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$advanced_validation_full)) {\n  avf &lt;- lmm_results$advanced_validation_full\n\n  if (!is.null(avf$participant_profile)) {\n    # Import ParticipantProfiler for plotting (use .. since QMD is in analyses/)\n    box::use(../R/calculators/participant_profiler[ParticipantProfiler])\n    profiler &lt;- ParticipantProfiler$new()\n    profiler$plot_profiles(\n      avf$participant_profile,\n      title = \"Participant Anomaly Profiles\"\n    )\n  }\n}\n\n\n\n\n\n\n\n\nFigure 26: Participant anomaly scores with threshold visualization. Each point represents a participant, ordered by their aggregate anomaly score. Red points indicate participants flagged as having unusual behavior patterns.\n\n\n\n\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$advanced_validation_full)) {\n  avf &lt;- lmm_results$advanced_validation_full\n\n  if (!is.null(avf$participant_profile)) {\n    pp &lt;- avf$participant_profile\n    anomalous_ids &lt;- pp$get_anomalous_ids()\n\n    if (length(anomalous_ids) &gt; 0) {\n      cat(\"**Anomalous Participants and SHAP-like Feature Contributions:**\\n\\n\")\n      cat(\"Each anomalous participant is explained by the features that most strongly contributed to their anomaly score. Positive contributions indicate features that *increased* the anomaly score (made the participant appear more unusual).\\n\\n\")\n\n      for (pid in anomalous_ids) {\n        explanation &lt;- pp$get_explanation(pid)\n        cat(sprintf(\"- %s\\n\", explanation))\n      }\n\n      cat(\"\\n**Interpretation:** These participants show unusual combinations of aggregate features (velocity consistency, load sensitivity, RIR patterns) compared to the population. The feature contributions above explain *why* each participant was flagged---the specific features that deviated most from population norms.\\n\")\n    } else {\n      cat(\"No participants were flagged as anomalous based on their aggregate behavior patterns. All participants show typical velocity-load-RIR relationships.\\n\")\n    }\n  }\n}\n\nAnomalous Participants and SHAP-like Feature Contributions:\nEach anomalous participant is explained by the features that most strongly contributed to their anomaly score. Positive contributions indicate features that increased the anomaly score (made the participant appear more unusual).\n\nP05 flagged: p_load_sensitivity increases score by 0.057; p_velocity_cv increases score by 0.032; p_rir_slope increases score by 0.027\nP09 flagged: p_mean_velocity increases score by 0.049; p_velocity_range increases score by 0.033; p_velocity_cv increases score by 0.012\nP12 flagged: p_rir_r2 increases score by 0.046; p_velocity_cv increases score by 0.040; p_load_sensitivity increases score by 0.037\nP19 flagged: p_rir_slope increases score by 0.039; p_reps_cv increases score by 0.024; p_velocity_cv increases score by 0.016\n\nInterpretation: These participants show unusual combinations of aggregate features (velocity consistency, load sensitivity, RIR patterns) compared to the population. The feature contributions above explain why each participant was flagged—the specific features that deviated most from population norms.\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$advanced_validation_full)) {\n  avf &lt;- lmm_results$advanced_validation_full\n\n  if (!is.null(avf$participant_profile)) {\n    pp &lt;- avf$participant_profile\n\n    if (sum(pp$is_anomaly) &gt; 0 && !is.null(pp$feature_contributions)) {\n      box::use(../R/calculators/participant_profiler[ParticipantProfiler])\n      profiler &lt;- ParticipantProfiler$new()\n\n      tryCatch({\n        profiler$plot_all_anomaly_explanations(pp, top_k = 5)\n      }, error = function(e) {\n        cat(\"Feature contribution plot not available.\\n\")\n      })\n    }\n  }\n}\n\n\n\n\n\n\n\n\nFigure 27: SHAP-like feature contributions for anomalous participants. Each bar shows how much a feature contributed to the participant’s anomaly score. Blue bars (negative) indicate features that decreased the anomaly score, while red bars (positive) indicate features that increased it.\n\n\n\n\n\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$advanced_validation_full)) {\n  avf &lt;- lmm_results$advanced_validation_full\n\n  if (!is.null(avf$participant_profile)) {\n    pp &lt;- avf$participant_profile\n\n    if (sum(pp$is_anomaly) &gt; 0 && !is.null(pp$feature_contributions)) {\n      box::use(../R/calculators/participant_profiler[ParticipantProfiler])\n      profiler &lt;- ParticipantProfiler$new()\n\n      tryCatch({\n        profiler$plot_aggregate_importance(pp, top_k = 10)\n      }, error = function(e) {\n        cat(\"Aggregate importance plot not available.\\n\")\n      })\n    }\n  }\n}\n\n\n\n\n\n\n\n\nFigure 28: Aggregate feature importance across all anomalous participants. This shows which features most commonly contribute to anomaly detection, helping identify the key behavioral patterns that distinguish anomalous participants.\n\n\n\n\n\n\n\n5.10.4 Feature Engineering Summary\nThe enhanced anomaly detection uses a comprehensive feature engineering system with 12 feature groups across multiple aggregation levels:\nFeature Group Categories:\n\nObservation-Level (per rep): Z-scores vs global and participant means, position in set\nSet-Level (per set): Mean velocity, velocity decay, coefficient of variation\nParticipant-Level (aggregated): Overall velocity patterns, RIR sensitivity, load response\nParticipant × Set: Performance deviation from participant’s own norm\nParticipant × Load: How each participant responds at different load levels\nParticipant × RIR: Velocity-RIR relationship for each participant\nCross-Set Ratios: Comparing performance across consecutive sets\nGlobal Context Ratios: Comparing each observation to population norms\nVelocity Curve Features: Decay patterns within sets\nSession-Level Features: Cumulative volume, fatigue tracking\nAdvanced Derived Features: Residuals from RIR predictions, standardized deviations\nInteraction Features: RIR × Load interactions, position × RIR\n\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$advanced_validation)) {\n  av &lt;- lmm_results$advanced_validation\n\n  if (!is.null(av$anomalies) && !is.null(av$anomalies$features_result)) {\n    fr &lt;- av$anomalies$features_result\n\n    features_df &lt;- data.frame(\n      Level = c(\"Observation\", \"Set\", \"Participant\", \"Cross-Level\"),\n      `N Features` = c(\n        length(fr$feature_names$observation),\n        length(fr$feature_names$set),\n        length(fr$feature_names$participant),\n        length(fr$feature_names$participant_set) + length(fr$feature_names$participant_load) + length(fr$feature_names$participant_rir)\n      ),\n      Description = c(\n        \"Per-rep: z-scores, position, decay contribution\",\n        \"Per-set: mean, CV, decay slope, range\",\n        \"Per-participant: velocity profile, RIR sensitivity\",\n        \"Participant × (Set, Load, RIR): cross-level interactions\"\n      ),\n      check.names = FALSE\n    )\n\n    format_table(features_df)\n  }\n}\n\n\n\n\nTable 28: Summary of engineered features for anomaly detection\n\n\n\n\n\n\n\n\n\n\n\nObservation\n25\nPer-rep: z-scores, position, decay contribution\n\n\nSet\n21\nPer-set: mean, CV, decay slope, range\n\n\nParticipant\n41\nPer-participant: velocity profile, RIR sensitivity\n\n\nCross-Level\n24\nParticipant × (Set, Load, RIR): cross-level interactions\n\n\n\n\n\n\n\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$advanced_validation)) {\n  av &lt;- lmm_results$advanced_validation\n\n  if (!is.null(av$anomalies) && !is.null(av$anomalies$features_result)) {\n    fr &lt;- av$anomalies$features_result\n\n    cat(\"**Key Observation-Level Features (for individual rep anomaly detection):**\\n\\n\")\n    key_obs &lt;- c(\"velocity_z_global\", \"velocity_z_within_participant\", \"velocity_z_within_set\",\n                 \"set_position_normalized\", \"velocity_residual_from_rir\", \"standardized_residual_rir\")\n    for (f in key_obs) {\n      if (f %in% fr$feature_names$observation) {\n        cat(sprintf(\"- `%s`\\n\", f))\n      }\n    }\n\n    cat(\"\\n**Key Participant-Level Features (for participant profiling):**\\n\\n\")\n    key_part &lt;- c(\"p_mean_velocity\", \"p_velocity_cv\", \"p_rir_slope\", \"p_rir_r2\",\n                  \"p_load_sensitivity\", \"p_mean_reps_per_set\")\n    for (f in key_part) {\n      if (f %in% fr$feature_names$participant) {\n        cat(sprintf(\"- `%s`\\n\", f))\n      }\n    }\n\n    cat(\"\\n**Feature Interpretation:**\\n\\n\")\n    cat(\"- **velocity_z_global**: How unusual is this velocity compared to all observations?\\n\")\n    cat(\"- **velocity_z_within_participant**: How unusual is this velocity for this specific participant?\\n\")\n    cat(\"- **velocity_residual_from_rir**: Deviation from expected velocity given the RIR\\n\")\n    cat(\"- **p_velocity_cv**: Coefficient of variation (consistency) across all observations\\n\")\n    cat(\"- **p_rir_slope**: How strongly velocity increases with RIR (steeper = more fatigable)\\n\")\n    cat(\"- **p_load_sensitivity**: How much velocity drops when load increases\\n\")\n  }\n}\n\nKey Observation-Level Features (for individual rep anomaly detection):\n\nvelocity_z_global\nvelocity_z_within_participant\nvelocity_z_within_set\nset_position_normalized\nvelocity_residual_from_rir\nstandardized_residual_rir\n\nKey Participant-Level Features (for participant profiling):\n\np_mean_velocity\np_velocity_cv\np_rir_slope\np_rir_r2\np_load_sensitivity\np_mean_reps_per_set\n\nFeature Interpretation:\n\nvelocity_z_global: How unusual is this velocity compared to all observations?\nvelocity_z_within_participant: How unusual is this velocity for this specific participant?\nvelocity_residual_from_rir: Deviation from expected velocity given the RIR\np_velocity_cv: Coefficient of variation (consistency) across all observations\np_rir_slope: How strongly velocity increases with RIR (steeper = more fatigable)\np_load_sensitivity: How much velocity drops when load increases\n\n\n\n5.10.5 Detection Method Comparison\nWe compare Isolation Forest with Mahalanobis distance for participant-level anomaly detection:\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$advanced_validation)) {\n  av &lt;- lmm_results$advanced_validation\n\n  if (!is.null(av$anomalies) && !is.null(av$anomalies$participant_method_comparison)) {\n    mc &lt;- av$anomalies$participant_method_comparison\n\n    mc_display &lt;- data.frame(\n      Method = mc$method,\n      `N Flagged` = mc$n_flagged,\n      Threshold = sprintf(\"%.4f\", mc$threshold),\n      `Mean Score` = sprintf(\"%.4f\", mc$score_mean),\n      `Score SD` = sprintf(\"%.4f\", mc$score_sd),\n      check.names = FALSE\n    )\n\n    format_table(mc_display)\n  }\n}\n\n\n\n\nTable 29: Comparison of participant anomaly detection methods\n\n\n\n\n\n\nisolation_forest\n4\n0.5113\n0.4811\n0.0624\n\n\nmahalanobis\n9\n0.0821\n0.1134\n0.1209\n\n\n\n\n\n\n\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$advanced_validation)) {\n  av &lt;- lmm_results$advanced_validation\n\n  if (!is.null(av$anomalies) && !is.null(av$anomalies$participant_method_comparison)) {\n    mc &lt;- av$anomalies$participant_method_comparison\n\n    if_row &lt;- mc[mc$method == \"isolation_forest\", ]\n    mah_row &lt;- mc[mc$method == \"mahalanobis\", ]\n\n    if (nrow(if_row) &gt; 0 && nrow(mah_row) &gt; 0) {\n      cat(sprintf(\n        \"**Interpretation:** Isolation Forest flagged %d participants while Mahalanobis distance flagged %d. \",\n        if_row$n_flagged, mah_row$n_flagged\n      ))\n\n      if (if_row$n_flagged == mah_row$n_flagged) {\n        cat(\"Agreement between methods increases confidence in the identified anomalies.\\n\")\n      } else {\n        cat(\"Differences between methods reflect their complementary detection approaches---Isolation Forest captures non-linear patterns while Mahalanobis assumes multivariate normality.\\n\")\n      }\n    }\n  }\n}\n\nInterpretation: Isolation Forest flagged 4 participants while Mahalanobis distance flagged 9. Differences between methods reflect their complementary detection approaches—Isolation Forest captures non-linear patterns while Mahalanobis assumes multivariate normality.\n\n\n\n5.11 Model Calibration\nModel calibration assesses whether predictions match reality. A well-calibrated model should have predictions that, on average, equal observed values.\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$advanced_validation)) {\n  av &lt;- lmm_results$advanced_validation\n\n  if (!is.null(av$calibration)) {\n    cal &lt;- av$calibration\n\n    cal_df &lt;- data.frame(\n      Metric = c(\n        \"Calibration Slope\",\n        \"Calibration Intercept\",\n        \"R-squared\",\n        \"Mean Bias\",\n        \"Assessment\"\n      ),\n      Value = c(\n        sprintf(\"%.3f\", cal$slope),\n        sprintf(\"%.4f\", cal$intercept),\n        sprintf(\"%.3f\", cal$r_squared),\n        sprintf(\"%.4e m/s\", cal$mean_bias),\n        cal$interpretation\n      )\n    )\n\n    format_table(cal_df, col.names = c(\"Metric\", \"Value\"))\n  }\n}\n\n\n\n\nTable 30: Model calibration metrics\n\n\n\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\nCalibration Slope\n1.036\n\n\nCalibration Intercept\n-0.0103\n\n\nR-squared\n0.708\n\n\nMean Bias\n3.0791e-16 m/s\n\n\nAssessment\nCalibration: slope = 1.036 (excellent), intercept = -0.0103 (small bias)\n\n\n\n\n\n\n\n\nInterpretation: A calibration slope close to 1.0 and intercept close to 0 indicates excellent calibration. The model’s predictions are unbiased representations of reality.\n\n\n5.11 Advanced Validation Summary\n\n\nShow code\nif (!is.null(lmm_results) && !is.null(lmm_results$advanced_validation)) {\n  av &lt;- lmm_results$advanced_validation\n\n  summary_rows &lt;- list()\n\n  # Coefficient stability\n  if (!is.null(av$coefficient_stability)) {\n    summary_rows[[1]] &lt;- data.frame(\n      Check = \"Coefficient Stability (LOO-CV)\",\n      Result = sprintf(\"CV = %.1f%%\", av$coefficient_stability$cv_percent),\n      Status = if (av$coefficient_stability$is_stable) \"✓ Stable\" else \"⚠ Variable\"\n    )\n  }\n\n  # Model selection stability\n  if (!is.null(av$model_selection_stability)) {\n    summary_rows[[2]] &lt;- data.frame(\n      Check = \"Model Selection Stability\",\n      Result = sprintf(\"%.0f%% agreement\", av$model_selection_stability$stability_percent),\n      Status = if (av$model_selection_stability$stability_percent &gt;= 90) \"✓ Excellent\" else \"⚠ Variable\"\n    )\n  }\n\n  # Prediction error\n  if (!is.null(av$prediction_error_by_participant)) {\n    summary_rows[[3]] &lt;- data.frame(\n      Check = \"Out-of-Sample Error\",\n      Result = sprintf(\"RMSE = %.3f, MAE = %.3f\",\n                       av$prediction_error_by_participant$overall_rmse,\n                       av$prediction_error_by_participant$overall_mae),\n      Status = \"✓ Acceptable\"\n    )\n  }\n\n  # Anomaly detection\n  if (!is.null(av$anomalies) && !is.null(av$anomalies$raw_data)) {\n    summary_rows[[4]] &lt;- data.frame(\n      Check = \"Anomaly Detection (IF)\",\n      Result = sprintf(\"%.1f%% flagged\", av$anomalies$raw_data$anomaly_rate * 100),\n      Status = if (av$anomalies$raw_data$anomaly_rate &lt; 0.10) \"✓ Normal\" else \"⚠ Review\"\n    )\n  }\n\n  # Calibration\n  if (!is.null(av$calibration)) {\n    summary_rows[[5]] &lt;- data.frame(\n      Check = \"Model Calibration\",\n      Result = sprintf(\"Slope = %.3f, R² = %.3f\", av$calibration$slope, av$calibration$r_squared),\n      Status = if (av$calibration$slope &gt; 0.9 && av$calibration$slope &lt; 1.1) \"✓ Excellent\" else \"⚠ Review\"\n    )\n  }\n\n  summary_df &lt;- do.call(rbind, summary_rows)\n  format_table(summary_df, col.names = c(\"Validation Check\", \"Result\", \"Status\"))\n}\n\n\n\n\nTable 31: Summary of advanced validation checks\n\n\n\n\n\n\n\n\n\n\n\nValidation Check\nResult\nStatus\n\n\n\n\nCoefficient Stability (LOO-CV)\nCV = 3.0%\n✓ Stable\n\n\nModel Selection Stability\n100% agreement\n✓ Excellent\n\n\nOut-of-Sample Error\nRMSE = 0.082, MAE = 0.062\n✓ Acceptable\n\n\nAnomaly Detection (IF)\n5.4% flagged\n✓ Normal\n\n\nModel Calibration\nSlope = 1.036, R² = 0.708\n✓ Excellent\n\n\n\n\n\n\n\n\nOverall Conclusion: The advanced validation confirms that our model is robust, well-calibrated, and generalizes to held-out participants. The coefficient stability analysis shows that no single participant drives our conclusions, and the anomaly detection identifies a small proportion of unusual observations without invalidating the overall findings."
  },
  {
    "objectID": "deadlift_study.html#discussion",
    "href": "deadlift_study.html#discussion",
    "title": "Velocity-Based Training for the Conventional Deadlift",
    "section": "6. Discussion",
    "text": "6. Discussion\n\n6.1 Key Findings Summary\nThis research provides the first systematic examination of the velocity-RIR relationship for the conventional deadlift:\n\nThe velocity-RIR relationship exists for deadlifts, though with greater variability than squats\nIndividual models substantially outperform general equations (~2x improvement)\nPrediction accuracy is acceptable (~1.4 rep error) but should account for uncertainty\nMVT varies considerably between individuals (CV ~25%), requiring individual calibration\nDay-to-day reliability is moderate, suggesting periodic recalibration\nFirst-rep velocity predicts set capacity with practical accuracy\n\n\n\n6.2 Practical Recommendations\n\nFor Athletes and Coaches\n\nUse individual calibration: General equations lose significant precision\nAccept greater prediction uncertainty: Target conservatively (e.g., RIR 3 if aiming for RIR 2)\nCombine VBT with RPE: Velocity is one tool, not the only tool\nRecalibrate periodically: Every 2-4 weeks or after significant training blocks\n\n\n\nVelocity Reference Table\n\n\nShow code\ntargets &lt;- aggregate(mean_velocity ~ rir + load_percentage, data = data, FUN = mean)\ntargets &lt;- reshape(targets, idvar = \"rir\", timevar = \"load_percentage\",\n                   direction = \"wide\", v.names = \"mean_velocity\")\nnames(targets) &lt;- c(\"RIR\", \"80% 1RM\", \"90% 1RM\")\ntargets &lt;- targets[order(-targets$RIR), ]\ntargets$`80% 1RM` &lt;- round(targets$`80% 1RM`, 3)\ntargets$`90% 1RM` &lt;- round(targets$`90% 1RM`, 3)\nrownames(targets) &lt;- NULL\n\nformat_table(targets, col.names = c(\"RIR\", \"80% 1RM (m/s)\", \"90% 1RM (m/s)\"))\n\n\n\n\nTable 32: Population-Average Velocity Thresholds (Use for Initial Reference Only)\n\n\n\n\n\n\nRIR\n80% 1RM (m/s)\n90% 1RM (m/s)\n\n\n\n\n7\n0.376\n0.283\n\n\n6\n0.372\n0.380\n\n\n5\n0.340\n0.408\n\n\n4\n0.334\n0.339\n\n\n3\n0.325\n0.303\n\n\n2\n0.288\n0.268\n\n\n1\n0.258\n0.229\n\n\n0\n0.217\n0.196\n\n\n\n\n\n\n\n\nThese are population averages. Individual calibration will substantially improve accuracy.\n\n\n\n6.3 Limitations\n\nSample size (n=19) limits statistical power\nLoad range (80-90% only) may not generalize to lighter loads\nSingle exercise variant (conventional deadlift only)\nShort-term reliability (2 days) - longer-term reliability unknown\nEquipment specificity - different devices may yield different values\n\n\n\n6.4 Future Directions\n\nLarger sample sizes with more diverse populations\nBroader load ranges (60-95% 1RM)\nSumo and trap bar deadlift variants\nLonger-term reliability studies\nIntegration with RPE for combined autoregulation strategies"
  },
  {
    "objectID": "deadlift_study.html#conclusion",
    "href": "deadlift_study.html#conclusion",
    "title": "Velocity-Based Training for the Conventional Deadlift",
    "section": "7. Conclusion",
    "text": "7. Conclusion\nThis thesis research demonstrates that velocity-based training principles can be applied to the conventional deadlift, though with important caveats:\n\nVBT works for deadlifts but with more variability than other exercises\nIndividual calibration is essential due to high inter-individual variability\nPractical velocity tables can guide training, but should be used alongside other autoregulation tools\nPeriodic recalibration is recommended given moderate day-to-day reliability\n\nFor coaches and athletes, VBT offers an objective complement to RPE-based autoregulation for the deadlift, but should not be used as the sole prescription tool."
  },
  {
    "objectID": "deadlift_study.html#references",
    "href": "deadlift_study.html#references",
    "title": "Velocity-Based Training for the Conventional Deadlift",
    "section": "References",
    "text": "References\n\nGonzález-Badillo, J. J., & Sánchez-Medina, L. (2010). Movement velocity as a measure of loading intensity in resistance training. International Journal of Sports Medicine, 31(5), 347-352.\nJukic, I., Prnjak, K., Helms, E. R., & McGuigan, M. R. (2024). Modeling the repetitions-in-reserve-velocity relationship. Physiological Reports, 12(5), e15955.\nKoo, T. K., & Li, M. Y. (2016). A guideline of selecting and reporting intraclass correlation coefficients for reliability research. Journal of Chiropractic Medicine, 15(2), 155-163.\nShrout, P. E., & Fleiss, J. L. (1979). Intraclass correlations: Uses in assessing rater reliability. Psychological Bulletin, 86(2), 420-428.\nZourdos, M. C., et al. (2016). Novel resistance training-specific rating of perceived exertion scale measuring repetitions in reserve. Journal of Strength and Conditioning Research, 30(1), 267-275.\n\n\nMSc Thesis Research - Filipe Braga Supervision - João Costa"
  },
  {
    "objectID": "03_rir_velocity_modeling.html",
    "href": "03_rir_velocity_modeling.html",
    "title": "Can We Predict How Many Reps You Have Left?",
    "section": "",
    "text": "Imagine you’re in the middle of a heavy set of squats. Your coach asks: “How many reps do you have left?” If you could answer accurately, you’d have a powerful tool for managing training intensity and fatigue.\nThis is where Repetitions in Reserve (RIR) comes in—a way to quantify how close you are to failure. But can we do better than guessing? Can we use objective measurements like bar velocity to predict RIR?"
  },
  {
    "objectID": "03_rir_velocity_modeling.html#the-question",
    "href": "03_rir_velocity_modeling.html#the-question",
    "title": "Can We Predict How Many Reps You Have Left?",
    "section": "",
    "text": "Imagine you’re in the middle of a heavy set of squats. Your coach asks: “How many reps do you have left?” If you could answer accurately, you’d have a powerful tool for managing training intensity and fatigue.\nThis is where Repetitions in Reserve (RIR) comes in—a way to quantify how close you are to failure. But can we do better than guessing? Can we use objective measurements like bar velocity to predict RIR?"
  },
  {
    "objectID": "03_rir_velocity_modeling.html#the-study",
    "href": "03_rir_velocity_modeling.html#the-study",
    "title": "Can We Predict How Many Reps You Have Left?",
    "section": "The Study",
    "text": "The Study\nJukic et al. (2024) investigated whether the relationship between bar velocity and RIR could be used for training monitoring and prescription. They compared:\n\nGeneral models: One equation for everyone\nIndividual models: Personalized equations for each lifter\n\nTheir key findings:\n\nIndividual models explain about 2x more variance than general models\nPolynomial (curved) relationships fit better than linear ones\nPrediction accuracy is acceptable for practical use"
  },
  {
    "objectID": "03_rir_velocity_modeling.html#our-data",
    "href": "03_rir_velocity_modeling.html#our-data",
    "title": "Can We Predict How Many Reps You Have Left?",
    "section": "Our Data",
    "text": "Our Data\nWe’re analyzing data from 46 resistance-trained individuals performing back squats at three intensities:\n\n\nShow code\nsummary_table(\n  metrics = c(\"Participants\", \"Male\", \"Female\",\n              \"Observations\", \"Velocity Range (m/s)\",\n              \"RIR Range\", \"Relative Strength (1RM/BW)\"),\n  values = c(\n    results$summary$n_participants,\n    round(results$summary$n_male),\n    round(results$summary$n_female),\n    results$summary$n_observations,\n    paste(round(results$summary$velocity_range, 2), collapse = \" - \"),\n    paste(results$summary$rir_range, collapse = \" - \"),\n    round(results$summary$mean_relative_strength, 2)\n  )\n)\n\n\n\n\nTable 1: Participant Characteristics\n\n\n\n\n\n\nParticipants\n46\n\n\nMale\n30\n\n\nFemale\n16\n\n\nObservations\n2601\n\n\nVelocity Range (m/s)\n0.13 - 0.81\n\n\nRIR Range\n0 - 25\n\n\nRelative Strength (1RM/BW)\n1.6"
  },
  {
    "objectID": "03_rir_velocity_modeling.html#the-velocity-rir-relationship",
    "href": "03_rir_velocity_modeling.html#the-velocity-rir-relationship",
    "title": "Can We Predict How Many Reps You Have Left?",
    "section": "The Velocity-RIR Relationship",
    "text": "The Velocity-RIR Relationship\nAs you get closer to failure, your bar speed slows down. This creates a relationship we can model:\n\n\nShow code\n# Create velocity-RIR scatter plot\nggplot(data, aes(x = mean_velocity, y = rir, color = set_type)) +\n  geom_point(alpha = 0.3, size = 1) +\n  geom_smooth(method = \"lm\", formula = y ~ poly(x, 2), se = FALSE, linewidth = 1.2) +\n  facet_wrap(~set_type, labeller = labeller(set_type = c(\n    RTF70 = \"70% 1RM\",\n    RTF80 = \"80% 1RM\",\n    RTF90 = \"90% 1RM\"\n  ))) +\n  labs(\n    x = \"Mean Velocity (m/s)\",\n    y = \"Repetitions in Reserve\",\n    color = \"Load\"\n  ) +\n  theme(\n    legend.position = \"none\",\n    strip.text = element_text(face = \"bold\", size = 12)\n  ) +\n  scale_color_manual(values = c(COLORS$load_80, COLORS$primary, COLORS$load_90))\n\n\n\n\n\n\n\n\nFigure 1: Velocity-RIR relationship across all participants and loads\n\n\n\n\n\nWhat you’re seeing: Each point is one repetition. As velocity decreases (moving left), RIR decreases (you’re closer to failure). The curves show polynomial fits for each load."
  },
  {
    "objectID": "03_rir_velocity_modeling.html#general-vs-individual-models",
    "href": "03_rir_velocity_modeling.html#general-vs-individual-models",
    "title": "Can We Predict How Many Reps You Have Left?",
    "section": "General vs Individual Models",
    "text": "General vs Individual Models\nHere’s where it gets interesting. A “general” model uses everyone’s data to create one equation. An “individual” model creates a unique equation for each person.\n\n\nShow code\nmodel_comparison_table(\n  models = c(\"General (Polynomial)\", \"Individual (Polynomial)\"),\n  r2_values = c(results$comparison$general_r2, results$comparison$individual_r2),\n  interpretations = c(\"Explains ~50% of variance\", \"Explains ~88% of variance\")\n)\n\n\n\n\nTable 2: Model Fit Comparison: General vs Individual\n\n\n\n\n\n\nGeneral (Polynomial)\n0.495\nExplains ~50% of variance\n\n\nIndividual (Polynomial)\n0.884\nExplains ~88% of variance\n\n\n\n\n\n\n\n\nThe takeaway: Individual models explain 1.8x more variance than general models. This means personalized equations are much better at predicting your RIR."
  },
  {
    "objectID": "03_rir_velocity_modeling.html#why-does-this-matter",
    "href": "03_rir_velocity_modeling.html#why-does-this-matter",
    "title": "Can We Predict How Many Reps You Have Left?",
    "section": "Why Does This Matter?",
    "text": "Why Does This Matter?\n\nFor Athletes and Coaches\nIf you can accurately predict RIR from velocity:\n\nAuto-regulate training: Stop a set when velocity indicates 2 RIR instead of guessing\nManage fatigue: Track velocity loss across a session to monitor accumulated fatigue\nOptimize intensity: Ensure you’re training at the intended proximity to failure\n\n\n\nFor Researchers\nThe strong individual relationships suggest:\n\nVelocity-based training is valid for monitoring RIR\nIndividualization is crucial—one-size-fits-all equations lose precision\nSimple technology (velocity trackers) can provide actionable data"
  },
  {
    "objectID": "03_rir_velocity_modeling.html#model-details-by-load",
    "href": "03_rir_velocity_modeling.html#model-details-by-load",
    "title": "Can We Predict How Many Reps You Have Left?",
    "section": "Model Details by Load",
    "text": "Model Details by Load\n\n\nShow code\ndata.frame(\n  Load = c(\"90% 1RM\", \"80% 1RM\", \"70% 1RM\"),\n  `Linear R2` = c(\n    round(results$general_results$RTF90$linear$r_squared, 3),\n    round(results$general_results$RTF80$linear$r_squared, 3),\n    round(results$general_results$RTF70$linear$r_squared, 3)\n  ),\n  `Polynomial R2` = c(\n    round(results$general_results$RTF90$polynomial$r_squared, 3),\n    round(results$general_results$RTF80$polynomial$r_squared, 3),\n    round(results$general_results$RTF70$polynomial$r_squared, 3)\n  ),\n  `RSE (reps)` = c(\n    round(results$general_results$RTF90$polynomial$rse, 2),\n    round(results$general_results$RTF80$polynomial$rse, 2),\n    round(results$general_results$RTF70$polynomial$rse, 2)\n  ),\n  check.names = FALSE\n) |&gt; format_table(col.names = c(\"Load\", \"Linear R2\", \"Polynomial R2\", \"RSE (reps)\"))\n\n\n\n\nTable 3: General Model Performance by Load\n\n\n\n\n\n\nLoad\nLinear R2\nPolynomial R2\nRSE (reps)\n\n\n\n\n90% 1RM\n0.466\n0.469\n1.22\n\n\n80% 1RM\n0.512\n0.516\n2.28\n\n\n70% 1RM\n0.500\n0.501\n3.43\n\n\n\n\n\n\n\n\nPattern: Heavier loads (90% 1RM) have lower RSE because there are fewer possible reps, making prediction easier in absolute terms."
  },
  {
    "objectID": "03_rir_velocity_modeling.html#individual-model-distribution",
    "href": "03_rir_velocity_modeling.html#individual-model-distribution",
    "title": "Can We Predict How Many Reps You Have Left?",
    "section": "Individual Model Distribution",
    "text": "Individual Model Distribution\n\n\nShow code\n# Extract R2 values from individual results\nr2_values &lt;- c()\nfor (id in names(results$general_results)) {\n  if (!is.null(results$general_results[[id]]$polynomial$r_squared)) {\n    r2_values &lt;- c(r2_values, results$general_results[[id]]$polynomial$r_squared)\n  }\n}\n\n# Create histogram using the summary data\nindividual_r2_median &lt;- results$polynomial_summary$median_r_squared\nindividual_r2_range &lt;- c(results$polynomial_summary$min_r_squared,\n                          results$polynomial_summary$max_r_squared)\n\n# Simple visualization\nggplot(data.frame(x = 1), aes(x = x)) +\n  annotate(\"rect\", xmin = 0.2, xmax = 1.0, ymin = 0, ymax = 1,\n           fill = COLORS$primary, alpha = 0.3) +\n  annotate(\"segment\", x = individual_r2_median, xend = individual_r2_median,\n           y = 0, yend = 1, color = COLORS$primary, linewidth = 2) +\n  annotate(\"text\", x = individual_r2_median, y = 1.1,\n           label = paste(\"Median:\", round(individual_r2_median, 2)),\n           fontface = \"bold\") +\n  annotate(\"text\", x = individual_r2_range[1], y = 0.5,\n           label = paste(\"Min:\", round(individual_r2_range[1], 2))) +\n  annotate(\"text\", x = individual_r2_range[2], y = 0.5,\n           label = paste(\"Max:\", round(individual_r2_range[2], 2))) +\n  scale_x_continuous(limits = c(0, 1.1), breaks = seq(0, 1, 0.2)) +\n  labs(x = \"R-squared (Individual Models)\", y = \"\") +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank()\n  )\n\n\n\n\n\n\n\n\nFigure 2: Distribution of R-squared values for individual polynomial models\n\n\n\n\n\nKey insight: Most individual models achieve R-squared &gt; 0.80, meaning velocity explains over 80% of the variance in RIR for most people."
  },
  {
    "objectID": "03_rir_velocity_modeling.html#validation-against-the-paper",
    "href": "03_rir_velocity_modeling.html#validation-against-the-paper",
    "title": "Can We Predict How Many Reps You Have Left?",
    "section": "Validation Against the Paper",
    "text": "Validation Against the Paper\n\n\n\nMetric\nPaper Finding\nOur Replication\nMatch?\n\n\n\n\nGeneral R-squared\n~0.50-0.60\n0.5\nYes\n\n\nIndividual R-squared (median)\n~0.85-0.95\n0.88\nYes\n\n\nImprovement factor\n~2x\n1.78x\nYes"
  },
  {
    "objectID": "03_rir_velocity_modeling.html#practical-recommendations",
    "href": "03_rir_velocity_modeling.html#practical-recommendations",
    "title": "Can We Predict How Many Reps You Have Left?",
    "section": "Practical Recommendations",
    "text": "Practical Recommendations\nBased on these findings:\n\nBuild your own velocity-RIR curve: Perform sets to failure at different loads while tracking velocity. Use this data to create your personalized equation.\nUse polynomial models: The curved relationship captures the data better than a straight line, especially as you approach failure.\nRecalibrate periodically: Your velocity-RIR relationship may shift with training adaptations.\nAccept some error: Even good individual models have ~1 rep error. Use velocity as a guide, not gospel."
  },
  {
    "objectID": "03_rir_velocity_modeling.html#technical-notes",
    "href": "03_rir_velocity_modeling.html#technical-notes",
    "title": "Can We Predict How Many Reps You Have Left?",
    "section": "Technical Notes",
    "text": "Technical Notes\nThis analysis used:\n\nR6 classes for clean, object-oriented code\nLinear and polynomial regression for model fitting\nCross-validation (Day 1 -&gt; Day 2 prediction) for practical validity\n\nData source: OSF Repository"
  },
  {
    "objectID": "03_rir_velocity_modeling.html#references",
    "href": "03_rir_velocity_modeling.html#references",
    "title": "Can We Predict How Many Reps You Have Left?",
    "section": "References",
    "text": "References\nJukic, I., Prnjak, K., Helms, E. R., & McGuigan, M. R. (2024). Modeling the repetitions-in-reserve-velocity relationship: A valid method for resistance training monitoring and prescription, and fatigue management. Physiological Reports, 12(5), e15955."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Velocity-Based Training & RIR Research",
    "section": "",
    "text": "This research compendium explores fundamental questions in resistance training through velocity-based training (VBT) methods:\n\nCan bar velocity objectively measure training proximity to failure?\nDoes this relationship hold for the deadlift, and how can we apply it in practice?\n\n\n\nThis project is organized into two main sections:\n\nBackground Studies (Studies 1-3) - Replications of key published research to validate methodology\nDeadlift Study (Thesis) - Original research on the conventional deadlift forming Filipe Braga’s MSc thesis\n\n\n\n\nThe original research investigates whether velocity-based training principles, well-established for exercises like the squat and bench press, can be successfully applied to the conventional deadlift—an exercise with unique biomechanical characteristics:\n\nNo stretch-shortening cycle (concentric-only movement)\nGrip limitations affecting perceived fatigue\nBinary failure patterns (grip release vs gradual deceleration)"
  },
  {
    "objectID": "index.html#about-this-research",
    "href": "index.html#about-this-research",
    "title": "Velocity-Based Training & RIR Research",
    "section": "",
    "text": "This research compendium explores fundamental questions in resistance training through velocity-based training (VBT) methods:\n\nCan bar velocity objectively measure training proximity to failure?\nDoes this relationship hold for the deadlift, and how can we apply it in practice?\n\n\n\nThis project is organized into two main sections:\n\nBackground Studies (Studies 1-3) - Replications of key published research to validate methodology\nDeadlift Study (Thesis) - Original research on the conventional deadlift forming Filipe Braga’s MSc thesis\n\n\n\n\nThe original research investigates whether velocity-based training principles, well-established for exercises like the squat and bench press, can be successfully applied to the conventional deadlift—an exercise with unique biomechanical characteristics:\n\nNo stretch-shortening cycle (concentric-only movement)\nGrip limitations affecting perceived fatigue\nBinary failure patterns (grip release vs gradual deceleration)"
  },
  {
    "objectID": "index.html#key-concepts",
    "href": "index.html#key-concepts",
    "title": "Velocity-Based Training & RIR Research",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nRepetitions in Reserve (RIR)\nRIR measures how close you are to muscular failure:\n\n\n\n\n\n\n\n\nRIR\nDescription\nExample\n\n\n\n\n0\nComplete failure\nCouldn’t complete another rep\n\n\n1-2\nNear failure\n1-2 reps left before failure\n\n\n3-4\nModerate intensity\nNoticeable fatigue but not close to failure\n\n\n5+\nLow intensity\nCould do many more reps\n\n\n\n\n\nVelocity-Based Training (VBT)\nAn objective approach to measuring effort using bar speed:\n\nFaster velocity = More reps remaining\nSlower velocity = Closer to failure\nVelocity loss = Fatigue accumulation within a set"
  },
  {
    "objectID": "index.html#background-studies",
    "href": "index.html#background-studies",
    "title": "Velocity-Based Training & RIR Research",
    "section": "Background Studies",
    "text": "Background Studies\nThese replications validate our methodology before applying it to original deadlift research.\n\nStudy 1: Does Training to Failure Matter?\nPelland et al. (2024) Replication\nA meta-regression examining RIR and training outcomes across strength and hypertrophy studies.\n\n\n\n\n\n\nKey Finding\n\n\n\nTraining closer to failure benefits hypertrophy but not strength.\n\n\n\n\n\nOutcome\nRIR Effect\nSignificant?\n\n\n\n\nStrength\n~0\nNo\n\n\nHypertrophy\nNegative\nYes\n\n\n\n\n\n\nStudy 2: Can Bar Speed Predict Effort?\nPaulsen et al. (2025) Replication\nAnalysis examining velocity-RIR relationships in squat and bench press.\n\n\n\n\n\n\nKey Finding\n\n\n\nVelocity moderately predicts perceived effort (r ~ 0.5), but varies substantially between individuals.\n\n\n\n\n\nMetric\nValue\n\n\n\n\nOverall correlation\nr = 0.45\n\n\nIndividual range\nr = 0.09 to 0.79\n\n\n\n\n\n\nStudy 3: General vs Individual Models\nJukic et al. (2024) Replication\nAnalysis examining whether individual RIR-velocity models outperform general equations.\n\n\n\n\n\n\nKey Finding\n\n\n\nIndividual models explain ~2x more variance than general models (R-squared = 0.88 vs 0.50).\n\n\n\n\n\nModel Type\nMedian R-squared\nImplication\n\n\n\n\nGeneral\n0.50\nOne-size-fits-all loses precision\n\n\nIndividual\n0.88\nPersonalization is essential"
  },
  {
    "objectID": "index.html#deadlift-study-msc-thesis",
    "href": "index.html#deadlift-study-msc-thesis",
    "title": "Velocity-Based Training & RIR Research",
    "section": "Deadlift Study (MSc Thesis)",
    "text": "Deadlift Study (MSc Thesis)\nFull Deadlift Study Report\nThis comprehensive report presents original thesis research investigating the velocity-RIR relationship for the conventional deadlift. It combines:\n\nExploratory analysis of the deadlift velocity-RIR relationship\nLinear Mixed Effects Models with nested random effects\nPractical velocity stop tables for training prescription\nAdvanced analyses including reliability, model comparison, and failure prediction\n\n\nKey Findings Summary\n\nShow code\nif (!is.null(deadlift_results)) {\n  cat(\"| Metric | Deadlift | Squat (Reference) |\\n\")\n  cat(\"|--------|----------|-------------------|\\n\")\n  cat(sprintf(\"| General R-squared | %.2f | 0.50 |\\n\", deadlift_results$comparison$general_r2))\n  cat(sprintf(\"| Individual R-squared | %.2f | 0.88 |\\n\", deadlift_results$comparison$individual_r2))\n  cat(sprintf(\"| Improvement Factor | %.1fx | 1.8x |\\n\", deadlift_results$comparison$improvement_factor))\n  cat(sprintf(\"| Participants | %d | 19 |\\n\", deadlift_results$summary$n_participants))\n} else {\n  cat(\"*Results will be available after running the analysis pipeline*\\n\")\n}\n\n\n\n\nMetric\nDeadlift\nSquat (Reference)\n\n\n\n\nGeneral R-squared\n0.36\n0.50\n\n\nIndividual R-squared\n0.71\n0.88\n\n\nImprovement Factor\n2.0x\n1.8x\n\n\nParticipants\n19\n19\n\n\n\n\n\nWhy Lower for Deadlifts?\nThe deadlift presents unique challenges for velocity-based training:\n\nBinary failure - Grip often fails suddenly rather than gradual velocity decline\nNo eccentric phase - Each rep starts from a dead stop (no stretch-shortening cycle)\nGrip fatigue - May affect velocity independently of target muscle fatigue"
  },
  {
    "objectID": "index.html#summary-of-key-findings",
    "href": "index.html#summary-of-key-findings",
    "title": "Velocity-Based Training & RIR Research",
    "section": "Summary of Key Findings",
    "text": "Summary of Key Findings\nBased on all studies in this compendium:\n\n\n\n\n\n\n\n\nQuestion\nFinding\nPractical Implication\n\n\n\n\nDoes VBT work for deadlifts?\nYes, but with more variability than squats\nUse VBT as one tool, not the only tool\n\n\nIndividual vs general models?\nIndividual models are ~2x more accurate\nCalibrate individually for serious athletes\n\n\nDoes load matter?\nNo significant effect (80% vs 90%)\nOne velocity table works across loads\n\n\nHow often to recalibrate?\nModerate day-to-day reliability\nUpdate every 2-4 weeks\n\n\nCan first rep predict set capacity?\nYes, with ~1-2 rep accuracy\nUse for real-time autoregulation"
  },
  {
    "objectID": "index.html#practical-implications",
    "href": "index.html#practical-implications",
    "title": "Velocity-Based Training & RIR Research",
    "section": "Practical Implications",
    "text": "Practical Implications\n\nFor Strength Training\nBased on the Pelland meta-regression:\n\nStop 2-3 reps short of failure on most sets\nFailure training is not necessary for strength gains\nThis allows higher training frequency and volume\n\n\n\nFor Muscle Building\nBased on the Pelland meta-regression:\n\nTrain closer to failure (0-2 RIR) for optimal hypertrophy\nThe relationship is statistically significant but effect size is modest\nBalance against recovery demands\n\n\n\nFor Velocity-Based Training\nBased on the Paulsen analysis:\n\nIndividual calibration is essential - Group averages don’t work for everyone\nExercise-specific targets - Different exercises need different thresholds\nUse as complement - Velocity + RPE together is better than either alone\n\n\n\nFor Deadlift Training\nBased on the thesis research:\n\nVBT works, but with caveats - Expect ~1.4 rep prediction error (vs ~1 rep for squat)\nIndividual calibration still essential - 2x improvement over general equations\nConsider grip fatigue - May affect velocity independently of target muscle fatigue"
  },
  {
    "objectID": "index.html#data-sources",
    "href": "index.html#data-sources",
    "title": "Velocity-Based Training & RIR Research",
    "section": "Data Sources",
    "text": "Data Sources\n\n\n\nStudy\nData\nType\n\n\n\n\nPelland et al. (2024)\nOSF Repository\nReplication\n\n\nPaulsen et al. (2025)\nPeerJ Supplement\nReplication\n\n\nJukic et al. (2024)\nOSF Repository\nReplication\n\n\nDeadlift Thesis (2024)\nOriginal Data\nOriginal Research"
  },
  {
    "objectID": "index.html#methodology",
    "href": "index.html#methodology",
    "title": "Velocity-Based Training & RIR Research",
    "section": "Methodology",
    "text": "Methodology\nAll analyses use:\n\nR for statistical computing\nlme4 for Linear Mixed Effects Models\nmetafor for meta-analysis models\nQuarto for reproducible reports\nrenv for dependency management\n\n\nCode Architecture\nThe project follows SOLID principles with R6 classes:\n\nR/domain/ - Domain objects (EffectSize, TreatmentGroup)\nR/calculators/ - Analysis services (RirVelocityModeler, AdvancedVelocityAnalyzer)\nR/loaders/ - Data loading classes (DeadliftRirDataLoader)\nscripts/ - Analysis scripts\nanalyses/ - Quarto reports\n\n\nResearch conducted by Joao Costa and Filipe Braga (MSc Thesis)"
  },
  {
    "objectID": "statistical_guide.html",
    "href": "statistical_guide.html",
    "title": "Statistical Methods Guide",
    "section": "",
    "text": "I want to understand…\nGo to section…\n\n\n\n\nWhy we used mixed models\nSection 1\n\n\nHow to read the plots\nSection 2\n\n\nWhat the numbers mean\nSection 3\n\n\nModel validation\nSection 4\n\n\nHow to apply this\nSection 5\n\n\n\n\n\n\n\n\n\nFor the Impatient Reader\n\n\n\nIf you just want the key takeaways:\n\nMixed models account for repeated measurements from the same athletes\nIndividual calibration is ~2x more accurate than population averages\nRecalibrate every 2-4 weeks due to moderate day-to-day reliability\nUse velocity tables as a guide, not gospel"
  },
  {
    "objectID": "statistical_guide.html#quick-start-which-section-do-i-need",
    "href": "statistical_guide.html#quick-start-which-section-do-i-need",
    "title": "Statistical Methods Guide",
    "section": "",
    "text": "I want to understand…\nGo to section…\n\n\n\n\nWhy we used mixed models\nSection 1\n\n\nHow to read the plots\nSection 2\n\n\nWhat the numbers mean\nSection 3\n\n\nModel validation\nSection 4\n\n\nHow to apply this\nSection 5\n\n\n\n\n\n\n\n\n\nFor the Impatient Reader\n\n\n\nIf you just want the key takeaways:\n\nMixed models account for repeated measurements from the same athletes\nIndividual calibration is ~2x more accurate than population averages\nRecalibrate every 2-4 weeks due to moderate day-to-day reliability\nUse velocity tables as a guide, not gospel"
  },
  {
    "objectID": "statistical_guide.html#why-mixed-models",
    "href": "statistical_guide.html#why-mixed-models",
    "title": "Statistical Methods Guide",
    "section": "2 Why Mixed Models?",
    "text": "2 Why Mixed Models?\n\n2.1 The Problem: Non-Independent Data\nWhen the same athlete provides multiple measurements, those measurements are correlated—they share that athlete’s unique characteristics (strength, technique, fatigue resistance).\nThe Analogy: Imagine asking 10 friends to rate a movie. Each friend rates the movie 5 times. You now have 50 ratings, but they’re not 50 independent opinions—they’re 10 opinions, each expressed 5 times.\nWhy This Matters:\n\n\n\nIf you ignore clustering…\nWhat happens…\n\n\n\n\nStandard errors are too small\nYou’re overconfident\n\n\nP-values are too small\nMore false positives\n\n\nConfidence intervals are too narrow\nMisleading precision\n\n\n\n\n\n2.2 The Solution: Random Effects\nMixed models include random effects that account for individual differences:\n\nPlain LanguageMathematical Details\n\n\n\nRandom intercepts: Each athlete has their own baseline velocity (some are naturally faster lifters)\nRandom slopes: Each athlete has their own RIR-velocity relationship (some slow down more as they fatigue)\n\n\n\n\n\n\n\n\n\nThe Model Formula\n\n\n\n\n\nThe basic mixed model is:\n\\[\ny_{ij} = \\beta_0 + \\beta_1 x_{ij} + u_{0j} + u_{1j} x_{ij} + \\varepsilon_{ij}\n\\]\nWhere:\n\n\\(y_{ij}\\) = velocity for observation \\(i\\) from athlete \\(j\\)\n\\(\\beta_0, \\beta_1\\) = fixed effects (population average intercept and slope)\n\\(u_{0j}\\) = random intercept for athlete \\(j\\) (deviation from average baseline)\n\\(u_{1j}\\) = random slope for athlete \\(j\\) (deviation from average RIR effect)\n\\(\\varepsilon_{ij}\\) = residual error\n\n\n\n\n\n\n\n\n\n2.3 Three Questions to Ask Before Using Mixed Models\n\nDo individuals differ at baseline?\n\nLook at individual means—is there substantial variation?\nIf yes → include random intercepts\n\nDo individuals differ in their response?\n\nLook at individual slopes—do some show steeper/flatter relationships?\nIf yes → include random slopes\n\nDoes the design support random slopes?\n\nNeed multiple observations per individual at different predictor values\nRule of thumb: at least 5-10 observations per individual"
  },
  {
    "objectID": "statistical_guide.html#reading-the-plots",
    "href": "statistical_guide.html#reading-the-plots",
    "title": "Statistical Methods Guide",
    "section": "3 Reading the Plots",
    "text": "3 Reading the Plots\n\n3.1 Model Plot (Spaghetti Plot)\nThe model plot shows individual athlete trajectories overlaid with the population average.\nWhat to look for:\n\n\n\n\n\n\n\n\n\nElement\nWhat it shows\nGood sign\nWarning sign\n\n\n\n\nGray lines\nIndividual athlete fits\nParallel-ish lines\nLines crossing everywhere\n\n\nRed line\nPopulation average\nCentral tendency\nFar from most individuals\n\n\nPoints\nActual data\nClustered around lines\nScattered randomly\n\n\nLine spread\nIndividual variability\nModerate spread\nExtreme spread\n\n\n\n\n\n3.2 Caterpillar Plot\nThe caterpillar plot shows each athlete’s deviation from the population average.\nHow to read it:\n\nZero line = the population average\nPoints = each athlete’s estimated deviation\nError bars = 95% confidence interval\nCrossing zero = not meaningfully different from average\n\nInterpretation guide:\n\nAthletes whose intervals DON’T cross zero differ significantly from average\nWide intervals = more uncertainty (often fewer observations)\nThe spread of points shows the amount of individual variation\n\n\n\n3.3 Q-Q Plot (Normality Check)\nThe basic idea: If residuals are normally distributed, points fall on the diagonal line.\n\n\n\nWhat you see\nWhat it means\nConcern level\n\n\n\n\nPoints on line\nNormal distribution\nNone\n\n\nSlight curve at ends\nLight tails\nLow\n\n\nS-shape\nHeavy tails\nModerate\n\n\nSystematic curve\nNon-normality\nCheck robustness\n\n\n\nKey insight: With large samples (n &gt; 30), mixed models are robust to moderate non-normality.\n\n\n3.4 Residual vs Fitted Plot\nWhat to look for:\n\nGood: Random scatter around zero\nBad: Fan/funnel shape (heteroscedasticity)\nBad: Curve pattern (non-linearity)"
  },
  {
    "objectID": "statistical_guide.html#interpreting-the-numbers",
    "href": "statistical_guide.html#interpreting-the-numbers",
    "title": "Statistical Methods Guide",
    "section": "4 Interpreting the Numbers",
    "text": "4 Interpreting the Numbers\n\n4.1 Effect Sizes\nEffect sizes tell you how big an effect is, not just whether it exists.\n\n4.1.1 Cohen’s d\nFormula: d = (Mean₁ - Mean₂) / SD\nInterpretation guide:\n\n\n\nCohen’s d\nInterpretation\nPractical example\n\n\n\n\n0.2\nSmall\nBarely noticeable\n\n\n0.5\nMedium\nNoticeable with attention\n\n\n0.8\nLarge\nObvious difference\n\n\n1.0+\nVery large\nHard to miss\n\n\n\n\n\n\n\n\n\nContext Matters\n\n\n\nThese benchmarks are guidelines, not rules. A “small” effect might be huge in some contexts (e.g., medication side effects) and trivial in others.\n\n\n\n\n4.1.2 R² (R-squared)\nFor mixed models, we report two R² values:\n\n\n\n\n\n\n\n\nType\nWhat it measures\nIn our study\n\n\n\n\nR² marginal\nVariance explained by fixed effects alone\nHow much does RIR explain?\n\n\nR² conditional\nVariance explained by fixed + random effects\nHow much do RIR + individual differences explain?\n\n\n\nThe gap between marginal and conditional R² shows how much individual differences matter. A large gap (&gt;20%) suggests individual calibration is valuable.\n\n\n\n4.2 Model Comparison\n\n4.2.1 AIC and BIC\nLower is better for both metrics.\n\n\n\nMetric\nPenalizes\nUse when\n\n\n\n\nAIC\nModel complexity less\nPrediction is goal\n\n\nBIC\nModel complexity more\nExplanation is goal\n\n\n\nRule of thumb: Differences &lt; 2 are negligible; differences &gt; 10 are decisive.\n\n\n4.2.2 Bayes Factors\nBayes factors answer: “How many times more likely is the data under Model A than Model B?”\n\n\n\nBayes Factor\nInterpretation\n\n\n\n\n&gt; 100\nDecisive evidence\n\n\n30-100\nVery strong evidence\n\n\n10-30\nStrong evidence\n\n\n3-10\nModerate evidence\n\n\n1-3\nWeak evidence\n\n\n~1\nNo evidence either way\n\n\n&lt; 1\nEvidence for other model\n\n\n\n\n\n\n\n\n\nBIC Approximation\n\n\n\nWe approximate Bayes factors using BIC:\n\\[BF \\approx e^{(\\Delta BIC / 2)}\\]\nThis works well for comparing nested models with reasonable sample sizes.\n\n\n\n\n\n4.3 Confidence vs Prediction Intervals\nThis distinction is crucial for practical application:\n\n\n\n\n\n\n\n\n\nInterval\nQuestion answered\nWidth\nUse for\n\n\n\n\nConfidence Interval\nWhere is the true mean?\nNarrower\nEstimating population parameters\n\n\nPrediction Interval\nWhere will a new observation fall?\nWider\nPredicting individual performance\n\n\n\nExample: - 95% CI for velocity at RIR 3: 0.42-0.44 m/s (where’s the average?) - 95% PI for velocity at RIR 3: 0.32-0.54 m/s (what might THIS rep look like?)\nFor practical training decisions, prediction intervals are more relevant."
  },
  {
    "objectID": "statistical_guide.html#model-validation",
    "href": "statistical_guide.html#model-validation",
    "title": "Statistical Methods Guide",
    "section": "5 Model Validation",
    "text": "5 Model Validation\n\n5.1 Cross-Validation\nThe idea: Test how well the model predicts data it hasn’t seen.\n\n5.1.1 Leave-One-Participant-Out CV\n\nRemove one athlete’s data\nFit model on remaining athletes\nPredict removed athlete’s velocities\nCalculate prediction error\nRepeat for all athletes\n\nWhy it matters: AIC/BIC tell you about model fit; CV tells you about prediction accuracy.\n\n\n5.1.2 K-Fold CV\n\nDivide athletes into K groups\nTrain on K-1 groups, test on held-out group\nRotate and repeat\nAverage prediction errors\n\n\n\n\n5.2 Influence Diagnostics\nGoal: Identify observations or athletes that unduly influence results.\n\n5.2.1 Cook’s Distance\nWhat it measures: How much would the results change if we removed this point?\nThreshold: Cook’s D &gt; 4/n suggests influential points\nWhat to do with influential points:\n\nCheck for data entry errors\nInvestigate if they’re genuine outliers\nRun sensitivity analysis with/without them\nReport both if results differ\n\n\n\n5.2.2 Participant-Level Influence\nFor clustered data, also check if any individual athlete is driving your results. If removing one athlete dramatically changes conclusions, investigate why.\n\n\n\n5.3 Calibration\nA well-calibrated model: predictions match reality.\n\n5.3.1 Calibration Plot\n\nX-axis: Predicted values\nY-axis: Observed values\nIdeal: Points fall on 45° line (predicted = observed)\n\nMetrics:\n\nCalibration slope = 1 is ideal\nCalibration intercept = 0 is ideal\n\n\n\n\n5.4 The “Pit Stop” Philosophy\nThink of robustness checks like pit stops in a race—they’re not emergency repairs, but planned checkpoints.\n\n\n\n\n\n\n\n\nPit Stop\nWhat it checks\nIf standard & robust agree…\n\n\n\n\nCluster-robust SEs\nHeteroscedasticity\nUnequal variance isn’t a problem\n\n\nBootstrap CIs\nDistributional assumptions\nNon-normality isn’t a problem\n\n\nSensitivity analysis\nModel specification\nSpecific choices don’t matter\n\n\n\nReporting: “Standard and robust methods agree, confirming that conclusions don’t depend on specific technical assumptions.”"
  },
  {
    "objectID": "statistical_guide.html#practical-application",
    "href": "statistical_guide.html#practical-application",
    "title": "Statistical Methods Guide",
    "section": "6 Practical Application",
    "text": "6 Practical Application\n\n6.1 Using Velocity Tables\nVelocity stop tables tell you when to end a set based on bar speed.\nHow to use them:\n\nFind your target RIR in the left column\nLook up the corresponding velocity\nStop the set when bar speed drops to that velocity\n\nImportant caveats:\n\nTables are population averages—your individual relationship may differ\nDay-to-day variability means some error is inevitable\nUse as a guide alongside RPE, not as the sole decision tool\n\n\n\n6.2 Individual vs Population Models\n\n\n\nApproach\nAccuracy\nWhen to use\n\n\n\n\nPopulation model\nLower (~2x error)\nQuick estimates, new athletes\n\n\nIndividual model\nHigher\nTrained athletes with historical data\n\n\n\nRecommendation for serious athletes: Collect ~20 observations across different loads and RIR values, then build an individual model.\n\n\n6.3 Calibration Frequency\nBased on day-to-day reliability (ICC ~0.5-0.7):\n\n\n\nAthlete type\nRecalibration frequency\n\n\n\n\nRecreational\nMonthly or after breaks\n\n\nCompetitive\nEvery 2-4 weeks\n\n\nElite\nWeekly during peaking phases\n\n\n\n\n\n6.4 Common Questions (FAQ)\nQ: How accurate is VBT for deadlifts?\nA: Population-level prediction error is ~1.3 reps (MAE). Individual calibration can reduce this to ~0.7 reps.\nQ: Should I use individual calibration?\nA: If you’re a serious athlete tracking velocity regularly, yes. For casual use, population averages are fine.\nQ: Why is deadlift more variable than squat/bench?\nA: The deadlift’s unique start position (no eccentric, starting from rest) and technical variations (grip position, back angle) introduce more variability.\nQ: How many reps do I need for calibration?\nA: Minimum 15-20 observations across 3+ RIR levels. More is better, especially for building individual models.\nQ: Can I use these tables for sumo deadlift?\nA: With caution. Our data is for conventional deadlift. Sumo may have different velocity profiles."
  },
  {
    "objectID": "statistical_guide.html#summary-key-takeaways",
    "href": "statistical_guide.html#summary-key-takeaways",
    "title": "Statistical Methods Guide",
    "section": "7 Summary: Key Takeaways",
    "text": "7 Summary: Key Takeaways\n\n\n\n\n\n\nThe Bottom Line\n\n\n\n\nMixed models properly handle repeated measures—don’t use regular regression\nIndividual calibration is twice as accurate as population averages\nRecalibrate regularly (every 2-4 weeks) due to day-to-day variability\nUse velocity as one tool among many (RPE, load, technique)\nCheck robustness—if different methods agree, you can trust the results"
  },
  {
    "objectID": "statistical_guide.html#references",
    "href": "statistical_guide.html#references",
    "title": "Statistical Methods Guide",
    "section": "8 References",
    "text": "8 References\nFor those wanting to dive deeper:\n\nNakagawa & Schielzeth (2013): R² for mixed models\nKoo & Li (2016): ICC interpretation guidelines\nJeffreys (1961): Bayes factor interpretation scale\nZuur et al. (2009): Mixed effects models in R\nGelman & Hill (2007): Data analysis using regression and multilevel models"
  }
]