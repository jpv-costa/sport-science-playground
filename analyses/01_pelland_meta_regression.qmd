---
title: "Does Training to Failure Matter?"
subtitle: "Replicating Pelland et al. (2024) Meta-Regression on RIR and Training Outcomes"
author: "Deadlift Study Project"
date: today
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    code-summary: "Show code"
    theme: cosmo
    fig-width: 8
    fig-height: 6
execute:
  warning: false
  message: false
bibliography: references.bib
---

```{r setup}
#| include: false
library(metafor)
library(ggplot2)

# Load our replication results
results <- readRDS("../data/processed/pelland_replication_results.rds")
```

## The Question Every Lifter Asks

**Should I push every set to complete failure, or stop a few reps short?**

This question has practical implications:

- Training to failure is **harder** and requires more recovery
- If stopping short gives the same results, you could train more frequently
- But if failure is necessary for gains, stopping short wastes your time

Pelland and colleagues tackled this question by analyzing **all available research** on the topic.

## What is "Proximity to Failure"?

Researchers use **Repetitions in Reserve (RIR)** to measure how close you are to failure:

| RIR | Meaning |
|-----|---------|
| 0 | Complete failure - couldn't do another rep |
| 1 | Stopped with 1 rep "left in the tank" |
| 2 | Stopped with 2 reps remaining |
| 3+ | Stopped well short of failure |

The key question: **Does RIR predict training outcomes?**

## The Study Design

### Meta-Regression: Analyzing All the Evidence

Rather than running a single study, the authors performed a **meta-regression** - a statistical technique that:

1. Gathers effect sizes from many individual studies
2. Weights each study by its precision (larger studies count more)
3. Models how a predictor (RIR) relates to outcomes across all studies
4. Accounts for non-independence (multiple effects from the same study)

### The Data

```{r data-summary}
#| label: tbl-data-summary
#| tbl-cap: "Summary of included effect sizes"

data.frame(
  Outcome = c("Strength", "Hypertrophy"),
  `Effect Sizes` = c(nrow(results$data_strength), nrow(results$data_hypertrophy)),
  `Imputed Correlation` = c(
    round(results$correlation_strength, 3),
    round(results$correlation_hypertrophy, 3)
  ),
  check.names = FALSE
) |> knitr::kable()
```

The analysis included **`r nrow(results$data_strength)` strength effect sizes** and **`r nrow(results$data_hypertrophy)` hypertrophy effect sizes**.

## The Key Finding: Strength vs Hypertrophy Differ

### Strength Gains

```{r strength-results}
str_mod <- results$strength_model
str_rir_b <- str_mod$b["avg.rir", 1]
str_rir_p <- str_mod$pval[rownames(str_mod$b) == "avg.rir"]
```

For **strength gains**, RIR showed:

- **Coefficient**: `r round(str_rir_b, 4)` (effect per 1 RIR increase)
- **P-value**: `r round(str_rir_p, 3)`
- **Interpretation**: Not statistically significant

::: {.callout-note}
## What This Means for Strength Training
Training closer to failure does **not** produce significantly greater strength gains. You can stop 2-3 reps short and still get similar strength improvements.
:::

### Muscle Hypertrophy

```{r hypertrophy-results}
hyp_mod <- results$hypertrophy_model
hyp_rir_b <- hyp_mod$b["avg.rir", 1]
hyp_rir_p <- hyp_mod$pval[rownames(hyp_mod$b) == "avg.rir"]
```

For **muscle growth**, RIR showed:

- **Coefficient**: `r round(hyp_rir_b, 4)` (effect per 1 RIR increase)
- **P-value**: `r round(hyp_rir_p, 4)`
- **Interpretation**: Statistically significant and negative

::: {.callout-important}
## What This Means for Muscle Building
Training closer to failure **does** produce greater muscle hypertrophy. Each RIR reduction (getting closer to failure) is associated with slightly more muscle growth.
:::

## Visualizing the Difference

```{r fig-comparison}
#| label: fig-comparison
#| fig-cap: "RIR Effect on Strength vs Hypertrophy Outcomes"

# Create comparison data
comparison_df <- data.frame(
  Outcome = c("Strength", "Hypertrophy"),
  Estimate = c(str_rir_b, hyp_rir_b),
  SE = c(str_mod$se[rownames(str_mod$b) == "avg.rir"],
         hyp_mod$se[rownames(hyp_mod$b) == "avg.rir"]),
  Significant = c("No", "Yes")
)

comparison_df$Lower <- comparison_df$Estimate - 1.96 * comparison_df$SE
comparison_df$Upper <- comparison_df$Estimate + 1.96 * comparison_df$SE

ggplot(comparison_df, aes(x = Outcome, y = Estimate, color = Significant)) +
  geom_point(size = 4) +
  geom_errorbar(aes(ymin = Lower, ymax = Upper), width = 0.2, linewidth = 1) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
  scale_color_manual(values = c("No" = "gray50", "Yes" = "#E41A1C")) +
  labs(
    x = NULL,
    y = "RIR Coefficient (Hedge's g per RIR)",
    title = "Effect of RIR on Training Outcomes",
    subtitle = "Negative values = closer to failure is better"
  ) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "bottom")
```

The figure shows:

- **Strength** (gray): Confidence interval includes zero - no significant effect
- **Hypertrophy** (red): Entirely below zero - significant negative effect

## Replication Validation

Our replication matches the published findings:

```{r validation}
#| label: tbl-validation
#| tbl-cap: "Replication vs Published Results"

data.frame(
  Outcome = c("Strength", "Hypertrophy"),
  `Paper Claim` = c(
    "CI contains null (not significant)",
    "Negative slope, CI excludes null"
  ),
  `Our Result` = c(
    paste0("p = ", round(str_rir_p, 3), " (not significant)"),
    paste0("b = ", round(hyp_rir_b, 3), ", p = ", round(hyp_rir_p, 3))
  ),
  Match = c("Yes", "Yes"),
  check.names = FALSE
) |> knitr::kable()
```

## Practical Implications

### For Strength Training

1. **Stop 2-3 reps short** of failure on most sets
2. Reserve failure training for testing or peaking phases
3. This allows higher frequency and volume

### For Muscle Building

1. **Train closer to failure** for optimal hypertrophy
2. 0-2 RIR appears beneficial
3. Balance this against recovery demands

### The Trade-Off

```{r fig-tradeoff}
#| label: fig-tradeoff
#| fig-cap: "The Strength-Hypertrophy Trade-off with RIR"

# Conceptual illustration
rir_seq <- 0:5
concept_df <- data.frame(
  RIR = rep(rir_seq, 2),
  Outcome = rep(c("Strength Gain", "Muscle Growth"), each = length(rir_seq)),
  Benefit = c(
    rep(1, length(rir_seq)),  # Strength: flat line (no effect)
    1 - 0.15 * rir_seq        # Hypertrophy: decreasing line
  )
)

ggplot(concept_df, aes(x = RIR, y = Benefit, color = Outcome)) +
  geom_line(linewidth = 1.5) +
  geom_point(size = 3) +
  scale_x_reverse() +
  scale_color_manual(values = c("Strength Gain" = "#377EB8", "Muscle Growth" = "#E41A1C")) +
  labs(
    x = "Repetitions in Reserve (RIR)",
    y = "Relative Benefit",
    title = "Conceptual Model: RIR Effects by Outcome",
    subtitle = "Lower RIR = closer to failure"
  ) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "bottom")
```

## Methods Summary

### Statistical Model

We used a multi-level meta-regression with:

- **Fixed effects**: RIR, load, volume equating, duration, training status
- **Random effects**: Nested structure (~1|study/group/obs)
- **Effect sizes**: Hedge's g (bias-corrected standardized mean change)
- **Estimation**: REML with t-distribution tests

### Pre-Post Correlation Imputation

Missing pre-post correlations were imputed via meta-analysis:

- Strength: r = `r round(results$correlation_strength, 3)`
- Hypertrophy: r = `r round(results$correlation_hypertrophy, 3)`

## References

::: {#refs}
:::

## Source

- Original paper: [Pelland et al. (2024) Sports Medicine](https://link.springer.com/article/10.1007/s40279-024-02069-2)
- Data and code: [OSF Repository](https://osf.io/7knsj/)
