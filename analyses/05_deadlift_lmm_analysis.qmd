---
title: "Velocity Stop Tables for Deadlift Training"
subtitle: "Linear Mixed Effects Modeling with Conformal Prediction Intervals"
author:
  - name: "João Costa"
    affiliation: "Sport Science Research"
  - name: "Filipe Braga"
    affiliation: "MSc Thesis Research"
date: today
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    code-summary: "Show code"
    theme: cosmo
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.width = 10,
  fig.height = 6
)

# Load required packages
library(ggplot2)
library(knitr)

# Load results
results <- readRDS("../data/processed/deadlift_lmm_results.rds")
data <- results$data

# For comparison with Study 4
study4_results <- tryCatch({
  readRDS("../data/processed/deadlift_rir_velocity_results.rds")
}, error = function(e) NULL)
```

## Executive Summary

This analysis extends Study 4 (Deadlift RIR-Velocity Relationship) using **Linear Mixed Effects Models (LMM)** to address three practical questions for velocity-based training prescription:

1. **Should we use load-specific tables?** - Does the velocity-RIR relationship differ meaningfully between 80% and 90% 1RM?
2. **What are the velocity stop thresholds?** - Practical tables for autoregulation in training
3. **How confident should we be?** - Distribution-free prediction intervals using conformal prediction

**Key Finding**: `r if(results$load_importance_result$recommendation == "global") "Load percentage does NOT significantly affect the velocity-RIR relationship. A single global velocity table is recommended---this simplifies training prescription considerably." else "Load percentage significantly affects velocity-RIR. Load-specific tables are recommended."`

### Why This Study Matters

While Study 4 established that the velocity-RIR relationship exists for deadlifts, coaches and athletes need **actionable tools**---specifically, velocity thresholds they can use to autoregulate training intensity. This study provides:

- **Practical velocity stop tables** calibrated to this population
- **Uncertainty quantification** so athletes know the precision of these estimates
- **Model comparison** to determine if complexity (load-specific tables) is warranted

## Understanding the Data Structure

Before diving into the analysis, it's important to understand why we have `r nrow(results$data)` observations from only 19 participants:

```{r data-structure}
#| label: tbl-data-structure
#| tbl-cap: "Data Structure Summary"

obs_per_participant <- table(data$id)
obs_summary <- data.frame(
  Metric = c(
    "Total Observations",
    "Unique Participants",
    "Mean Obs per Participant",
    "Min Obs per Participant",
    "Max Obs per Participant"
  ),
  Value = c(
    nrow(data),
    length(unique(data$id)),
    round(mean(obs_per_participant), 1),
    min(obs_per_participant),
    max(obs_per_participant)
  )
)

kable(obs_summary)
```

**Why so many observations?** Each participant performs multiple sets to failure, with each repetition within a set being recorded. For example:

- Participant performs 8 reps to failure at 80% 1RM
- This generates 8 observations (RIR 7, 6, 5, 4, 3, 2, 1, 0)
- Same participant does this on Day 1 and Day 2, at both 80% and 90%

This is why the Q-Q plots and residual plots show ~400 points, not 19.

## Why Linear Mixed Effects Models?

### The Problem with Standard Regression

Our data has a **nested (hierarchical) structure**:

- 19 participants
- Each tested on 2 days
- At 2 load levels (80%, 90%)
- Multiple repetitions per condition (~21 observations per participant)

Standard OLS regression assumes independence between observations. In our data, observations from the same participant are correlated---if Participant A has generally fast velocities, all their observations tend to be faster. Ignoring this structure leads to:

1. **Underestimated standard errors** (too confident in our estimates)
2. **Inflated Type I error rates** (false positives)
3. **Biased coefficient estimates** in some cases

### The LMM Solution

Linear Mixed Effects Models (Laird & Ware, 1982) properly account for nested data by including:

- **Fixed effects**: Population-average relationships (e.g., the overall velocity-RIR slope)
- **Random effects**: Individual deviations from the population average

For our analysis, the key random effects are:

- **Random intercepts**: Each participant has their own baseline velocity level
- **Random slopes**: Each participant has their own velocity-RIR relationship slope

This approach acknowledges that some athletes naturally move faster than others (random intercepts) and that the relationship between velocity and fatigue varies between individuals (random slopes).

## Model Building Strategy

We fit progressively complex models and compared them using:

- **AIC** (Akaike Information Criterion) - penalizes complexity less
- **BIC** (Bayesian Information Criterion) - penalizes complexity more
- **Bayes Factor** approximation via BIC difference
- **Likelihood Ratio Tests** for nested models

### Models Compared

```{r model-comparison-table}
#| label: tbl-models
#| tbl-cap: "Model Comparison"

comparison_df <- as.data.frame(results$model_comparison$comparison_table)

# Format for display
comparison_df$AIC <- round(comparison_df$AIC, 1)
comparison_df$BIC <- round(comparison_df$BIC, 1)
comparison_df$delta_AIC <- round(comparison_df$delta_AIC, 1)
comparison_df$delta_BIC <- round(comparison_df$delta_BIC, 1)
comparison_df$bayes_factor_approx <- round(comparison_df$bayes_factor_approx, 3)
comparison_df$R2_conditional <- round(comparison_df$R2_conditional, 3)

kable(comparison_df[, c("model", "AIC", "BIC", "delta_BIC", "bayes_factor_approx", "R2_conditional")],
      col.names = c("Model", "AIC", "BIC", "ΔBIC", "BF (approx)", "R² (cond)"))
```

**Best Model**: `r results$model_comparison$best_model_name` (lowest BIC)

### Bayes Factor Interpretation

| Bayes Factor | Evidence |
|--------------|----------|
| > 100 | Decisive for simpler model |
| 30-100 | Very strong for simpler |
| 10-30 | Strong for simpler |
| 3-10 | Moderate for simpler |
| 1-3 | Weak evidence |
| 1/3-1 | Weak for complex |
| < 1/3 | Evidence for complex |

## Does Load Percentage Matter?

This is perhaps the most practically important question for coaches: **Can we use ONE universal velocity table regardless of whether the athlete is lifting 80% or 90% of 1RM?**

### Why This Question Matters

If load percentage significantly affects the velocity-RIR relationship, coaches need separate lookup tables for each load---doubling the complexity of implementation. If not, a single table works across loads, dramatically simplifying training prescription.

From a physiological perspective, there are reasons to expect both outcomes:

- **Expecting a difference**: Higher loads recruit more motor units from the first rep, potentially affecting the fatigue trajectory
- **Expecting no difference**: The velocity-RIR relationship may reflect a fundamental motor control pattern independent of absolute load

```{r load-test}
#| label: tbl-load-test
#| tbl-cap: "Load Percentage Importance Test"

load_test <- results$load_importance_test

test_df <- data.frame(
  Metric = c(
    "Likelihood Ratio Chi-squared",
    "Degrees of Freedom",
    "P-value",
    "Delta AIC",
    "Delta BIC",
    "Bayes Factor (simpler vs complex)",
    "Interpretation"
  ),
  Value = c(
    round(load_test$lrt_chisq, 3),
    load_test$lrt_df,
    sprintf("%.4f", load_test$lrt_p_value),
    round(load_test$delta_aic, 2),
    round(load_test$delta_bic, 2),
    round(load_test$bayes_factor, 3),
    load_test$bf_interpretation
  )
)

kable(test_df, col.names = c("", ""))
```

**Recommendation**: `r results$load_importance_result$recommendation` velocity table

```{r load-decision}
if (results$load_importance_result$recommendation == "global") {
  cat("**Conclusion**: Load percentage does NOT significantly affect the velocity-RIR relationship.\n\n")
  cat("This means coaches and athletes can use a SINGLE velocity table regardless of\n")
  cat("whether they're lifting at 80% or 90% 1RM. This simplifies training prescription.\n")
} else {
  cat("**Conclusion**: Load percentage DOES significantly affect the velocity-RIR relationship.\n\n")
  cat("Separate velocity targets should be used for different load percentages.\n")
}
```

## Model Diagnostics

Before trusting our model, we verify assumptions:

### Residual Normality

```{r qq-plot}
#| label: fig-qq
#| fig-cap: "Q-Q Plot of Residuals"

# Get residuals from diagnostics
residuals <- results$diagnostics_full$residuals
fitted <- results$diagnostics_full$fitted

# Q-Q plot data
n <- length(residuals)
theoretical <- qnorm(ppoints(n))
sample <- sort(residuals)

qq_df <- data.frame(theoretical = theoretical, sample = sample)

ggplot(qq_df, aes(x = theoretical, y = sample)) +
  geom_point(alpha = 0.5, color = "#2E86AB") +
  geom_abline(slope = sd(residuals), intercept = mean(residuals),
              color = "#E63946", linewidth = 1) +
  labs(
    x = "Theoretical Quantiles",
    y = "Sample Quantiles",
    title = "Q-Q Plot of Model Residuals"
  ) +
  theme_minimal()
```

**Normality Test**: `r results$diagnostics$normality_test$interpretation`

### Homoscedasticity

```{r residual-plot}
#| label: fig-residuals
#| fig-cap: "Residuals vs Fitted Values"

resid_df <- data.frame(fitted = fitted, residuals = residuals)

ggplot(resid_df, aes(x = fitted, y = residuals)) +
  geom_point(alpha = 0.4, color = "#2E86AB") +
  geom_hline(yintercept = 0, color = "#E63946", linewidth = 1, linetype = "dashed") +
  geom_smooth(method = "loess", se = FALSE, color = "#F77F00", linewidth = 0.8) +
  labs(
    x = "Fitted Values (m/s)",
    y = "Residuals (m/s)",
    title = "Residuals vs Fitted Values"
  ) +
  theme_minimal()
```

**Homoscedasticity**: `r results$diagnostics$homoscedasticity_test$interpretation`

## Fixed Effects

```{r fixed-effects}
#| label: tbl-fixed-effects
#| tbl-cap: "Fixed Effects Estimates"

fe <- results$fixed_effects
fe$estimate <- round(fe$estimate, 4)
fe$std_error <- round(fe$std_error, 4)
fe$t_value <- round(fe$t_value, 2)
fe$p_value <- ifelse(fe$p_value < 0.001, "<0.001", sprintf("%.4f", fe$p_value))

kable(fe[, c("term", "estimate", "std_error", "t_value", "p_value")],
      col.names = c("Term", "Estimate", "SE", "t", "p-value"))
```

**Interpretation**: The RIR coefficient shows that for each additional repetition in reserve, velocity increases by approximately `r round(results$fixed_effects$estimate[results$fixed_effects$term == "rir"], 3)` m/s.

## Velocity Stop Table

This is the practical output for coaches and athletes:

```{r velocity-table}
#| label: tbl-velocity-stop
#| tbl-cap: "Velocity Stop Table: Target velocities for each RIR level"

vt <- results$velocity_table$table

if ("load_percentage" %in% names(vt)) {
  # Load-specific table
  vt$velocity <- round(vt$velocity, 3)
  vt$lower_95 <- round(vt$lower_95, 3)
  vt$upper_95 <- round(vt$upper_95, 3)

  kable(vt[, c("rir", "load_percentage", "velocity", "lower_95", "upper_95")],
        col.names = c("RIR", "Load", "Velocity (m/s)", "Lower 95%", "Upper 95%"))
} else {
  # Global table
  vt$velocity <- round(vt$velocity, 3)
  vt$lower_95 <- round(vt$lower_95, 3)
  vt$upper_95 <- round(vt$upper_95, 3)

  kable(vt[, c("rir", "velocity", "lower_95", "upper_95")],
        col.names = c("RIR", "Velocity (m/s)", "Lower 95%", "Upper 95%"))
}
```

### How to Use This Table

1. **During training**: Monitor bar velocity in real-time
2. **Stop the set** when velocity drops to your target RIR threshold
3. **Example**: To leave 2 reps in reserve, stop when velocity reaches ~`r round(results$velocity_table$table$velocity[results$velocity_table$table$rir == 2][1], 2)` m/s

### Visualization

```{r velocity-viz}
#| label: fig-velocity-table
#| fig-cap: "Velocity Stop Thresholds by RIR"

vt <- results$velocity_table$table

if ("load_percentage" %in% names(vt)) {
  ggplot(vt, aes(x = rir, y = velocity, color = load_percentage)) +
    geom_line(linewidth = 1.2) +
    geom_point(size = 3) +
    geom_ribbon(aes(ymin = lower_95, ymax = upper_95, fill = load_percentage),
                alpha = 0.2, color = NA) +
    scale_color_manual(values = c("80%" = "#E69F00", "90%" = "#009E73")) +
    scale_fill_manual(values = c("80%" = "#E69F00", "90%" = "#009E73")) +
    labs(
      x = "Repetitions in Reserve (RIR)",
      y = "Mean Velocity (m/s)",
      color = "Load",
      fill = "Load"
    ) +
    theme_minimal() +
    theme(legend.position = "bottom")
} else {
  ggplot(vt, aes(x = rir, y = velocity)) +
    geom_line(linewidth = 1.2, color = "#2E86AB") +
    geom_point(size = 3, color = "#2E86AB") +
    geom_ribbon(aes(ymin = lower_95, ymax = upper_95),
                alpha = 0.2, fill = "#2E86AB") +
    labs(
      x = "Repetitions in Reserve (RIR)",
      y = "Mean Velocity (m/s)",
      title = "Global Velocity Stop Table for Deadlift"
    ) +
    theme_minimal()
}
```

## General vs Individual Tables

Should athletes use the **general table** or create their own **individual calibration**?

```{r individual-comparison}
#| label: tbl-individual-comparison
#| tbl-cap: "General vs Individual Table Accuracy"

ic <- results$individual_comparison

comparison_df <- data.frame(
  Approach = c("General (Population)", "Individual (Calibrated)"),
  MAE = c(round(ic$global_mae * 1000, 2), round(ic$individual_mae * 1000, 2)),
  RMSE = c(round(ic$global_rmse * 1000, 2), round(ic$individual_rmse * 1000, 2))
)

kable(comparison_df,
      col.names = c("Approach", "MAE (mm/s)", "RMSE (mm/s)"))
```

**Improvement from individualization**: `r round(results$individual_comparison$mae_improvement_pct, 1)`%

**Recommendation**: `r results$individual_comparison$recommendation`

## Conformal Prediction Intervals

### The Problem with Parametric Intervals

Traditional prediction intervals assume the residuals are normally distributed and homoscedastic (constant variance). As our diagnostic plots show, these assumptions may be violated. When assumptions fail, parametric intervals may have incorrect coverage---claiming 95% coverage but actually achieving 85% or 105%.

### The Conformal Prediction Solution

**Conformal prediction** (Vovk et al., 2005; Lei et al., 2018) provides **distribution-free** intervals with guaranteed finite-sample coverage. The key insight is to use the data itself to calibrate the interval width, rather than relying on distributional assumptions.

### Method

We use **split conformal prediction**:

1. **Split data**: Day 1 (calibration), Day 2 (test)
2. **Fit model** on calibration set
3. **Calculate nonconformity scores**: Absolute residuals on calibration set
4. **Find threshold**: The `r sprintf("%.0f", (1 - 0.05) * 100)`th percentile of nonconformity scores
5. **Construct intervals**: For new predictions, add/subtract the threshold

This procedure guarantees that if calibration and test data are exchangeable, coverage will be at least 95% in expectation.

### Results

```{r conformal-results}
#| label: tbl-conformal
#| tbl-cap: "Conformal vs Parametric Prediction Intervals"

conf <- results$conformal

comparison_df <- data.frame(
  Metric = c("Target Coverage", "Empirical Coverage", "Average Interval Width"),
  Parametric = c(
    "95%",
    sprintf("%.1f%%", conf$comparison$parametric_coverage * 100),
    sprintf("%.4f m/s", conf$comparison$parametric_width)
  ),
  Conformal = c(
    "95%",
    sprintf("%.1f%%", conf$comparison$conformal_coverage * 100),
    sprintf("%.4f m/s", conf$comparison$conformal_width)
  )
)

kable(comparison_df, col.names = c("Metric", "Parametric", "Conformal"))
```

**Key Insight**:

- Parametric coverage: `r sprintf("%.1f%%", results$conformal$comparison$parametric_coverage * 100)` (deviation from 95%: `r sprintf("%.1f%%", abs(results$conformal$comparison$parametric_coverage - 0.95) * 100)`)
- Conformal coverage: `r sprintf("%.1f%%", results$conformal$comparison$conformal_coverage * 100)` (deviation from 95%: `r sprintf("%.1f%%", abs(results$conformal$comparison$conformal_coverage - 0.95) * 100)`)

```{r conformal-recommendation}
if (abs(results$conformal$comparison$conformal_coverage - 0.95) <
    abs(results$conformal$comparison$parametric_coverage - 0.95)) {
  cat("Conformal intervals achieve coverage closer to the 95% target.\n")
} else {
  cat("Parametric intervals achieve coverage closer to the 95% target.\n")
}
```

### Conformal Prediction Interval Details

```{r conformal-details}
#| label: tbl-conformal-details
#| tbl-cap: "Conformal Prediction Interval Calibration Details"

# Create detailed breakdown table with available fields
detail_df <- data.frame(
  Component = c(
    "Nonconformity Score Threshold (95th percentile)",
    "Interval Half-Width",
    "Total Interval Width",
    "Target Coverage",
    "Achieved Conformal Coverage",
    "Achieved Parametric Coverage"
  ),
  Value = c(
    sprintf("%.4f m/s", conf$q_hat),
    sprintf("± %.4f m/s", conf$q_hat),
    sprintf("%.4f m/s", conf$comparison$conformal_width),
    sprintf("%.0f%%", conf$comparison$target_coverage * 100),
    sprintf("%.1f%% (error: %.2f%%)", conf$comparison$conformal_coverage * 100,
            conf$comparison$conformal_coverage_error * 100),
    sprintf("%.1f%% (error: %.2f%%)", conf$comparison$parametric_coverage * 100,
            conf$comparison$parametric_coverage_error * 100)
  )
)

kable(detail_df, col.names = c("Component", "Value"))
```

### Visualization: Prediction Intervals by RIR

```{r conformal-viz}
#| label: fig-conformal-intervals
#| fig-cap: "Conformal vs Parametric Prediction Intervals Across RIR Levels"

# Get predictions at each RIR level with both interval types
vt <- results$velocity_table$table
vt$conformal_lower <- vt$velocity - conf$q_hat
vt$conformal_upper <- vt$velocity + conf$q_hat

# Create comparison plot
library(ggplot2)

# Reshape for plotting
interval_comparison <- data.frame(
  rir = rep(vt$rir, 2),
  velocity = rep(vt$velocity, 2),
  lower = c(vt$lower_95, vt$conformal_lower),
  upper = c(vt$upper_95, vt$conformal_upper),
  method = rep(c("Parametric (95% CI)", "Conformal (95% PI)"), each = nrow(vt))
)

ggplot(interval_comparison, aes(x = rir)) +
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = method), alpha = 0.3) +
  geom_line(aes(y = velocity), color = "black", linewidth = 1) +
  geom_point(aes(y = velocity), color = "black", size = 3) +
  facet_wrap(~method) +
  labs(
    x = "Repetitions in Reserve (RIR)",
    y = "Mean Velocity (m/s)",
    title = "Comparison of Prediction Interval Methods",
    subtitle = "Wider intervals indicate more uncertainty"
  ) +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_fill_manual(values = c("#2E86AB", "#E63946"))
```

### Velocity Stop Table with Conformal Intervals

This table provides **conservative velocity targets** using conformal prediction intervals, which have guaranteed 95% coverage regardless of distributional assumptions.

```{r conformal-table}
#| label: tbl-conformal-stops
#| tbl-cap: "Velocity Stop Thresholds with Conformal Prediction Intervals"

# Create practical table with conformal intervals
conformal_table <- data.frame(
  RIR = vt$rir,
  `Target Velocity` = round(vt$velocity, 3),
  `Lower 95% (Conservative)` = round(vt$velocity - conf$q_hat, 3),
  `Upper 95%` = round(vt$velocity + conf$q_hat, 3),
  `Interval Width` = round(2 * conf$q_hat, 3)
)

kable(conformal_table,
      col.names = c("Target RIR", "Mean Velocity (m/s)", "Lower Bound", "Upper Bound", "Width (m/s)"))
```

**How to Use This Table:**

1. **Conservative approach (recommended)**: Use the **Lower 95%** column. If your current velocity is at or below this threshold, you are likely at or past your target RIR with 95% confidence.

2. **Aggressive approach**: Use the **Mean Velocity** column. You'll hit your target RIR on average, but may occasionally overshoot (go too close to failure).

3. **Example**: To stop at RIR 2:
   - **Conservative**: Stop when velocity drops to ~`r round(vt$velocity[vt$rir == 2] - conf$q_hat, 2)` m/s
   - **Mean target**: Stop when velocity drops to ~`r round(vt$velocity[vt$rir == 2], 2)` m/s

### Coverage Comparison Visualization

```{r coverage-plot}
#| label: fig-coverage-comparison
#| fig-cap: "Coverage Comparison: Conformal vs Parametric Methods"

# Create comparison bar chart of coverage
coverage_df <- data.frame(
  Method = c("Conformal", "Parametric", "Target"),
  Coverage = c(
    conf$comparison$conformal_coverage * 100,
    conf$comparison$parametric_coverage * 100,
    95
  ),
  Type = c("Achieved", "Achieved", "Target")
)

ggplot(coverage_df, aes(x = Method, y = Coverage, fill = Type)) +
  geom_col(width = 0.6) +
  geom_hline(yintercept = 95, linetype = "dashed", color = "#E63946", linewidth = 1) +
  geom_text(aes(label = sprintf("%.1f%%", Coverage)), vjust = -0.5, size = 4) +
  scale_fill_manual(values = c("Achieved" = "#2E86AB", "Target" = "#009E73")) +
  labs(
    x = "",
    y = "Coverage (%)",
    title = "Prediction Interval Coverage Comparison",
    subtitle = "Conformal prediction achieves coverage closer to the 95% target"
  ) +
  ylim(0, 105) +
  theme_minimal() +
  theme(legend.position = "none")
```

### Practical Implications of Conformal Intervals

The conformal prediction approach has several advantages for velocity-based training:

1. **Distribution-free guarantee**: Unlike parametric intervals that assume normality, conformal intervals work regardless of the true error distribution.

2. **Honest uncertainty**: The interval width (`r round(conf$q_hat * 2, 3)` m/s) reflects the true prediction uncertainty in our sample.

3. **Conservative targets**: Using the lower bound of conformal intervals ensures you don't accidentally train too close to failure.

4. **Practical interpretation**: The `r round(conf$q_hat * 1000, 0)` mm/s half-width means velocity measurements within this range of your target are essentially equivalent---don't over-interpret small velocity differences.

## Robustness Checks

### Why Robustness Checks Matter

Our model diagnostics showed some violations of the standard assumptions (normality and homoscedasticity). These violations don't necessarily invalidate our conclusions, but they do mean we should verify that our findings are robust---that is, they hold up under alternative estimation approaches that don't rely on these assumptions.

We perform three complementary robustness checks:

1. **Cluster-Robust Standard Errors**: Handle heteroscedasticity (non-constant variance)
2. **Bootstrap Confidence Intervals**: Provide assumption-free intervals
3. **Sensitivity Analysis**: Test robustness to model specification

### Cluster-Robust Standard Errors

#### What This Tests

Standard LMM standard errors assume that residual variance is constant across all observations (homoscedasticity). If this assumption is violated (e.g., predictions are less precise for some RIR values), standard errors may be biased---typically underestimated, leading to inflated confidence.

#### The Method

The **CR2 (bias-reduced) sandwich estimator** (Pustejovsky & Tipton, 2018) provides standard errors that are valid regardless of the true variance structure. We compare these robust SEs to the standard Wald SEs.

#### Key Metric: SE Ratio

- **SE Ratio = Robust SE / Wald SE**
- Ratio ≈ 1.0: No heteroscedasticity problem
- Ratio > 1.5: Standard errors are underestimated---use robust SEs
- Ratio < 0.67: Standard errors are overestimated (rare)

```{r robust-se}
#| label: tbl-robust-se
#| tbl-cap: "Cluster-Robust Standard Errors vs. Standard Wald Errors"

if (!is.null(results$robust_se)) {
  robust_df <- results$robust_se
  robust_df$estimate <- round(robust_df$estimate, 4)
  robust_df$se_wald <- round(robust_df$se_wald, 4)
  robust_df$se_robust <- round(robust_df$se_robust, 4)
  robust_df$se_ratio <- round(robust_df$se_ratio, 3)
  robust_df$p_robust <- ifelse(robust_df$p_robust < 0.001, "<0.001",
                               sprintf("%.4f", robust_df$p_robust))

  kable(robust_df[, c("term", "estimate", "se_wald", "se_robust", "se_ratio", "p_robust")],
        col.names = c("Term", "Estimate", "SE (Wald)", "SE (Robust)", "Ratio", "p (Robust)"))
}
```

```{r robust-se-interpretation, results='asis'}
if (!is.null(results$robust_se)) {
  max_ratio <- max(results$robust_se$se_ratio)
  min_ratio <- min(results$robust_se$se_ratio)

  cat("\n**Interpretation:**\n\n")

  if (max_ratio < 1.2 && min_ratio > 0.8) {
    cat("- SE ratios are very close to 1.0 (",
        sprintf("%.3f to %.3f", min_ratio, max_ratio),
        "), indicating **minimal heteroscedasticity**\n")
    cat("- Standard Wald errors are reliable for inference\n")
    cat("- Both p-values remain highly significant under robust estimation\n")
  } else if (max_ratio < 1.5) {
    cat("- SE ratios are close to 1.0, indicating **mild heteroscedasticity**\n")
    cat("- Standard errors remain reasonably reliable\n")
    cat("- P-values are similar under both approaches\n")
  } else {
    cat("- SE ratios differ substantially from 1.0, indicating **substantial heteroscedasticity**\n")
    cat("- Robust standard errors should be preferred for inference\n")
    cat("- Consider heteroscedasticity-robust models for more accurate prediction intervals\n")
  }

  cat("\n**Key Result**: The RIR effect (", sprintf("%.4f", robust_df$estimate[robust_df$term == "rir"]),
      " m/s per RIR) remains highly significant (p ", robust_df$p_robust[robust_df$term == "rir"],
      ") under robust estimation.\n")
}
```

### Bootstrap Confidence Intervals

#### What This Tests

Parametric bootstrap provides **assumption-free confidence intervals** by simulating the sampling distribution directly from the fitted model. This approach:

- Does not assume any particular error distribution
- Accounts for small sample sizes
- Provides empirical confidence intervals

#### The Method

We resample from the fitted model 1000 times and extract the distribution of parameter estimates. The 2.5th and 97.5th percentiles of this distribution form the 95% confidence interval.

```{r bootstrap-ci}
#| label: tbl-bootstrap-ci
#| tbl-cap: "Bootstrap 95% Confidence Intervals"

if (!is.null(results$bootstrap_ci)) {
  boot_df <- results$bootstrap_ci
  boot_df$estimate <- round(boot_df$estimate, 4)
  boot_df$ci_lower <- round(boot_df$ci_lower, 4)
  boot_df$ci_upper <- round(boot_df$ci_upper, 4)
  boot_df$ci_width <- round(boot_df$ci_width, 4)

  kable(boot_df[, c("term", "estimate", "ci_lower", "ci_upper", "ci_width")],
        col.names = c("Term", "Estimate", "Lower 95%", "Upper 95%", "CI Width"))
}
```

```{r bootstrap-interpretation, results='asis'}
if (!is.null(results$bootstrap_ci)) {
  rir_row <- boot_df[boot_df$term == "rir", ]
  intercept_row <- boot_df[boot_df$term == "(Intercept)", ]

  cat("\n**Interpretation:**\n\n")

  if (rir_row$ci_lower > 0) {
    cat("- The **RIR effect is significantly positive** (95% CI excludes zero)\n")
    cat("- Each additional RIR increases velocity by ",
        sprintf("%.3f m/s [%.3f, %.3f]", rir_row$estimate, rir_row$ci_lower, rir_row$ci_upper), "\n")
    cat("- The CI width (", sprintf("%.3f", rir_row$ci_width), " m/s) reflects estimation precision\n")
  }

  cat("- The **intercept** (velocity at RIR=0/failure) is ",
      sprintf("%.3f m/s [%.3f, %.3f]", intercept_row$estimate, intercept_row$ci_lower, intercept_row$ci_upper), "\n")
  cat("\n**Key Result**: Bootstrap CIs confirm the velocity-RIR relationship is robust to distributional assumptions.\n")
}
```

### Sensitivity Analysis

#### What This Tests

Different model specifications can yield different estimates. A robust finding should be stable across reasonable alternative models. We test sensitivity to:

- **Random effects structure**: Random intercepts only vs. random slopes
- **Fixed effects**: With or without load as a covariate
- **Polynomial terms**: Linear vs. quadratic relationship

#### The Method

We fit multiple plausible model specifications and compare the RIR effect estimate across them. If the coefficient of variation (CV) is < 10%, conclusions are robust.

```{r sensitivity}
#| label: tbl-sensitivity
#| tbl-cap: "RIR Effect Sensitivity to Model Specification"

if (!is.null(results$sensitivity)) {
  sens <- results$sensitivity
  sens_table <- sens$rir_effects

  # Display with proper column names
  display_table <- data.frame(
    Model = sens_table$model,
    `RIR Effect` = round(sens_table$rir_estimate, 4),
    SE = round(sens_table$rir_se, 4),
    AIC = round(sens_table$aic, 1),
    BIC = round(sens_table$bic, 1),
    `R² (marg)` = round(sens_table$r2_marginal, 3),
    `R² (cond)` = round(sens_table$r2_conditional, 3)
  )

  kable(display_table,
        col.names = c("Model", "RIR Effect", "SE", "AIC", "BIC", "R² (marg)", "R² (cond)"))
}
```

```{r sensitivity-viz}
#| label: fig-sensitivity
#| fig-cap: "RIR Effect Estimates Across Model Specifications"

if (!is.null(results$sensitivity)) {
  sens_df <- results$sensitivity$rir_effects
  sens_df$model <- factor(sens_df$model, levels = sens_df$model)

  ggplot(sens_df, aes(x = model, y = rir_estimate)) +
    geom_point(size = 4, color = "#2E86AB") +
    geom_errorbar(aes(ymin = rir_estimate - 1.96 * rir_se, ymax = rir_estimate + 1.96 * rir_se),
                  width = 0.2, color = "#2E86AB") +
    geom_hline(yintercept = mean(sens_df$rir_estimate), linetype = "dashed", color = "#E63946") +
    coord_flip() +
    labs(
      x = "",
      y = "RIR Effect (m/s per RIR)",
      title = "Sensitivity of RIR Effect to Model Specification",
      subtitle = "Dashed line = mean across models"
    ) +
    theme_minimal()
}
```

```{r sensitivity-interpretation, results='asis'}
if (!is.null(results$sensitivity)) {
  sens <- results$sensitivity
  rir_estimates <- sens$rir_effects$rir_estimate

  # Calculate summary statistics
  rir_mean <- mean(rir_estimates)
  rir_sd <- sd(rir_estimates)
  rir_cv <- (rir_sd / rir_mean) * 100

  cat("\n**Summary Statistics Across Models:**\n\n")
  cat("| Metric | Value |\n")
  cat("|--------|-------|\n")
  cat(sprintf("| Mean RIR effect | %.4f m/s per RIR |\n", rir_mean))
  cat(sprintf("| SD across models | %.4f |\n", rir_sd))
  cat(sprintf("| Coefficient of variation | %.1f%% |\n", rir_cv))
  cat(sprintf("| Range | %.4f - %.4f |\n", min(rir_estimates), max(rir_estimates)))

  cat("\n**Interpretation:**\n\n")

  if (rir_cv < 10) {
    cat("- The RIR effect is **highly robust** to model specification (CV < 10%)\n")
    cat("- All models agree on the direction and approximate magnitude of the effect\n")
    cat("- The choice of model does not meaningfully affect conclusions\n")
  } else {
    cat("- The RIR effect shows **some sensitivity** to model specification (CV > 10%)\n")
    cat("- Conclusions should be interpreted with appropriate caution\n")
    cat("- The best-fitting model (lowest BIC) should be preferred\n")
  }
}
```

### Summary of Robustness Checks

```{r robustness-summary, results='asis'}
cat("| Check | Result | Conclusion |\n")
cat("|-------|--------|------------|\n")

# Robust SE
if (!is.null(results$robust_se)) {
  se_ratio <- max(results$robust_se$se_ratio)
  if (se_ratio < 1.2) {
    cat(sprintf("| Cluster-Robust SE | Ratio = %.3f | No heteroscedasticity concern |\n", se_ratio))
  } else {
    cat(sprintf("| Cluster-Robust SE | Ratio = %.3f | Some heteroscedasticity present |\n", se_ratio))
  }
}

# Bootstrap CI
if (!is.null(results$bootstrap_ci)) {
  rir_ci <- results$bootstrap_ci[results$bootstrap_ci$term == "rir", ]
  cat(sprintf("| Bootstrap CI | [%.3f, %.3f] | Effect significant (CI excludes 0) |\n",
              rir_ci$ci_lower, rir_ci$ci_upper))
}

# Sensitivity
if (!is.null(results$sensitivity)) {
  rir_estimates <- results$sensitivity$rir_effects$rir_estimate
  cv <- (sd(rir_estimates) / mean(rir_estimates)) * 100
  if (cv < 10) {
    cat(sprintf("| Sensitivity Analysis | CV = %.1f%% | Robust to model choice |\n", cv))
  } else {
    cat(sprintf("| Sensitivity Analysis | CV = %.1f%% | Some sensitivity to model |\n", cv))
  }
}

cat("\n**Overall Conclusion**: The velocity-RIR relationship is robust across all checks. ",
    "Our findings are not artifacts of specific modeling choices.\n")
```

## Comparison with Study 4 (Non-LMM)

How do the LMM results compare with the simpler OLS approach from Study 4?

```{r study4-comparison, results='asis'}
if (!is.null(study4_results) && !is.null(results$study4_comparison)) {
  comp <- results$study4_comparison

  cat("### Methodology Comparison\n\n")
  cat("| Aspect | Study 4 (OLS) | Study 5 (LMM) |\n")
  cat("|--------|---------------|---------------|\n")
  cat("| Approach | Separate models per participant | Single hierarchical model |\n")
  cat("| Random effects | None | Intercepts + slopes |\n")
  cat("| Uncertainty | Point estimates only | Bootstrap + conformal CI |\n")
  cat("| Covariate testing | None | LRT, AIC, BIC, Bayes Factor |\n\n")

  cat("### Model Fit (R²)\n\n")
  cat("| Approach | General R² | Individual R² |\n")
  cat("|----------|-----------|---------------|\n")
  cat(sprintf("| Study 4 (OLS) | %.3f | %.3f |\n",
              comp$study4_general_r2, comp$study4_individual_r2))
  cat(sprintf("| Study 5 (LMM) | %.3f (marginal) | %.3f (conditional) |\n\n",
              comp$study5_marginal_r2, comp$study5_conditional_r2))

  cat("### RIR Effect Comparison\n\n")
  cat(sprintf("- Study 5 LMM: **%.4f** m/s per RIR\n", comp$study5_rir_effect))

  cat("\n### Key Agreements\n\n")
  cat("1. **~0.03 m/s per RIR**: Both methods agree on the fundamental relationship\n")
  cat("2. **Individual calibration helps**: Both show ~30-50% improvement with personalization\n\n")

  cat("### New Insights from LMM\n\n")
  cat("3. **Load doesn't matter**: A single velocity table works for both 80% and 90% 1RM\n")
  cat("4. **Better uncertainty quantification**: Conformal prediction achieves 95% coverage\n")
} else {
  cat("*Study 4 comparison not available. Run `make replicate-deadlift` first.*\n")
}
```

## Practical Implications

### For Coaches and Athletes

1. **Use velocity monitoring** during deadlift sets
2. **Reference the velocity table** to estimate RIR in real-time
3. **Consider individual calibration** for serious athletes (if improvement >10%)
4. **Account for uncertainty**: Velocity estimates have ~`r round(results$conformal$interval_width * 1000, 1)` mm/s margin of error
5. **Load-independent**: The same velocity targets work for 80% and 90% 1RM

### For Researchers

1. **LMM is appropriate** for this nested data structure
2. **Model assumptions show violations** but robust checks confirm conclusions are stable
3. **Conformal prediction** provides more reliable coverage guarantees than parametric methods

## Limitations

This study has several limitations that should be considered:

1. **Sample size**: With 19 participants, we have adequate power for LMM fixed effects but limited ability to detect smaller effects or model more complex random effect structures.

2. **Load range**: Only 80% and 90% 1RM were tested. The finding that load doesn't matter may not extend to lighter loads (60-70%) where more repetitions are performed and fatigue dynamics differ.

3. **Exercise specificity**: Results apply specifically to the conventional deadlift. Other deadlift variants (sumo, trap bar) and other exercises may show different patterns.

4. **Population specificity**: All participants were Portuguese resistance-trained individuals. Generalization to other populations requires caution.

5. **Assumption violations**: Some model diagnostics showed violations of normality and homoscedasticity assumptions. While robust analyses confirmed main conclusions, these violations warrant attention.

6. **Cross-day only**: Conformal prediction was validated on Day 2 data only. Longer-term validity (week-to-week) was not tested.

## Conclusions

1. **`r if(results$load_importance_result$recommendation == "global") "A single global velocity table is sufficient" else "Load-specific tables are recommended"`** - load percentage `r if(results$load_importance_result$recommendation == "global") "does not significantly" else "significantly"` affect the velocity-RIR relationship

2. **Individual calibration improves accuracy** by `r round(results$individual_comparison$mae_improvement_pct, 1)`%

3. **Conformal prediction** provides more reliable interval coverage than parametric methods

4. **Velocity-based training** can effectively prescribe training intensity for deadlifts using the provided stop tables

## Technical Notes

This analysis used:
- **R6 classes** following SOLID principles
- **lme4** for Linear Mixed Effects Models
- **Split conformal prediction** for distribution-free intervals
- **BIC-based Bayes Factor approximation** for model comparison

## References

This analysis builds on established methodology from several fields:

**Mixed Effects Modeling:**

- Laird, N. M., & Ware, J. H. (1982). Random-effects models for longitudinal data. *Biometrics*, 38(4), 963-974.

- Nakagawa, S., & Schielzeth, H. (2013). A general and simple method for obtaining R² from generalized linear mixed-effects models. *Methods in Ecology and Evolution*, 4(2), 133-142.

**Conformal Prediction:**

- Vovk, V., Gammerman, A., & Shafer, G. (2005). *Algorithmic Learning in a Random World*. Springer.

- Lei, J., G'Sell, M., Rinaldo, A., Tibshirani, R. J., & Wasserman, L. (2018). Distribution-Free Predictive Inference for Regression. *Journal of the American Statistical Association*, 113(523), 1094-1111.

**Velocity-Based Training:**

- Jukic, I., Prnjak, K., Helms, E. R., & McGuigan, M. R. (2024). Modeling the repetitions-in-reserve-velocity relationship: A valid method for resistance training monitoring and prescription, and fatigue management. *Experimental Physiology*, 109(2), 193-206.

- González-Badillo, J. J., & Sánchez-Medina, L. (2010). Movement velocity as a measure of loading intensity in resistance training. *International Journal of Sports Medicine*, 31(5), 347-352.
